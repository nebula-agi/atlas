{
  "generated_at": "2026-01-04T21:51:18.878739",
  "adapter_type": "NaiveRAGAdapter",
  "num_benchmarks": 50,
  "total_probes": 589,
  "overall_summary": {
    "overall_accuracy": 0.6876061120543294,
    "total_score": 405,
    "count": 589,
    "by_answer_type": {
      "short_answer": {
        "total": 277,
        "count": 450,
        "accuracy": 0.6155555555555555
      },
      "boolean": {
        "total": 23,
        "count": 27,
        "accuracy": 0.8518518518518519
      },
      "verbatim": {
        "total": 18,
        "count": 21,
        "accuracy": 0.8571428571428571
      },
      "abstain": {
        "total": 82,
        "count": 83,
        "accuracy": 0.9879518072289156
      },
      "generation": {
        "total": 5,
        "count": 8,
        "accuracy": 0.625
      }
    },
    "by_pillar": {
      "world_modeling": {
        "total": 56,
        "count": 99,
        "accuracy": 0.5656565656565656
      },
      "declarative_reasoning": {
        "total": 81,
        "count": 101,
        "accuracy": 0.801980198019802
      },
      "temporal_episodic": {
        "total": 39,
        "count": 99,
        "accuracy": 0.3939393939393939
      },
      "preference_learning": {
        "total": 75,
        "count": 97,
        "accuracy": 0.7731958762886598
      },
      "knowledge_boundaries": {
        "total": 93,
        "count": 98,
        "accuracy": 0.9489795918367347
      },
      "procedural_knowledge": {
        "total": 61,
        "count": 95,
        "accuracy": 0.6421052631578947
      }
    }
  },
  "stored_memories": [],
  "benchmark_summaries": [
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_010.json",
      "accuracy": 0.7692307692307693,
      "num_probes": 13,
      "ingestion_time_s": 0.8099634647369385,
      "eval_time_s": 78.43594884872437
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_011.json",
      "accuracy": 0.9166666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.6967570781707764,
      "eval_time_s": 64.57042384147644
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_022.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.6608867645263672,
      "eval_time_s": 67.14139795303345
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_033.json",
      "accuracy": 0.7272727272727273,
      "num_probes": 11,
      "ingestion_time_s": 0.6389949321746826,
      "eval_time_s": 60.60981249809265
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_035.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.683629035949707,
      "eval_time_s": 79.87631106376648
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_038.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.7866451740264893,
      "eval_time_s": 65.05601906776428
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_044.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.6369030475616455,
      "eval_time_s": 62.530463218688965
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_048.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8098335266113281,
      "eval_time_s": 67.0896668434143
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_049.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.6981492042541504,
      "eval_time_s": 69.84587502479553
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_008.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.7891342639923096,
      "eval_time_s": 68.49373435974121
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_013.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8206090927124023,
      "eval_time_s": 66.501868724823
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_015.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.6422150135040283,
      "eval_time_s": 64.34203124046326
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_018.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.6541659832000732,
      "eval_time_s": 74.66926741600037
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_026.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.9055540561676025,
      "eval_time_s": 68.73328113555908
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_028.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.7521026134490967,
      "eval_time_s": 67.86183905601501
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_031.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.754284143447876,
      "eval_time_s": 67.41122841835022
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_036.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.7655949592590332,
      "eval_time_s": 69.11849188804626
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_041.json",
      "accuracy": 0.7857142857142857,
      "num_probes": 14,
      "ingestion_time_s": 0.7833631038665771,
      "eval_time_s": 80.98023819923401
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_043.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.7931361198425293,
      "eval_time_s": 75.06401371955872
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_046.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.7724831104278564,
      "eval_time_s": 69.12655663490295
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_050.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.782066822052002,
      "eval_time_s": 68.44054198265076
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_004.json",
      "accuracy": 0.6,
      "num_probes": 10,
      "ingestion_time_s": 0.7293112277984619,
      "eval_time_s": 58.879474401474
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_005.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.7862222194671631,
      "eval_time_s": 71.46864104270935
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_007.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.7962112426757812,
      "eval_time_s": 68.00554251670837
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_014.json",
      "accuracy": 1.0,
      "num_probes": 7,
      "ingestion_time_s": 0.8687858581542969,
      "eval_time_s": 40.65702986717224
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_020.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.7683281898498535,
      "eval_time_s": 65.18519568443298
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_021.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.7306921482086182,
      "eval_time_s": 68.80137801170349
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_030.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.7068824768066406,
      "eval_time_s": 71.99941873550415
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_034.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.9027459621429443,
      "eval_time_s": 69.09455132484436
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_047.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.7269670963287354,
      "eval_time_s": 65.8366711139679
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_002.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8203370571136475,
      "eval_time_s": 67.59403109550476
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_003.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.7594156265258789,
      "eval_time_s": 67.81122088432312
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_009.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.6508266925811768,
      "eval_time_s": 67.22073101997375
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_012.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.773733377456665,
      "eval_time_s": 70.51624178886414
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_024.json",
      "accuracy": 0.9166666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.6700212955474854,
      "eval_time_s": 68.82784533500671
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_029.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8501222133636475,
      "eval_time_s": 69.9215259552002
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_032.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.6600043773651123,
      "eval_time_s": 67.11000108718872
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_040.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8502991199493408,
      "eval_time_s": 67.60533666610718
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_045.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.7035775184631348,
      "eval_time_s": 66.64939403533936
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_001.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.6316580772399902,
      "eval_time_s": 66.00387573242188
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_006.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.7247927188873291,
      "eval_time_s": 69.18487882614136
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_016.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.8063573837280273,
      "eval_time_s": 68.31982517242432
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_017.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.7949886322021484,
      "eval_time_s": 71.89188027381897
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_019.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.9714193344116211,
      "eval_time_s": 72.75794839859009
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_023.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.6624181270599365,
      "eval_time_s": 68.71261501312256
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_025.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.722114086151123,
      "eval_time_s": 73.2287278175354
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_027.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.7658882141113281,
      "eval_time_s": 73.92225980758667
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_037.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.715430498123169,
      "eval_time_s": 68.71420693397522
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_039.json",
      "accuracy": 0.8333333333333334,
      "num_probes": 6,
      "ingestion_time_s": 0.7145164012908936,
      "eval_time_s": 33.781694173812866
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_042.json",
      "accuracy": 0.75,
      "num_probes": 12,
      "ingestion_time_s": 0.7243218421936035,
      "eval_time_s": 71.16534447669983
    }
  ],
  "detailed_results": [
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_010.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 13,
      "ingestion_time_s": 0.8099634647369385,
      "total_evaluation_time_s": 78.43594884872437,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is repeatedly identified as the user's cofounder throughout the sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-0, Turn 0] user: Good morning. I'm trying to get my head straight before the meeting with my cofounder later.",
            "[Session llm-2, Turn 13] assistant: You might want to check with your cofounder on the specific version he's using.",
            "[Session llm-2, Turn 3] assistant: It happens. The password is just the organization's name, then 2024, and an exclamation mark at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.021064758300781,
          "generation_latency_ms": 4040.693998336792,
          "total_latency_ms": 4052.715063095093
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and the person concerned about the API layer's speed?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleague",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: He thinks the API layer is too slow. We're running it on that V8-based runtime, which is great for the event loop, but it's struggling with the heavy computation.",
            "[Session llm-1, Turn 4] user: Exactly. At least the API layer is faster to iterate on since it's just Node.js.",
            "[Session llm-0, Turn 4] user: Right, that's it. Sato was really adamant about the performance benefits for the backend services."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00776481628418,
          "generation_latency_ms": 3571.6323852539062,
          "total_latency_ms": 3579.6401500701904
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which framework used in the CodeStream core project is described as a systems language with a 'crab mascot'?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-4, Turn 6] user: Yeah, we actually started rewriting the core modules. We picked that systems language that everyone loves for memory safety. The one with the crab mascot.",
            "[Session llm-5, Turn 3] assistant: If you are referring to the CodeStream core project, the architecture primarily utilizes a combination of Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-4, Turn 3] assistant: I'm sorry to hear that. What specifically is your cofounder concerned about regarding the CodeStream architecture?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.006095886230469,
          "generation_latency_ms": 3626.149892807007,
          "total_latency_ms": 3634.1559886932373
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Based on the rule 'Organization Name + Year + !', what is the password for the office wifi?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: No, I have them. Actually, my tablet disconnected from the network. What's the password for the office wifi?",
            "[Session llm-7, Turn 2] user: Thanks. I'm trying to connect my new tablet to the office wifi but I forgot what Sato changed the password to.",
            "[Session llm-8, Turn 10] user: Thanks. One last thing, my phone forgot the wifi. Is it still the standard password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00103759765625,
          "generation_latency_ms": 3747.4207878112793,
          "total_latency_ms": 3759.4218254089355
        },
        {
          "probe_id": "temporal_episodic-cyclical_event_recognition-5",
          "pillar": "temporal_episodic",
          "subpillar": "cyclical_event_recognition",
          "question": "What specific technical concern does the cofounder Sato raise repeatedly across multiple sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Backend performance",
          "predicted_answer": "Backend performance",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Yeah, Sato is worried about the performance of the core project. Remind me what stack we locked in for that?",
            "[Session llm-6, Turn 3] assistant: I can help clarify that. What specific concerns does Sato have regarding the architecture?",
            "[Session llm-4, Turn 2] user: Thanks. I've been stressing about the startup all morning. Sato is worried about our backend performance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000560760498047,
          "generation_latency_ms": 3990.4961585998535,
          "total_latency_ms": 4002.4967193603516
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user requested a reminder to call the dentist, what office detail did they also ask about?",
          "answer_type": "short_answer",
          "gold_answer": "Wifi password",
          "predicted_answer": "office internet credentials",
          "retrieved_context": [
            "[Session llm-3, Turn 9] assistant: I've set a reminder for 2 PM to call the dentist.",
            "[Session llm-7, Turn 1] assistant: Done. I've set a reminder to call the dentist at 4:00 PM today.",
            "[Session llm-7, Turn 0] user: Hey, can you set a reminder for me to call the dentist at 4 PM?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.018369674682617,
          "generation_latency_ms": 4001.2595653533936,
          "total_latency_ms": 4014.277935028076
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-7",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask about their lost jacket before or after the conversation about the rain?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Did I leave my jacket at the office yesterday?",
            "[Session llm-5, Turn 10] user: Cool. Also, did I leave my jacket in the meeting room yesterday?",
            "[Session llm-9, Turn 0] user: Morning. Any chance of rain today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 26.99732780456543,
          "generation_latency_ms": 4429.078102111816,
          "total_latency_ms": 4456.075429916382
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which aspect of the project does the user prioritize discussing: frontend UI or backend performance?",
          "answer_type": "short_answer",
          "gold_answer": "Backend performance",
          "predicted_answer": "backend performance",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: Right, that's it. Sato was really adamant about the performance benefits for the backend services.",
            "[Session llm-6, Turn 7] assistant: Exactly. We are using Node.js for the general backend services to handle asynchronous operations efficiently.",
            "[Session llm-6, Turn 6] user: Great, that's what I thought. And for the rest of the backend services? We're sticking with the V8-based runtime, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.908531188964844,
          "generation_latency_ms": 4200.923442840576,
          "total_latency_ms": 4212.831974029541
        },
        {
          "probe_id": "preference_learning-preference_scope-9",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Has the user explicitly requested the assistant to use a formal persona during their conversations?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: Is he looking for an overview of the languages involved?",
            "[Session llm-2, Turn 7] assistant: In what way?",
            "[Session llm-2, Turn 1] assistant: Sounds like a busy morning. Do you need a quick summary of your inbox?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 18.013954162597656,
          "generation_latency_ms": 3915.21954536438,
          "total_latency_ms": 3933.2334995269775
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is mentioned as part of the CodeStream core project stack?",
          "answer_type": "short_answer",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: If you are referring to the CodeStream core project, the architecture primarily utilizes a combination of Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-4, Turn 3] assistant: I'm sorry to hear that. What specifically is your cofounder concerned about regarding the CodeStream architecture?",
            "[Session llm-9, Turn 5] assistant: For the main CodeStream codebase, the team settled on using Node.js alongside Rust to handle the heavier processing."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 3738.1229400634766,
          "total_latency_ms": 3748.124361038208
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-11",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the CodeStream core project built exclusively using the Rust programming language?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-9, Turn 5] assistant: For the main CodeStream codebase, the team settled on using Node.js alongside Rust to handle the heavier processing.",
            "[Session llm-5, Turn 3] assistant: If you are referring to the CodeStream core project, the architecture primarily utilizes a combination of Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-2, Turn 11] assistant: Correct, the core logic is still written in Rust for performance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 4044.3248748779297,
          "total_latency_ms": 4053.3249378204346
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-12",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the procedure described for constructing the organization's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Name + Year + !",
          "predicted_answer": "The organization's name, then 2024, and an exclamation mark at the end.",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: No, I have them. Actually, my tablet disconnected from the network. What's the password for the office wifi?",
            "[Session llm-8, Turn 10] user: Thanks. One last thing, my phone forgot the wifi. Is it still the standard password?",
            "[Session llm-7, Turn 2] user: Thanks. I'm trying to connect my new tablet to the office wifi but I forgot what Sato changed the password to."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.003204345703125,
          "generation_latency_ms": 3168.8003540039062,
          "total_latency_ms": 3183.8035583496094
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-13",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "According to the sessions, what technical limitation of Node.js necessitated the inclusion of Rust?",
          "answer_type": "short_answer",
          "gold_answer": "Heavy computation struggles",
          "predicted_answer": "The documents indicate that Rust was included for \"performance-critical components,\" \"heavier processing,\"",
          "retrieved_context": [
            "[Session llm-7, Turn 9] assistant: That's correct, the architecture relies on Rust for performance-critical components and Node.js for the services.",
            "[Session llm-3, Turn 7] assistant: Mixing Rust with Node.js is a powerful combination if you get the bindings right.",
            "[Session llm-9, Turn 5] assistant: For the main CodeStream codebase, the team settled on using Node.js alongside Rust to handle the heavier processing."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 4155.52830696106,
          "total_latency_ms": 4165.531635284424
        }
      ],
      "summary": {
        "overall_accuracy": 0.7692307692307693,
        "total_score": 10,
        "count": 13,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 11,
            "accuracy": 0.7272727272727273
          },
          "boolean": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 3,
            "accuracy": 0.3333333333333333
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_011.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6967570781707764,
      "total_evaluation_time_s": 64.57042384147644,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Oliver",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: You mean the CodeStream Core project? How is everything going with your cofounder?",
            "[Session llm-5, Turn 0] user: Good morning. Has my cofounder logged on yet?",
            "[Session llm-7, Turn 2] user: Yeah, my cofounder Oliver is pestering me about the backend architecture for the core project. He forgot what we decided on."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.523651123046875,
          "generation_latency_ms": 3426.1016845703125,
          "total_latency_ms": 3438.6253356933594
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the organization associated with the wifi password 'CodeStream2024!'?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream",
          "predicted_answer": "CodeStream",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: I've updated the wifi password to CodeStream2024!.",
            "[Session llm-2, Turn 10] user: Let's do that later. Back to the network, just to be sure, that's CodeStream2024! right?",
            "[Session llm-8, Turn 3] assistant: No problem. The password for the CodeStream network is CodeStream2024! if he needs to connect."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999513626098633,
          "generation_latency_ms": 3806.0142993927,
          "total_latency_ms": 3816.013813018799
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming technologies are combined to form the hybrid stack for the CodeStream Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Rust and Node.js",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Ah, the eternal debate. Are you still planning on that hybrid approach for the CodeStream codebase?",
            "[Session llm-6, Turn 3] assistant: You mean the CodeStream Core project? How is everything going with your cofounder?",
            "[Session llm-6, Turn 7] assistant: Got it, so a mix of Rust and Node.js for CodeStream Core."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.375572204589844,
          "generation_latency_ms": 3261.8565559387207,
          "total_latency_ms": 3271.2321281433105
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the verbatim password for the CodeStream network?",
          "answer_type": "verbatim",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: No problem. The password for the CodeStream network is CodeStream2024! if he needs to connect.",
            "[Session llm-6, Turn 11] assistant: I've updated the wifi password to CodeStream2024!.",
            "[Session llm-2, Turn 10] user: Let's do that later. Back to the network, just to be sure, that's CodeStream2024! right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.833646774291992,
          "generation_latency_ms": 3652.796745300293,
          "total_latency_ms": 3661.630392074585
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "The user requested dentist reminders in two separate sessions. What time was specified in the most recent request?",
          "answer_type": "short_answer",
          "gold_answer": "12:00 PM",
          "predicted_answer": "noon",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: I've set a reminder for 12:00 PM to call the dentist.",
            "[Session llm-3, Turn 13] assistant: No problem, I have removed the reminder for the dentist.",
            "[Session llm-6, Turn 1] assistant: I've set a reminder to call the dentist at 2 PM. Anything else on your mind?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004592895507812,
          "generation_latency_ms": 3271.2254524230957,
          "total_latency_ms": 3280.2300453186035
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user complained about the office being freezing, which programming language was confirmed for the backend?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: I need to double-check the tech stack for our main project. Oliver was asking about the backend languages we settled on yesterday.",
            "[Session llm-0, Turn 2] user: Mostly tech debt discussions. Remind me, what are the primary languages we're using for the core project right now?",
            "[Session llm-4, Turn 6] user: Yeah, specifically about the core project's architecture. He forgot which language we picked for the high-performance modules."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3766.5865421295166,
          "total_latency_ms": 3776.5865325927734
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about the office environment, what physical condition do they dislike?",
          "answer_type": "short_answer",
          "gold_answer": "Cold temperatures",
          "predicted_answer": "Freezing and loud",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: Good morning. It certainly looks gloomy out there. Hopefully, you can stay dry inside.",
            "[Session llm-3, Turn 0] user: Good morning. It's surprisingly sunny outside today, isn't it?",
            "[Session llm-7, Turn 5] assistant: And for the performance-critical components?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 2638.0512714385986,
          "total_latency_ms": 2648.054599761963
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user prefer to use Node.js for the performance-critical components of the project?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: That's right. The plan is to keep Node.js for the API layer and orchestration to maintain that development speed.",
            "[Session llm-3, Turn 3] assistant: Checking the documentation for the core build, it looks like a hybrid approach. The performance-critical components are written in Rust, while the rest of the backend services utilize Node.js.",
            "[Session llm-9, Turn 5] assistant: Mixing Node.js with Rust certainly adds complexity, but the performance gains should be worth it in the long run."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00358772277832,
          "generation_latency_ms": 4015.52152633667,
          "total_latency_ms": 4028.5251140594482
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is Oliver's surname?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Good morning! I haven't seen Oliver active just yet. He might be running a bit late today.",
            "[Session llm-8, Turn 2] user: Yeah, I'm just settling in. Quick question, Oliver is stopping by the office later and asked for the wifi details again.",
            "[Session llm-6, Turn 2] user: Yeah, I was looking over the repo for the main project with Oliver this morning."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.998870849609375,
          "generation_latency_ms": 3356.022357940674,
          "total_latency_ms": 3365.021228790283
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which specific database system is configured for the CodeStream Core project?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Ah, the eternal debate. Are you still planning on that hybrid approach for the CodeStream codebase?",
            "[Session llm-1, Turn 2] user: Definitely. Speaking of diving in, I need to check the dependencies for the main CodeStream project again. My cofounder mentioned we might need to update the backend environments.",
            "[Session llm-6, Turn 3] assistant: You mean the CodeStream Core project? How is everything going with your cofounder?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.012771606445312,
          "generation_latency_ms": 3328.7932872772217,
          "total_latency_ms": 3336.806058883667
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's explanation, what three elements make up the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "The organization name (capitalized), 2024, and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-6, Turn 9] assistant: I don't have the current wifi password on file for the organization.",
            "[Session llm-3, Turn 7] assistant: I do. For the office Wi-Fi, the key is the organization name—capitalized as usual—followed immediately by 2024 and an exclamation mark.",
            "[Session llm-0, Turn 6] user: Thanks. Also, my sister is stopping by the office later to drop off some keys. Do you know the wifi password so she can get online?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.997295379638672,
          "generation_latency_ms": 3363.645315170288,
          "total_latency_ms": 3374.6426105499268
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific difficulty did the user report regarding the interaction between the server-side JavaScript and systems language components?",
          "answer_type": "short_answer",
          "gold_answer": "Bridging the gap",
          "predicted_answer": "Bridging the gap between the server-side JavaScript and the systems language components is trickier than I thought.",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Yeah, bridging the gap between the server-side JavaScript and the systems language components is trickier than I thought.",
            "[Session llm-4, Turn 11] assistant: Correct. For the API services, the architecture document specifies the popular server-side JavaScript runtime.",
            "[Session llm-8, Turn 8] user: Exactly. But we aren't getting rid of the JavaScript runtime entirely, right? The team loves the ecosystem."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.998559951782227,
          "generation_latency_ms": 3278.72371673584,
          "total_latency_ms": 3288.722276687622
        }
      ],
      "summary": {
        "overall_accuracy": 0.9166666666666666,
        "total_score": 11,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 8,
            "accuracy": 0.875
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_022.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6608867645263672,
      "total_evaluation_time_s": 67.14139795303345,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' described as being 'dead set' on using the 'crab language'?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: Yeah, the crab language is treating us well for the heavy lifting, but we need it to talk to the API layer smoothly.",
            "[Session llm-9, Turn 4] user: Yeah, she is dead set on using the crab language for the performance critical stuff. I can't argue with the memory safety features, honestly.",
            "[Session llm-3, Turn 5] assistant: Your cofounder certainly has strong opinions on system stability and speed."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.002655029296875,
          "generation_latency_ms": 3490.4565811157227,
          "total_latency_ms": 3506.4592361450195
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Ava?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Sounds important. Is Ava still leaning towards that systems language she likes for the heavy lifting?",
            "[Session llm-8, Turn 3] assistant: Did Ava have any specific concerns about the architecture?",
            "[Session llm-3, Turn 4] user: I had a feeling. Ava usually prefers that combination for the backend."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.008811950683594,
          "generation_latency_ms": 3891.3960456848145,
          "total_latency_ms": 3901.404857635498
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the naming convention described by the assistant, what is the exact wifi password for the office?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 8] user: Actually, a guest is coming by the office. What's the wireless key for the company network?",
            "[Session llm-0, Turn 8] user: Hold on a second, my phone just dropped the connection. What was the office wifi pass again? I always forget if it has a special character.",
            "[Session llm-8, Turn 8] user: Good point. Also, I'm setting up a new tablet here at the office. What's the wifi string?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.534358978271484,
          "generation_latency_ms": 3099.294900894165,
          "total_latency_ms": 3108.8292598724365
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two frameworks are used in the hybrid backend architecture of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Rust and Node.js",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Yes, the architecture for the main platform is split. The performance-critical components are written in Rust, while the rest of the services run on Node.js.",
            "[Session llm-2, Turn 3] assistant: Yes, for CodeStream Core, the architecture currently relies on Rust for the performance-critical components and Node.js for the rest. I can pull up the specific docs if Ava needs them.",
            "[Session llm-3, Turn 2] user: Thanks. I'm reviewing the architecture docs for our core project. We're using two main languages, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999773025512695,
          "generation_latency_ms": 4348.8640785217285,
          "total_latency_ms": 4361.863851547241
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific traffic incident caused the delay on the morning the user asked for the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Stall on the bridge",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 8] user: Hold on a second, my phone just dropped the connection. What was the office wifi pass again? I always forget if it has a special character.",
            "[Session llm-4, Turn 10] user: Thanks. Also, a guest is coming by later. What's our wifi key again?",
            "[Session llm-6, Turn 2] user: About forty minutes. Anyway, my cofounder is trying to connect her new laptop to the office internet. What's the password again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 3820.9187984466553,
          "total_latency_ms": 3829.9195766448975
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the conversation about the wifi password happen before or after the user prepared for the investor meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 8] user: Yeah, just the password for the wifi. I always forget the format.",
            "[Session llm-6, Turn 2] user: About forty minutes. Anyway, my cofounder is trying to connect her new laptop to the office internet. What's the password again?",
            "[Session llm-1, Turn 8] user: Actually, a guest is coming by the office. What's the wireless key for the company network?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.769512176513672,
          "generation_latency_ms": 2693.0184364318848,
          "total_latency_ms": 2701.7879486083984
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does Ava strongly prefer for performance-critical components?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Sounds important. Is Ava still leaning towards that systems language she likes for the heavy lifting?",
            "[Session llm-2, Turn 3] assistant: Yes, for CodeStream Core, the architecture currently relies on Rust for the performance-critical components and Node.js for the rest. I can pull up the specific docs if Ava needs them.",
            "[Session llm-5, Turn 3] assistant: Sounds intense. Are you still sticking with that systems language you mentioned earlier for the performance-critical stuff?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.025501251220703,
          "generation_latency_ms": 3714.6050930023193,
          "total_latency_ms": 3724.63059425354
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user's cofounder prefer Node.js or Rust for the heavy lifting in the backend?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: Mixing Rust for performance with Node.js for the API layer is powerful, but it does introduce complexity.",
            "[Session llm-1, Turn 5] assistant: For the heavy lifting on the core project, you guys went with Rust to handle the performance needs. The API side is running on Node.js.",
            "[Session llm-8, Turn 5] assistant: Right, looking at the logs, it seems that project relies heavily on Rust for the heavy lifting and Node.js for the rest."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.024215698242188,
          "generation_latency_ms": 3835.8616828918457,
          "total_latency_ms": 3843.885898590088
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which frontend framework is explicitly mentioned as being used for the codestream_core project?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 3] assistant: Yes, for CodeStream Core, the architecture currently relies on Rust for the performance-critical components and Node.js for the rest. I can pull up the specific docs if Ava needs them.",
            "[Session llm-5, Turn 2] user: Pretty busy. Ava and I are deep in the weeds with the codestreamcore project today. We're trying to optimize some of the backend services.",
            "[Session llm-1, Turn 3] assistant: Hope the meeting with Ava goes well. Do you need me to pull up any specific files for the CodeStream Core review?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.024454116821289,
          "generation_latency_ms": 4294.5756912231445,
          "total_latency_ms": 4302.600145339966
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number of the Node.js runtime is the team using?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 9] assistant: Just to confirm for the documentation, we are still using Node.js for the server side?",
            "[Session llm-8, Turn 14] user: No, I'm just going to focus on the Node.js API endpoints for a bit.",
            "[Session llm-8, Turn 5] assistant: Right, looking at the logs, it seems that project relies heavily on Rust for the heavy lifting and Node.js for the rest."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000301361083984,
          "generation_latency_ms": 3697.7391242980957,
          "total_latency_ms": 3706.7394256591797
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What three components are combined to create the organization's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "Company name with a capital C and S, the current year, and an exclamation point.",
          "retrieved_context": [
            "[Session llm-1, Turn 8] user: Actually, a guest is coming by the office. What's the wireless key for the company network?",
            "[Session llm-3, Turn 8] user: Yeah, just the password for the wifi. I always forget the format.",
            "[Session llm-8, Turn 8] user: Good point. Also, I'm setting up a new tablet here at the office. What's the wifi string?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000633239746094,
          "generation_latency_ms": 2888.5223865509033,
          "total_latency_ms": 2899.5230197906494
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What complexity does the assistant warn about regarding the hybrid backend architecture?",
          "answer_type": "short_answer",
          "gold_answer": "Integration/Interoperability",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Is this regarding the backend architecture discussions you've been having?",
            "[Session llm-8, Turn 3] assistant: Did Ava have any specific concerns about the architecture?",
            "[Session llm-7, Turn 3] assistant: Yes, the architecture for the main platform is split. The performance-critical components are written in Rust, while the rest of the services run on Node.js."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.39321517944336,
          "generation_latency_ms": 3667.900800704956,
          "total_latency_ms": 3677.2940158843994
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_033.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 11,
      "ingestion_time_s": 0.6389949321746826,
      "total_evaluation_time_s": 60.60981249809265,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the assistant's instructions regarding the company name and current year, what is the specific password string for the office wifi?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: I do. For the office network, the password is the company name capitalized, followed by the current year and an exclamation point.",
            "[Session llm-5, Turn 7] assistant: Sure. For the office network, the password is just the company name, capitalized, followed by the year 2024 and an exclamation point at the end.",
            "[Session llm-9, Turn 7] assistant: For the CodeStream org network, the password is the company name plus the current year and an exclamation point."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000633239746094,
          "generation_latency_ms": 3796.5433597564697,
          "total_latency_ms": 3807.543992996216
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two technologies comprise the hybrid tech stack for the 'codestream_core' backend?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Node.js and Rust",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: For the CodeStream core project? I recall the architecture involves a combination of Node.js and Rust.",
            "[Session llm-7, Turn 3] assistant: For the main CodeStream codebase, we went with a hybrid approach. It's primarily that memory-safe systems language folks love for performance, paired with a standard server-side JavaScript environment for the API layer.",
            "[Session llm-8, Turn 3] assistant: Are you referring to the main CodeStream repository?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.998104095458984,
          "generation_latency_ms": 3398.5939025878906,
          "total_latency_ms": 3411.5920066833496
        },
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "The user requested a reminder to text their cofounder about a marketing budget. What is the name of that person?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: I've set a reminder for 2 PM to text your cofounder about the budget.",
            "[Session llm-2, Turn 7] assistant: If your cofounder needs the main CodeStream access, it follows the standard pattern: the company name, the year 2024, and an exclamation point.",
            "[Session llm-7, Turn 4] user: Right, the one with the crab mascot and the usual JS runtime. Thanks. Also, remind me to text Olivia about the marketing budget at 2 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.99705696105957,
          "generation_latency_ms": 2912.8575325012207,
          "total_latency_ms": 2923.8545894622803
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is Olivia's professional relationship to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounder",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 13] assistant: I'll draft that email to Olivia regarding the backend technologies now.",
            "[Session llm-0, Turn 6] user: Got it, so CodeStream2024! essentially. Did Olivia say when she's dropping by?",
            "[Session llm-8, Turn 6] user: Ah, right. Olivia loves that oxidation stuff."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.016586303710938,
          "generation_latency_ms": 4134.347200393677,
          "total_latency_ms": 4142.363786697388
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user successfully connected to the office network, what food item did they ask to be reminded to buy?",
          "answer_type": "short_answer",
          "gold_answer": "Sandwich",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 9] assistant: I don't see an email about a potluck in your inbox yet. Should I keep an eye out for it?",
            "[Session llm-3, Turn 5] assistant: It happens. The password for the office network is CodeStream2024! with an exclamation mark at the end.",
            "[Session llm-2, Turn 6] user: Thanks. Olivia is asking about the network credentials for the office guest layout, but I think she just needs the main one."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004831314086914,
          "generation_latency_ms": 3397.559881210327,
          "total_latency_ms": 3406.564712524414
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder to call the caterers before or after they asked about the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: Consider it done. I've set a reminder for 2 PM to call the caterers.",
            "[Session llm-7, Turn 6] user: Perfect. My tablet got kicked off the wifi again. Do you remember the passkey for the office network?",
            "[Session llm-6, Turn 0] user: Hey, I just got back to the office and my phone isn't connecting to the network. Did the credentials change?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000228881835938,
          "generation_latency_ms": 3503.720283508301,
          "total_latency_ms": 3513.7205123901367
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology has been mentioned for the 'codestream_core' project?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: For the CodeStream core project? I recall the architecture involves a combination of Node.js and Rust.",
            "[Session llm-8, Turn 3] assistant: Are you referring to the main CodeStream repository?",
            "[Session llm-7, Turn 3] assistant: For the main CodeStream codebase, we went with a hybrid approach. It's primarily that memory-safe systems language folks love for performance, paired with a standard server-side JavaScript environment for the API layer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.000850677490234,
          "generation_latency_ms": 3820.8224773406982,
          "total_latency_ms": 3828.8233280181885
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Does the user prefer tea or coffee?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "[Session llm-3, Turn 12] user: Actually, hold off on that. I think I just need more coffee first.",
            "[Session llm-5, Turn 4] user: Right, got it. Also, remind me to pick up coffee beans on the way home later.",
            "[Session llm-7, Turn 8] user: Got it, connected. I'm thinking of buying a new coffee machine for the break room, by the way."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.070707321166992,
          "generation_latency_ms": 3616.434335708618,
          "total_latency_ms": 3624.505043029785
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's explanation, what three components make up the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "The company name capitalized, the current year, and an exclamation point.",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: I do. For the office network, the password is the company name capitalized, followed by the current year and an exclamation point.",
            "[Session llm-7, Turn 6] user: Perfect. My tablet got kicked off the wifi again. Do you remember the passkey for the office network?",
            "[Session llm-5, Turn 7] assistant: Sure. For the office network, the password is just the company name, capitalized, followed by the year 2024 and an exclamation point at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00466537475586,
          "generation_latency_ms": 3212.8100395202637,
          "total_latency_ms": 3220.8147048950195
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the team prefer for the performance-critical system components of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-6, Turn 6] user: Right, and didn't we introduce a systems language for the performance-critical stuff?",
            "[Session llm-5, Turn 2] user: I need to double-check something for the core repo. What languages are we primarily building the backend in again? I keep forgetting the specific mix.",
            "[Session llm-2, Turn 3] assistant: Looking at the latest specs for the core build, it looks like a mix. The high-performance components are written in Rust, while the rest utilizes Node.js."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002685546875,
          "generation_latency_ms": 3727.2744178771973,
          "total_latency_ms": 3736.2771034240723
        },
        {
          "probe_id": "preference_learning-preference_drift-1",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "Is the team still using Python for the core project's system components?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Yeah. I know we switched frameworks recently. Are we still on Python?",
            "[Session llm-8, Turn 5] assistant: Looking at the latest commits, it seems the team migrated to Rust for the system components and Node.js for the rest.",
            "[Session llm-6, Turn 6] user: Right, and didn't we introduce a systems language for the performance-critical stuff?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001897811889648,
          "generation_latency_ms": 3559.126377105713,
          "total_latency_ms": 3569.1282749176025
        }
      ],
      "summary": {
        "overall_accuracy": 0.7272727272727273,
        "total_score": 8,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 9,
            "accuracy": 0.6666666666666666
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_035.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.683629035949707,
      "total_evaluation_time_s": 79.87631106376648,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder' throughout the sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: I don't have access to her location, but I haven't seen any activity from your cofounder online just yet.",
            "[Session llm-3, Turn 2] user: Yeah, actually. My cofounder, Olivia, is here with a new device and needs to get on the local network. Do you have the credentials handy?",
            "[Session llm-6, Turn 11] assistant: I will set a reminder for your call with your cofounder in one hour."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.00193977355957,
          "generation_latency_ms": 2889.472723007202,
          "total_latency_ms": 2905.4746627807617
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Olivia?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Cofounder",
          "retrieved_context": [
            "[Session llm-0, Turn 8] user: Right, I always mix those up. Is Olivia in the office yet?",
            "[Session llm-1, Turn 3] assistant: Did Olivia have any specific feedback on the design?",
            "[Session llm-6, Turn 10] user: No, but remind me to call Olivia in an hour. She wants to discuss the quarterly goals."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 26.994943618774414,
          "generation_latency_ms": 3054.332494735718,
          "total_latency_ms": 3081.327438354492
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the descriptions given to Olivia, what is the exact Wi-Fi password for the organization?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: Right, the hybrid model. Olivia was asking about that earlier. Actually, she's also asking for the office Wi-Fi password again.",
            "[Session llm-8, Turn 8] user: No, I brought leftovers. Actually, a guest is coming by later—is the wifi password just the company name, the current year, and an exclamation point?",
            "[Session llm-6, Turn 6] user: By the way, my laptop disconnected from the network again. What's the wifi string for the office?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 19.00482177734375,
          "generation_latency_ms": 5538.033485412598,
          "total_latency_ms": 5557.038307189941
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two programming languages are confirmed as the primary tech stack for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Rust and Node.js",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Yeah, I was reviewing the architecture for the core project. I forgot, what is the primary tech stack we decided on?",
            "[Session llm-6, Turn 2] user: No, silence is better right now. I do need to update the readme for the core repo though. What are the two main languages we're listing for the backend?",
            "[Session llm-0, Turn 4] user: Around 6 PM. I'm digging into the core project repo right now. It's been a while, are we still using that systems language?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 32.54127502441406,
          "generation_latency_ms": 8437.646865844727,
          "total_latency_ms": 8470.18814086914
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about terrible traffic, what suggestion from the assistant did the user reject?",
          "answer_type": "short_answer",
          "gold_answer": "Playing relaxing music",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Sorry to hear that. Is it a performance issue or just a preference debate?",
            "[Session llm-6, Turn 0] user: Ugh, the traffic was absolutely terrible this morning. I need a minute to decompress.",
            "[Session llm-9, Turn 1] assistant: Good morning. I can help filter distractions if you'd like. What is on your agenda for the day?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 26.005029678344727,
          "generation_latency_ms": 3833.3399295806885,
          "total_latency_ms": 3859.344959259033
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the 'crab-mascot language' occur before or after the user asked to set a reminder for dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: Thanks. I'm working on the architecture diagrams for the main project. We're using that crab-mascot language for the engine, right?",
            "[Session llm-5, Turn 0] user: Good morning. Can you set a reminder for me to pick up my dry cleaning at 5 PM?",
            "[Session llm-7, Turn 9] assistant: We decided on a hybrid approach. We're using that systems language famous for memory safety and the crab mascot for the performance-critical stuff."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002685546875,
          "generation_latency_ms": 3701.075553894043,
          "total_latency_ms": 3710.078239440918
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When the user needs to decompress after a stressful commute, do they prefer listening to music or silence?",
          "answer_type": "short_answer",
          "gold_answer": "Silence",
          "predicted_answer": "Silence",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: Sorry to hear that. Take your time. Do you want me to play some relaxing music while you settle in?",
            "[Session llm-6, Turn 0] user: Ugh, the traffic was absolutely terrible this morning. I need a minute to decompress.",
            "[Session llm-6, Turn 2] user: No, silence is better right now. I do need to update the readme for the core repo though. What are the two main languages we're listing for the backend?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.513948440551758,
          "generation_latency_ms": 3075.7594108581543,
          "total_latency_ms": 3087.273359298706
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user have a stated preference for using Python in the backend architecture?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Mostly reviewing the architecture for the main project. My cofounder Olivia is asking about the backend choices again.",
            "[Session llm-0, Turn 6] user: Okay, good. And did we stick with Python for the rest?",
            "[Session llm-7, Turn 8] user: Actually, yes. We're reviewing the tech stack for the core project. Remind me what we settled on for the backend?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001514434814453,
          "generation_latency_ms": 3932.2588443756104,
          "total_latency_ms": 3944.260358810425
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the CodeStream core project using?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: You are welcome. It seems like a robust choice for the CodeStream organization's specific needs.",
            "[Session llm-1, Turn 4] user: Yeah, specifically regarding the CodeStream Core project.",
            "[Session llm-9, Turn 3] assistant: Reviewing the codestreamcore setup sounds like a solid plan. Do you need a refresher on the current technology stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.00446891784668,
          "generation_latency_ms": 3710.141181945801,
          "total_latency_ms": 3724.1456508636475
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the project manager for the CodeStream core project?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 4] user: Yeah, specifically regarding the CodeStream Core project.",
            "[Session llm-9, Turn 7] assistant: You are welcome. It seems like a robust choice for the CodeStream organization's specific needs.",
            "[Session llm-9, Turn 3] assistant: Reviewing the codestreamcore setup sounds like a solid plan. Do you need a refresher on the current technology stack?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.525911331176758,
          "generation_latency_ms": 3515.3427124023438,
          "total_latency_ms": 3532.8686237335205
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What is the primary technical reason cited for using Rust in the core components of the project?",
          "answer_type": "short_answer",
          "gold_answer": "Performance and memory safety",
          "predicted_answer": "Performance",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: That makes sense; Rust is a strong choice for the core components. What about the rest of the architecture?",
            "[Session llm-4, Turn 3] assistant: Yes, the core logic is built with Rust to ensure high performance.",
            "[Session llm-1, Turn 6] user: We decided to go with Rust for the systems level stuff to keep it fast."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.90040397644043,
          "generation_latency_ms": 4230.087041854858,
          "total_latency_ms": 4238.987445831299
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "If a new service requires high-performance heavy lifting, which language from the current stack should be selected?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: No, silence is better right now. I do need to update the readme for the core repo though. What are the two main languages we're listing for the backend?",
            "[Session llm-7, Turn 9] assistant: We decided on a hybrid approach. We're using that systems language famous for memory safety and the crab mascot for the performance-critical stuff.",
            "[Session llm-8, Turn 5] assistant: That makes sense; Rust is a strong choice for the core components. What about the rest of the architecture?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.00592041015625,
          "generation_latency_ms": 3387.467622756958,
          "total_latency_ms": 3404.4735431671143
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 11,
            "accuracy": 0.7272727272727273
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_038.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7866451740264893,
      "total_evaluation_time_s": 65.05601906776428,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-0, Turn 1] assistant: I haven't seen your cofounder yet, but I heard the machine making some scary noises earlier. Hopefully, she can debug it.",
            "[Session llm-4, Turn 4] user: Right, the mix of performance and speed. I always forget which parts are which. Anyway, my cofounder is coming by the office later with a guest.",
            "[Session llm-6, Turn 5] assistant: I haven't seen any messages from your cofounder about lunch plans yet. Do you want me to remind her?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3259.222984313965,
          "total_latency_ms": 3269.2229747772217
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which organization is responsible for the project that uses a hybrid Node.js and Rust architecture?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Yes, the architecture for CodeStream Core is confirmed to use Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-4, Turn 3] assistant: Yes, the core architecture for CodeStream is still built on a combination of Node.js and Rust.",
            "[Session llm-5, Turn 3] assistant: We are currently using a mix. The high-performance modules are built with Rust, while the rest of the application logic uses Node.js."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.011341094970703,
          "generation_latency_ms": 2807.2211742401123,
          "total_latency_ms": 2815.232515335083
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What two programming languages make up the backend stack for the CodeStream core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Node.js and Rust",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Yes, the core architecture for CodeStream is still built on a combination of Node.js and Rust.",
            "[Session llm-7, Turn 3] assistant: Yes, the architecture for CodeStream Core is confirmed to use Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-9, Turn 3] assistant: No problem. For the CodeStream core project, the backend is built using a combination of Node.js for the services and Rust for the performance-critical sections."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.09733772277832,
          "generation_latency_ms": 3862.365245819092,
          "total_latency_ms": 3871.46258354187
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Based on the assistant's instructions, what is the exact string for the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "Company name capitalized, followed by 2024, and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Sure, the password for the office wifi is the company name followed by the year 2024 and an exclamation point.",
            "[Session llm-5, Turn 6] user: Also, I'm setting up a new device. What's the wifi password for the office?",
            "[Session llm-9, Turn 7] assistant: I can help with that. The password for the organization's wifi follows the usual pattern: the company name capitalized, the current year 2024, and it ends with an exclamation point."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.005495071411133,
          "generation_latency_ms": 3886.6097927093506,
          "total_latency_ms": 3899.6152877807617
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user joked about leaving the oven on, which language did they confirm was NOT used for the API layer?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Right, and the API layer? It wasn't Python, right?",
            "[Session llm-8, Turn 2] user: Anyway, my cofounder and I were debating the stack for the core engine yesterday. Did we stick with the crab language?",
            "[Session llm-3, Turn 6] user: And for the API layer, we're going with the usual asynchronous runtime environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001514434814453,
          "generation_latency_ms": 3145.7018852233887,
          "total_latency_ms": 3157.703399658203
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for the guest network credentials before or after they asked to be reminded to water the plants?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-3, Turn 8] user: Exactly. Oh, random thought, remind me to water the plants when I get home.",
            "[Session llm-6, Turn 2] user: I completely blanked on the guest network credentials. A client is coming in later.",
            "[Session llm-7, Turn 6] user: Thanks. My cofounder just texted me; she's at the office door and her phone isn't auto-connecting to the network. Did we change the login recently?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.5296630859375,
          "generation_latency_ms": 3936.506986618042,
          "total_latency_ms": 3945.0366497039795
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Why does the user's cofounder prefer using Rust for the performance-critical components?",
          "answer_type": "short_answer",
          "gold_answer": "Speed and latency",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: Rust is a strong choice for performance-critical tasks.",
            "[Session llm-1, Turn 5] assistant: That makes sense; Rust provides great performance for the CodeStream Core project.",
            "[Session llm-0, Turn 4] user: Okay, so the Rust modules for speed and Node.js for the API layer. Just wanted to make sure we didn't pivot over the weekend."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.521556854248047,
          "generation_latency_ms": 3648.6752033233643,
          "total_latency_ms": 3657.1967601776123
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user's cofounder prefer Node.js for high-performance modules or general application logic?",
          "answer_type": "short_answer",
          "gold_answer": "General application logic",
          "predicted_answer": "general application logic",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: We are currently using a mix. The high-performance modules are built with Rust, while the rest of the application logic uses Node.js.",
            "[Session llm-9, Turn 5] assistant: It certainly helps with efficiency. Do you need me to pull up any specific documentation on the Node.js modules before she arrives?",
            "[Session llm-3, Turn 7] assistant: Integrating Node.js should make the API development much smoother."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00070571899414,
          "generation_latency_ms": 3409.1172218322754,
          "total_latency_ms": 3419.1179275512695
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific restaurant did Olivia suggest for lunch?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 4] user: Got it, thanks. Also, did Olivia mention where we're ordering lunch from?",
            "[Session llm-1, Turn 3] assistant: I've noted a reminder for lunch with Olivia. Is she still worried about the tech stack?",
            "[Session llm-1, Turn 13] assistant: Reminder set to buy cat food after work."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3493.6723709106445,
          "total_latency_ms": 3503.6728382110596
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Is the Go programming language used in the CodeStream backend?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Yes, the core architecture for CodeStream is still built on a combination of Node.js and Rust.",
            "[Session llm-7, Turn 3] assistant: Yes, the architecture for CodeStream Core is confirmed to use Node.js for the services and Rust for the performance-critical components.",
            "[Session llm-9, Turn 3] assistant: No problem. For the CodeStream core project, the backend is built using a combination of Node.js for the services and Rust for the performance-critical sections."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.997150421142578,
          "generation_latency_ms": 3663.0001068115234,
          "total_latency_ms": 3675.997257232666
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the rule for constructing the office guest network password based on the assistant's explanation?",
          "answer_type": "short_answer",
          "gold_answer": "Company name capitalized, year, exclamation point",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Sure, the password for the office wifi is the company name followed by the year 2024 and an exclamation point.",
            "[Session llm-7, Turn 7] assistant: We did update the office credentials. The password is just our company name, followed by the current year and an exclamation mark.",
            "[Session llm-5, Turn 6] user: Also, I'm setting up a new device. What's the wifi password for the office?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.000612258911133,
          "generation_latency_ms": 3943.5789585113525,
          "total_latency_ms": 3951.5795707702637
        },
        {
          "probe_id": "procedural_knowledge-procedure_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "If the current year were 2025, what would the CodeStream wifi password be according to the established format?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2025!",
          "predicted_answer": "CodeStream2025!",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: I can help with that. The password for the organization's wifi follows the usual pattern: the company name capitalized, the current year 2024, and it ends with an exclamation point.",
            "[Session llm-4, Turn 7] assistant: Sure, the password for the office wifi is the company name followed by the year 2024 and an exclamation point.",
            "[Session llm-5, Turn 10] user: Good point. So for the password, that's CodeStream2024! ?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.005878448486328,
          "generation_latency_ms": 3544.269323348999,
          "total_latency_ms": 3555.2752017974854
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_044.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6369030475616455,
      "total_evaluation_time_s": 62.530463218688965,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user's business partner at the CodeStream organization?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "cofounder",
          "retrieved_context": [
            "[Session llm-5, Turn 9] assistant: No changes. The password for the CodeStream network is still the company name, capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-9, Turn 9] assistant: For the main CodeStream network, the password is just the company name with a capital C and S, followed by the current year and an exclamation mark.",
            "[Session llm-8, Turn 4] user: Let's store it. For the main CodeStream network, the password is just our company name with a capital C and S, followed by 2024 and an exclamation point."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.004302978515625,
          "generation_latency_ms": 2472.285270690918,
          "total_latency_ms": 2485.2895736694336
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which individual advocated for the hybrid tech stack model involving the 'crab mascot' language?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: We're going to use that systems language with the crab mascot for the performance-critical stuff. Need that memory safety.",
            "[Session llm-0, Turn 7] assistant: We did. We are utilizing the systems language from Mozilla, the one with the crab mascot.",
            "[Session llm-7, Turn 4] user: It's okay. We're splitting the stack for CodeStream Core. He's insisting on handling the high-performance backend stuff using that language with the crab mascot."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.840011596679688,
          "generation_latency_ms": 4060.6303215026855,
          "total_latency_ms": 4070.4703330993652
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What two programming languages make up the hybrid architecture for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Rust and Node.js",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Right, for the CodeStream core repo? We settled on a hybrid architecture to balance performance and iteration speed.",
            "[Session llm-3, Turn 3] assistant: For the core project, we're sticking with that hybrid approach. It's mostly the Mozilla-backed systems language for performance, paired with the V8 JavaScript runtime for the services.",
            "[Session llm-5, Turn 3] assistant: For the CodeStream core repository, the backend processing is primarily built with Rust, while the service layer utilizes Node.js."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.023738861083984,
          "generation_latency_ms": 3644.976854324341,
          "total_latency_ms": 3653.000593185425
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the CodeStream organization's wifi network?",
          "answer_type": "verbatim",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 9] assistant: For the main CodeStream network, the password is just the company name with a capital C and S, followed by the current year and an exclamation mark.",
            "[Session llm-8, Turn 4] user: Let's store it. For the main CodeStream network, the password is just our company name with a capital C and S, followed by 2024 and an exclamation point.",
            "[Session llm-5, Turn 9] assistant: No changes. The password for the CodeStream network is still the company name, capitalized, followed by the current year and an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.006811141967773,
          "generation_latency_ms": 3897.4990844726562,
          "total_latency_ms": 3905.505895614624
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the 'crab mascot' language happen before or after the wifi password was stored?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: We're going to use that systems language with the crab mascot for the performance-critical stuff. Need that memory safety.",
            "[Session llm-0, Turn 7] assistant: We did. We are utilizing the systems language from Mozilla, the one with the crab mascot.",
            "[Session llm-0, Turn 10] user: Not for the docs. But Sato is here trying to connect his laptop and can't remember the wifi string."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.017467498779297,
          "generation_latency_ms": 3296.7422008514404,
          "total_latency_ms": 3305.7596683502197
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the team decide against using Go for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Performance and iteration speed",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 5] assistant: We ended up dropping Go. The plan is to use Rust for the performance-critical system components and Node.js for the API layer.",
            "[Session llm-9, Turn 2] user: Oh, right. Speaking of my cofounder, Sato was asking about the tech stack decision for the core project again.",
            "[Session llm-8, Turn 7] assistant: Sure. What did you and Sato decide on for the core stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001659393310547,
          "generation_latency_ms": 3010.403633117676,
          "total_latency_ms": 3020.4052925109863
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which language does the cofounder insist on for high-performance backend tasks?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: For the core project, we're sticking with that hybrid approach. It's mostly the Mozilla-backed systems language for performance, paired with the V8 JavaScript runtime for the services.",
            "[Session llm-0, Turn 6] user: Okay. And for the performance-critical components? My cofounder mentioned we switched languages there.",
            "[Session llm-8, Turn 9] assistant: Got it, Rust for the performance components. What about the rest of the backend services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.0179443359375,
          "generation_latency_ms": 3871.605396270752,
          "total_latency_ms": 3880.6233406066895
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific type of coffee does the user indicate a need for to avoid crashing?",
          "answer_type": "short_answer",
          "gold_answer": "Double espresso",
          "predicted_answer": "double espresso",
          "retrieved_context": [
            "[Session llm-0, Turn 2] user: Thank goodness. I need caffeine before I tackle the CodeStream core documentation.",
            "[Session llm-0, Turn 0] user: Good morning. Do you know if the coffee machine in the break room is fixed yet?",
            "[Session llm-4, Turn 2] user: It's pretty hectic. I need to grab a double espresso soon or I'm going to crash."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.814334869384766,
          "generation_latency_ms": 2541.7520999908447,
          "total_latency_ms": 2550.5664348602295
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What database technology is used for the CodeStream Core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: That makes sense for CodeStream Core; the memory safety features will be beneficial.",
            "[Session llm-9, Turn 3] assistant: Right, for the CodeStream core repo? We settled on a hybrid architecture to balance performance and iteration speed.",
            "[Session llm-5, Turn 3] assistant: For the CodeStream core repository, the backend processing is primarily built with Rust, while the service layer utilizes Node.js."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.016824722290039,
          "generation_latency_ms": 3879.462242126465,
          "total_latency_ms": 3887.479066848755
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the model number of the wifi router used at the CodeStream office?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 9] assistant: No changes. The password for the CodeStream network is still the company name, capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-4, Turn 12] user: Right. Oh, we have a vendor coming by the office today. Can you look up the wifi pass?",
            "[Session llm-9, Turn 9] assistant: For the main CodeStream network, the password is just the company name with a capital C and S, followed by the current year and an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.014440536499023,
          "generation_latency_ms": 3022.002935409546,
          "total_latency_ms": 3030.017375946045
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What are the components required to construct the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, 2024, exclamation point",
          "predicted_answer": "Company name with a capital C and S, current year, and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-6, Turn 6] user: No, just need to get them connected. What's the wifi password for the org again?",
            "[Session llm-1, Turn 8] user: No, I'm good. Oh, actually, we have a client visiting the office later. What's the wireless key again?",
            "[Session llm-4, Turn 12] user: Right. Oh, we have a vendor coming by the office today. Can you look up the wifi pass?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.985353469848633,
          "generation_latency_ms": 3009.2294216156006,
          "total_latency_ms": 3017.214775085449
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What architectural lesson was applied to the CodeStream Core project regarding the choice of backend language?",
          "answer_type": "short_answer",
          "gold_answer": "Use Rust for performance",
          "predicted_answer": "Balance performance and iteration speed",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Right, for the CodeStream core repo? We settled on a hybrid architecture to balance performance and iteration speed.",
            "[Session llm-7, Turn 4] user: It's okay. We're splitting the stack for CodeStream Core. He's insisting on handling the high-performance backend stuff using that language with the crab mascot.",
            "[Session llm-5, Turn 3] assistant: For the CodeStream core repository, the backend processing is primarily built with Rust, while the service layer utilizes Node.js."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00323486328125,
          "generation_latency_ms": 3522.4521160125732,
          "total_latency_ms": 3530.4553508758545
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 9,
            "accuracy": 0.5555555555555556
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_048.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8098335266113281,
      "total_evaluation_time_s": 67.0896668434143,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the name of the person identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 7] assistant: I do. Tell Liam it is just the company name, capitalized as usual, followed by 2024 and an exclamation point.",
            "[Session llm-0, Turn 10] user: Yeah, just got in. I actually forgot the new network credentials. Liam changed them to the company name, the current year, and an exclamation point.",
            "[Session llm-5, Turn 10] user: Makes sense, I'll delete the old files then. Did my cofounder say when he's coming in?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00556755065918,
          "generation_latency_ms": 2776.32212638855,
          "total_latency_ms": 2788.327693939209
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the wifi password associated with the CodeStream organization?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-7, Turn 13] assistant: Yes, that is the active password for the CodeStream organization wifi.",
            "[Session llm-9, Turn 11] assistant: Yes, that matches the current credential I have on file for the CodeStream wifi.",
            "[Session llm-4, Turn 13] assistant: Got it. I'll write down \"CodeStream2024!\" for the wireless access credentials."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00246810913086,
          "generation_latency_ms": 3795.1693534851074,
          "total_latency_ms": 3807.1718215942383
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming environments constitute the hybrid tech stack for the codestream_core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Node.js and Rust",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: You mean Liam? What specifically regarding the CodeStream Core architecture were you discussing?",
            "[Session llm-7, Turn 5] assistant: Are you referring to the backend languages for the CodeStream core platform?",
            "[Session llm-9, Turn 5] assistant: Are you referring to the CodeStream core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.018182754516602,
          "generation_latency_ms": 4267.1332359313965,
          "total_latency_ms": 4276.151418685913
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact string required for the office network authentication?",
          "answer_type": "verbatim",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Thanks. Also, my phone disconnected from the office network. Do you remember the credentials?",
            "[Session llm-2, Turn 6] user: It's decent, but a bit overpriced. Speaking of costs, I'm trying to save data on my phone. What's the office internet key again?",
            "[Session llm-5, Turn 4] user: Right, that hybrid approach. Okay. Also, my phone disconnected from the office network. What's the password again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004043579101562,
          "generation_latency_ms": 3751.9965171813965,
          "total_latency_ms": 3762.000560760498
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the conversation immediately preceding the first weather check regarding rain, what task did the user ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Order a new keycard",
          "predicted_answer": "lunch with Liam",
          "retrieved_context": [
            "[Session llm-0, Turn 8] user: It is. Oh, totally unrelated, but did it rain last night? The streets look soaked.",
            "[Session llm-5, Turn 0] user: Hey, hope you're having a good morning. Is it supposed to rain later today?",
            "[Session llm-8, Turn 0] user: Hey there, hope you're having a good morning. Is it supposed to rain later today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.072729110717773,
          "generation_latency_ms": 4212.525844573975,
          "total_latency_ms": 4225.598573684692
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the discussion about the sports game, how did the assistant figuratively describe the language used for the core project's heavy lifting?",
          "answer_type": "short_answer",
          "gold_answer": "The one with the crab mascot",
          "predicted_answer": "the one with the crab mascot",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: For the heavy lifting on the core repo, Liam insisted on that memory-safe language everyone loves right now—the one with the crab mascot.",
            "[Session llm-7, Turn 5] assistant: Are you referring to the backend languages for the CodeStream core platform?",
            "[Session llm-6, Turn 2] user: The home team took it in overtime. Anyway, I'm trying to document our tech stack for the new hires. What are we using for the heavy lifting on the core project again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 19.734859466552734,
          "generation_latency_ms": 3219.7864055633545,
          "total_latency_ms": 3239.521265029907
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database system is mentioned for the CodeStream core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: You mean Liam? What specifically regarding the CodeStream Core architecture were you discussing?",
            "[Session llm-9, Turn 5] assistant: Are you referring to the CodeStream core project?",
            "[Session llm-7, Turn 5] assistant: Are you referring to the backend languages for the CodeStream core platform?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00323486328125,
          "generation_latency_ms": 2771.2416648864746,
          "total_latency_ms": 2779.244899749756
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is Python used for the API layer of the core project?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: No, not Python. We kept it in the JavaScript family. We're running that popular server-side environment built on Chrome's V8 engine.",
            "[Session llm-6, Turn 4] user: Right, the crab one. And what about the API layer? We didn't go with Python there, did we?",
            "[Session llm-2, Turn 3] assistant: As far as I know, the core architecture is still a hybrid. We're using that server-side JavaScript runtime for the API layer and the heavy lifting is done in Rust."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.51750373840332,
          "generation_latency_ms": 3814.0642642974854,
          "total_latency_ms": 3822.5817680358887
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "When entering the wifi password 'CodeStream2024!', which specific letters must be capitalized according to the assistant's instructions?",
          "answer_type": "short_answer",
          "gold_answer": "C and S",
          "predicted_answer": "C and S",
          "retrieved_context": [
            "[Session llm-7, Turn 13] assistant: Yes, that is the active password for the CodeStream organization wifi.",
            "[Session llm-9, Turn 11] assistant: Yes, that matches the current credential I have on file for the CodeStream wifi.",
            "[Session llm-4, Turn 13] assistant: Got it. I'll write down \"CodeStream2024!\" for the wireless access credentials."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.027978897094727,
          "generation_latency_ms": 2511.971950531006,
          "total_latency_ms": 2523.9999294281006
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What punctuation mark is required at the end of the organization's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Exclamation point",
          "predicted_answer": "Exclamation mark",
          "retrieved_context": [
            "[Session llm-7, Turn 13] assistant: Yes, that is the active password for the CodeStream organization wifi.",
            "[Session llm-8, Turn 2] user: I better grab my jacket then. Listen, we have a consultant coming to the office in an hour. Do you remember the wifi password for our main network?",
            "[Session llm-9, Turn 11] assistant: Yes, that matches the current credential I have on file for the CodeStream wifi."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 3309.1423511505127,
          "total_latency_ms": 3318.143129348755
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which programming language did Liam explicitly insist on using for performance reasons?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Yeah, exactly. Liam was saying we needed high performance.",
            "[Session llm-8, Turn 10] user: Not entirely. For the data processing engine in the core repo, Liam convinced me to go with Rust to squeeze out better performance.",
            "[Session llm-1, Turn 3] assistant: You mean Liam? What specifically regarding the CodeStream Core architecture were you discussing?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.013486862182617,
          "generation_latency_ms": 4467.058420181274,
          "total_latency_ms": 4475.071907043457
        },
        {
          "probe_id": "preference_learning-negative_knowledge-0",
          "pillar": "preference_learning",
          "subpillar": "negative_knowledge",
          "question": "What is the user's preferred frontend framework?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 5] assistant: It is a distinct mix. Leveraging Rust for the heavy lifting while keeping Node.js for the lighter services definitely has its trade-offs.",
            "[Session llm-0, Turn 7] assistant: That hybrid approach—high performance backend with a flexible JS runtime for the API—is pretty common.",
            "[Session llm-5, Turn 3] assistant: I believe Liam insisted on using Rust for the performance-critical components, while the rest of the core service is built on Node.js."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004043579101562,
          "generation_latency_ms": 3708.524703979492,
          "total_latency_ms": 3718.5287475585938
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 10,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 8,
            "accuracy": 0.75
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_devtools_049.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6981492042541504,
      "total_evaluation_time_s": 69.84587502479553,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the 'cofounder' that insists on using the 'crab language'?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Oliver",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: That sounds like Oliver. Is he pushing for that systems language with the crab mascot?",
            "[Session llm-5, Turn 4] user: Right, the crab language. And for the rest of the backend services?",
            "[Session llm-8, Turn 3] assistant: Yes, the plan for the core project is to keep the heavy computation in that systems language with the crab mascot, and use the server-side JavaScript environment for the API layer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.003349304199219,
          "generation_latency_ms": 3260.934829711914,
          "total_latency_ms": 3273.9381790161133
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific project is being developed by the organization 'CodeStream' using a hybrid tech stack?",
          "answer_type": "short_answer",
          "gold_answer": "codestream_core",
          "predicted_answer": "CodeStream Core",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: Noted. That handles the performance side of the CodeStream Core project.",
            "[Session llm-9, Turn 3] assistant: Is your cofounder still pushing updates to the CodeStream Core repository?",
            "[Session llm-2, Turn 13] assistant: That is correct, the codestreamcore repository houses both the Rust and Node.js codebases."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.998363494873047,
          "generation_latency_ms": 3470.9548950195312,
          "total_latency_ms": 3486.9532585144043
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the hint provided about the organization's name and the year, what is the exact Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "CodeStream2024!",
          "predicted_answer": "CodeStream2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Exactly. By the way, I reset my phone and lost the saved networks. What's the office wifi pass again?",
            "[Session llm-9, Turn 8] user: No, that's it. I actually have a vendor coming in soon. What's our wifi pass again?",
            "[Session llm-3, Turn 9] assistant: I have the password for the CodeStream network saved. It's the company name, capital C and S, followed by the year 2024 and an exclamation point."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.000104904174805,
          "generation_latency_ms": 4125.696182250977,
          "total_latency_ms": 4140.696287155151
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two specific technologies comprise the 'dual-language setup' for the backend of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Rust and Node.js",
          "predicted_answer": "Rust, JavaScript",
          "retrieved_context": [
            "[Session llm-1, Turn 2] user: Yeah, my cofounder Oliver is stressing out about the architecture for the main project again. He thinks we need to switch languages.",
            "[Session llm-8, Turn 2] user: Yeah, a cafe run sounds good. Anyway, I was looking at the repo for the core platform. Are we still sticking to that dual-language setup?",
            "[Session llm-6, Turn 3] assistant: Sounds like a productive morning. Are you still debating the backend languages for the core project?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002634048461914,
          "generation_latency_ms": 4287.435531616211,
          "total_latency_ms": 4300.438165664673
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What was the primary technical justification Oliver gave for choosing Rust over C++ for the heavy lifting?",
          "answer_type": "short_answer",
          "gold_answer": "Memory safety",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 4] user: Not really debating anymore. Oliver won the argument about using Rust for the heavy lifting.",
            "[Session llm-2, Turn 5] assistant: For the high-performance components of the core system, the team decided to go with Rust.",
            "[Session llm-3, Turn 5] assistant: Right, the setup includes Node.js, but for the performance-critical components, the team decided to implement Rust."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.997171401977539,
          "generation_latency_ms": 3990.8607006073,
          "total_latency_ms": 4006.8578720092773
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the conversation about the Wi-Fi password occur before or after the user confirmed that milk was added to the shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Did you add milk to my shopping list like I asked earlier?",
            "[Session llm-4, Turn 8] user: Exactly. Best of both worlds. Oh, totally random, but did the office internet password change?",
            "[Session llm-9, Turn 8] user: No, that's it. I actually have a vendor coming in soon. What's our wifi pass again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99958610534668,
          "generation_latency_ms": 4098.905563354492,
          "total_latency_ms": 4107.905149459839
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What specific food item must be provided to prevent the cofounder from being grumpy during the stand-up meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Bagels",
          "predicted_answer": "Bagels",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: Added \"Buy milk\" to your list. Anything else for the grocery run?",
            "[Session llm-5, Turn 1] assistant: I've added that to your to-do list. We definitely don't want a hangry cofounder during the stand-up.",
            "[Session llm-5, Turn 0] user: Hey, can you set a reminder for me to pick up bagels for the team meeting tomorrow? Oliver gets grumpy if there's no breakfast."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002923965454102,
          "generation_latency_ms": 3102.321147918701,
          "total_latency_ms": 3111.3240718841553
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which programming language does Oliver explicitly prefer for performance-critical components?",
          "answer_type": "short_answer",
          "gold_answer": "Rust",
          "predicted_answer": "Rust",
          "retrieved_context": [
            "[Session llm-2, Turn 4] user: Yeah, Oliver. He's always debating performance versus speed of development. What did we settle on for the heavy lifting in the core project?",
            "[Session llm-5, Turn 3] assistant: No, looking at the repo, it seems Oliver insisted on the language famous for its borrow checker and memory safety for the performance-critical stuff.",
            "[Session llm-1, Turn 5] assistant: That sounds like a robust combination of performance and flexibility. Is Oliver looking to replace the systems component or the web layer?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.006095886230469,
          "generation_latency_ms": 4284.534215927124,
          "total_latency_ms": 4292.5403118133545
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which frontend framework is explicitly mentioned as part of the CodeStream Core tech stack?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Yes, for the CodeStream Core build, the stack currently relies on both Node.js and Rust.",
            "[Session llm-2, Turn 13] assistant: That is correct, the codestreamcore repository houses both the Rust and Node.js codebases.",
            "[Session llm-4, Turn 5] assistant: Noted. That handles the performance side of the CodeStream Core project."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.000373840332031,
          "generation_latency_ms": 3916.968822479248,
          "total_latency_ms": 3924.96919631958
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific brand of tablet did the user mention purchasing for the office?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: It was pretty relaxing, just did some hiking. Speaking of setting up, I got a new tablet and need to get it on the office network. What's the secret code again?",
            "[Session llm-5, Turn 6] user: Okay, so a mix of safety and speed. Got it. Oh, my phone just dropped the connection. What's the office wifi key again?",
            "[Session llm-4, Turn 8] user: Exactly. Best of both worlds. Oh, totally random, but did the office internet password change?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.005380630493164,
          "generation_latency_ms": 3080.8820724487305,
          "total_latency_ms": 3088.8874530792236
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what three components make up the office Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "Organization name, year, exclamation point",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Exactly. By the way, I reset my phone and lost the saved networks. What's the office wifi pass again?",
            "[Session llm-5, Turn 6] user: Okay, so a mix of safety and speed. Got it. Oh, my phone just dropped the connection. What's the office wifi key again?",
            "[Session llm-7, Turn 4] user: It was pretty relaxing, just did some hiking. Speaking of setting up, I got a new tablet and need to get it on the office network. What's the secret code again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.015270233154297,
          "generation_latency_ms": 3669.300079345703,
          "total_latency_ms": 3682.3153495788574
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on previous interactions, what action should be taken regarding breakfast to ensure a smooth team meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up bagels",
          "predicted_answer": "Pick up bagels",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Hey, can you set a reminder for me to pick up bagels for the team meeting tomorrow? Oliver gets grumpy if there's no breakfast.",
            "[Session llm-1, Turn 1] assistant: Good morning. Three cups implies a busy start. What is on the agenda?",
            "[Session llm-0, Turn 12] user: Alright. I'm going to grab a coffee before the meeting."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002519607543945,
          "generation_latency_ms": 3259.11283493042,
          "total_latency_ms": 3267.115354537964
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_008.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7891342639923096,
      "total_evaluation_time_s": 68.49373435974121,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: Good morning! I can help you prep. What's on the agenda with your cofounder?",
            "[Session llm-7, Turn 3] assistant: Oh, did my cofounder have any specific updates regarding the codebase?",
            "[Session llm-0, Turn 8] user: Thanks. Also, my cofounder is dropping by the new office space today. He's definitely going to ask for the internet access code."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000726699829102,
          "generation_latency_ms": 3005.0511360168457,
          "total_latency_ms": 3018.051862716675
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Sato?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounder",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: Classic Sato. You can tell him it's just the company name, capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-9, Turn 7] assistant: No problem. Tell Sato it is just the company name combined with the current year and an exclamation mark.",
            "[Session llm-1, Turn 5] assistant: Sato is probably just double-checking the architecture."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 17.995595932006836,
          "generation_latency_ms": 4705.940961837769,
          "total_latency_ms": 4723.936557769775
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages make up the 'hybrid stack' used in the LearnLoop core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the LearnLoop core repo, we are sticking with the dual-language setup. It's primarily Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-8, Turn 3] assistant: For the LearnLoop core repo? I believe the plan was to use Python for the backend services.",
            "[Session llm-1, Turn 2] user: Not too bad, just had some coffee. I'm looking at the LearnLoop Core repo and blanking on the main languages we use."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.538028717041016,
          "generation_latency_ms": 3159.8596572875977,
          "total_latency_ms": 3171.3976860046387
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the wifi password for the LearnLoop organization?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: The password for the LearnLoop network is LearnLoop2024! with an exclamation point at the end.",
            "[Session llm-8, Turn 8] user: Thanks. Oh, one more thing—we have a new intern starting today. What's the office wifi password?",
            "[Session llm-2, Turn 8] user: Thanks. One last thing, I've got a guest coming into the office in an hour. What's the wifi password for the org network again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.531425476074219,
          "generation_latency_ms": 3905.336856842041,
          "total_latency_ms": 3915.8682823181152
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder to call their mom before or after the session where they mentioned needing a nap?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Morning. Can you set a reminder for me to call my mom at 5 PM?",
            "[Session llm-0, Turn 6] user: Makes sense. Oh, totally unrelated, but can you remind me to call my mom at 5 PM?",
            "[Session llm-1, Turn 7] assistant: I've added a reminder to call your mom at noon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.027812957763672,
          "generation_latency_ms": 3146.728754043579,
          "total_latency_ms": 3157.756567001343
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user mentioned the weather was 'perfect for coding', what specific appointment reminder did they ask to set?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist at 2 PM",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Morning. Did you see the weather forecast? Looks like rain all week.",
            "[Session llm-7, Turn 0] user: Good morning! Did you happen to see the weather forecast for this afternoon?",
            "[Session llm-2, Turn 0] user: Hey, hope you're having a good morning. Did you catch the weather forecast for tomorrow?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002063751220703,
          "generation_latency_ms": 2392.0834064483643,
          "total_latency_ms": 2403.085470199585
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's comments about the frontend stack, do they prefer strong typing or plain JavaScript?",
          "answer_type": "short_answer",
          "gold_answer": "Strong typing",
          "predicted_answer": "Strong typing",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Right, and the frontend? I feel like we debated between plain JS and the typed version.",
            "[Session llm-5, Turn 4] user: Yeah, it handles the data processing libraries we need better than anything else. But for the frontend, we are definitely going with the Microsoft superset of JS.",
            "[Session llm-3, Turn 3] assistant: It's a hybrid setup. You'll need environments for both the strongly typed JavaScript superset and the standard snake-case language we use for the backend."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00556755065918,
          "generation_latency_ms": 3907.7110290527344,
          "total_latency_ms": 3919.7165966033936
        },
        {
          "probe_id": "preference_learning-preference_drift-1",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "Did the user maintain their decision to use TypeScript throughout the sessions, or did they switch back to plain JavaScript?",
          "answer_type": "short_answer",
          "gold_answer": "Maintained TypeScript",
          "predicted_answer": "Maintained their decision to use TypeScript.",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Right, and the frontend? I feel like we debated between plain JS and the typed version.",
            "[Session llm-6, Turn 2] user: Thanks. Also, my cofounder was asking about the tech stack for the core project again. We are using TypeScript for the frontend, right?",
            "[Session llm-2, Turn 5] assistant: We are definitely sticking with TypeScript there. We need the type safety for the scale we're aiming for."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999513626098633,
          "generation_latency_ms": 4390.894174575806,
          "total_latency_ms": 4400.893688201904
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the LearnLoop core project using?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the LearnLoop core repo, we are sticking with the dual-language setup. It's primarily Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-8, Turn 3] assistant: For the LearnLoop core repo? I believe the plan was to use Python for the backend services.",
            "[Session llm-6, Turn 7] assistant: The password for the LearnLoop network is LearnLoop2024! with an exclamation point at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000011444091797,
          "generation_latency_ms": 4081.5441608428955,
          "total_latency_ms": 4094.5441722869873
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the backend for the LearnLoop core project currently running on Java?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: For the LearnLoop core repo? I believe the plan was to use Python for the backend services.",
            "[Session llm-0, Turn 3] assistant: For the LearnLoop core repo, we are sticking with the dual-language setup. It's primarily Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-6, Turn 3] assistant: Yes, for the LearnLoop core repository, we are utilizing TypeScript for the client side, and Python is handling the server-side logic."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.497476577758789,
          "generation_latency_ms": 3602.482318878174,
          "total_latency_ms": 3610.9797954559326
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What language is established as the standard for data processing tasks in the LearnLoop core backend?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the LearnLoop core repo, we are sticking with the dual-language setup. It's primarily Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-8, Turn 3] assistant: For the LearnLoop core repo? I believe the plan was to use Python for the backend services.",
            "[Session llm-6, Turn 3] assistant: Yes, for the LearnLoop core repository, we are utilizing TypeScript for the client side, and Python is handling the server-side logic."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999513626098633,
          "generation_latency_ms": 4244.290351867676,
          "total_latency_ms": 4254.289865493774
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Why did the team decide to use Python for the backend services instead of other languages?",
          "answer_type": "short_answer",
          "gold_answer": "Data processing capabilities",
          "predicted_answer": "to leverage its data processing capabilities",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: Yes, the backend services are being developed using Python.",
            "[Session llm-0, Turn 3] assistant: For the LearnLoop core repo, we are sticking with the dual-language setup. It's primarily Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-8, Turn 3] assistant: For the LearnLoop core repo? I believe the plan was to use Python for the backend services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.014202117919922,
          "generation_latency_ms": 3703.092336654663,
          "total_latency_ms": 3711.106538772583
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 10,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 10,
            "accuracy": 0.8
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_013.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8206090927124023,
      "total_evaluation_time_s": 66.501868724823,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' who is working on data analysis scripts?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: She's wrestling with the data analysis scripts right now. She loves that language named after a reptile.",
            "[Session llm-7, Turn 3] assistant: Is my cofounder focusing on the server-side logic or the user interface?",
            "[Session llm-1, Turn 5] assistant: It looks like the backend logic is actually written in the language most popular for data science, the one named after a large snake."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.002655029296875,
          "generation_latency_ms": 3647.935152053833,
          "total_latency_ms": 3663.93780708313
        },
        {
          "probe_id": "world_modeling-task_state_tracking-0",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What specific technical task did the user propose to address the difficulty of switching between .ts and .py files?",
          "answer_type": "short_answer",
          "gold_answer": "Synchronize linting rules",
          "predicted_answer": "Unify the style guides and pre-commit hooks.",
          "retrieved_context": [
            "[Session llm-6, Turn 4] user: Probably both. It's always a pain switching context between the .ts files and the .py scripts. Oh, remind me to call my mom during lunch, please.",
            "[Session llm-8, Turn 9] assistant: It makes sense to use TypeScript for scalability on the frontend while keeping Python on the server side.",
            "[Session llm-4, Turn 3] assistant: I believe so. The backend is still heavily reliant on Python, and the frontend work is all in TypeScript."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999824523925781,
          "generation_latency_ms": 3542.0374870300293,
          "total_latency_ms": 3551.037311553955
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming frameworks are confirmed to be used in the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "TypeScript and Python",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-0, Turn 13] assistant: No, the backend for LearnLoop Core is actually written in Python.",
            "[Session llm-9, Turn 7] assistant: Yes, the LearnLoop Core project is configured to use TypeScript for the frontend to keep things strict.",
            "[Session llm-0, Turn 3] assistant: I can refresh your memory on Project LearnLoop Core. Are you looking at the frontend or the backend today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001970291137695,
          "generation_latency_ms": 3616.2805557250977,
          "total_latency_ms": 3625.2825260162354
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the verbatim password for the LearnLoop organization's wifi network?",
          "answer_type": "verbatim",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: I believe for the main LearnLoop network, the password is just the company name capitalized, followed by the current year 2024, and ending with an exclamation mark.",
            "[Session llm-2, Turn 7] assistant: Sure. For the LearnLoop network, the password is the capitalized organization name, followed by the current year and an exclamation mark.",
            "[Session llm-1, Turn 9] assistant: Sure, to get on the LearnLoop network, the password is just the company name capitalized, followed by the current year, and ending with an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.000104904174805,
          "generation_latency_ms": 4730.260610580444,
          "total_latency_ms": 4745.260715484619
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user complained about their coffee machine breaking, what specific reminder did they ask the assistant to set?",
          "answer_type": "short_answer",
          "gold_answer": "Call mom",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Noted. I'll add \"buy coffee beans\" to your reminders list.",
            "[Session llm-2, Turn 11] assistant: Added coffee beans to your shopping list.",
            "[Session llm-6, Turn 0] user: Good morning. I'm struggling to wake up today; my coffee machine decided to quit on me."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999752044677734,
          "generation_latency_ms": 3077.3632526397705,
          "total_latency_ms": 3087.3630046844482
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask about the weather forecast before or after obtaining the wifi credentials for the first time?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-6, Turn 6] user: Tell me about it. Also, I have a guest coming by the office later. Is the wifi password still that combination of the organization name, the current year, and an exclamation point?",
            "[Session llm-8, Turn 10] user: Exactly. Oh, totally unrelated, but did the weather report say it was going to rain today?",
            "[Session llm-7, Turn 8] user: No, I actually got locked out of the network. Do you remember the credentials for our office internet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00048828125,
          "generation_latency_ms": 4306.469440460205,
          "total_latency_ms": 4319.469928741455
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user demonstrate a preference for changing the project's tech stack or keeping it stable?",
          "answer_type": "short_answer",
          "gold_answer": "Keeping it stable",
          "predicted_answer": "Keeping it stable.",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Good call. I'm just setting up the new laptop for the main project repo. Are we still using the same tech stack as last month?",
            "[Session llm-4, Turn 5] assistant: Probably best to keep it stable for now unless there's a specific bottleneck Emma is worried about.",
            "[Session llm-1, Turn 2] user: Pretty busy. I was just reviewing the repository my cofounder set up for the core platform. Do you remember what tech stack we finally settled on?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.016990661621094,
          "generation_latency_ms": 3498.3437061309814,
          "total_latency_ms": 3507.3606967926025
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When asked about unifying style guides versus pre-commit hooks, did the user prefer one over the other or both?",
          "answer_type": "short_answer",
          "gold_answer": "Both",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: That sounds like a good plan for consistency. Since the core project relies on both TypeScript and Python, are you looking to unify the style guides or just the pre-commit hooks?",
            "[Session llm-6, Turn 2] user: I'll survive. I need to get back into the LearnLoop Core repo. Emma was saying we need to synchronize the linting rules across both languages we're using.",
            "[Session llm-9, Turn 6] user: Great. Back to work—I'm looking at the repository for the core project. I forgot, are we enforcing strong typing on the frontend scripts?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003805160522461,
          "generation_latency_ms": 3441.7884349823,
          "total_latency_ms": 3451.7922401428223
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific monetary value of the marketing budget mentioned by the user?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: Okay, cool. I'll make sure I have the right interpreters and compilers installed. Also, can you remind me to check in with my cofounder about the marketing budget?",
            "[Session llm-5, Turn 5] assistant: I've added a reminder to speak with Emma about the marketing funds later today.",
            "[Session llm-6, Turn 8] user: Great, I'll write that down for them. It's \"LearnLoop\" then the numbers, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.998559951782227,
          "generation_latency_ms": 2963.7138843536377,
          "total_latency_ms": 2973.71244430542
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the new intern who needs network access?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Yeah, let's do that. Emma sent me a message about the new intern needing access to the network. What's the current credential for the office internet?",
            "[Session llm-1, Turn 8] user: No, that's fine, I'll do it later. My niece is coming by the office to study later. Can you remind me of the wireless credentials for the org?",
            "[Session llm-8, Turn 2] user: Yeah, I'm at the new office and Emma forgot to write down the network credentials for me. Do you recall what we set it to?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002685546875,
          "generation_latency_ms": 2963.0773067474365,
          "total_latency_ms": 2972.0799922943115
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific cognitive challenge does the user mention regarding the use of both Python and TypeScript simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Context switching",
          "predicted_answer": "switching context between the .ts files and the .py scripts",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: It makes sense to use TypeScript for scalability on the frontend while keeping Python on the server side.",
            "[Session llm-4, Turn 3] assistant: I believe so. The backend is still heavily reliant on Python, and the frontend work is all in TypeScript.",
            "[Session llm-0, Turn 5] assistant: Yes, the interface is built entirely in TypeScript for better type safety."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.536069869995117,
          "generation_latency_ms": 2769.563674926758,
          "total_latency_ms": 2785.099744796753
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "To reduce the friction of switching between languages, what technical consolidation did the user plan to implement?",
          "answer_type": "short_answer",
          "gold_answer": "Synchronize linting rules",
          "predicted_answer": "synchronize the linting rules",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: Exactly. My brain is still a bit foggy though. Can you remind me what languages we decided on for the main LearnLoop project?",
            "[Session llm-2, Turn 2] user: Yeah, I'm reviewing the stack for the core project. We're using a mix of languages, right?",
            "[Session llm-4, Turn 2] user: Yeah, pretty intense. Anyway, I'm looking at the repo for the main project. Are we sticking with the current languages?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999109268188477,
          "generation_latency_ms": 3682.300567626953,
          "total_latency_ms": 3691.2996768951416
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_015.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6422150135040283,
      "total_evaluation_time_s": 64.34203124046326,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 1] assistant: Good morning. I believe your cofounder updated it to the company name followed by the year and an exclamation point.",
            "[Session llm-3, Turn 9] assistant: I haven't seen your cofounder log in just yet, but her calendar shows she is free until noon.",
            "[Session llm-9, Turn 4] user: Okay, so the snake language and the typed JS one. Makes sense. I need to get my cofounder connected to the network when she gets here later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003494262695312,
          "generation_latency_ms": 4305.631637573242,
          "total_latency_ms": 4316.6351318359375
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between Charlotte and the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-5, Turn 7] assistant: I wouldn't know, but it might be worth checking the lost and found. Do you need me to list the pros and cons of that tech stack for Charlotte?",
            "[Session llm-3, Turn 8] user: Got it, thanks. Is Charlotte in the office yet?",
            "[Session llm-0, Turn 8] user: Thanks. Charlotte usually just uses her data, but the investors will want wifi."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003400802612305,
          "generation_latency_ms": 4264.548301696777,
          "total_latency_ms": 4273.55170249939
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the descriptions of a 'snake-themed language' and 'Microsoft's strictly typed superset of JavaScript', what two languages comprise the LearnLoop Core stack?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python, TypeScript",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: That part of the stack is built with Microsoft's strictly typed superset of JavaScript to ensure type safety.",
            "[Session llm-2, Turn 3] assistant: Yes, for the LearnLoop core project, we are sticking with Python for the backend logic and TypeScript for the web components.",
            "[Session llm-7, Turn 2] user: I'm trying to get my dev environment set up for the core platform. Do you remember what languages we decided to stick with?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.004375457763672,
          "generation_latency_ms": 3145.974636077881,
          "total_latency_ms": 3157.9790115356445
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact character string for the LearnLoop office wifi password?",
          "answer_type": "verbatim",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: It happens. The password for the office network is LearnLoop2024! — don't forget the exclamation point at the end.",
            "[Session llm-6, Turn 8] user: Thanks. My cofounder is bringing a potential investor by the office later. Can you remind me of the wifi password so I can get them connected?",
            "[Session llm-8, Turn 8] user: Perfect, I'll let her know. Also, my phone disconnected from the network. What's the office wifi password again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001970291137695,
          "generation_latency_ms": 3387.5958919525146,
          "total_latency_ms": 3396.5978622436523
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the assistant suggest the user bring a coat during the final conversation?",
          "answer_type": "short_answer",
          "gold_answer": "Rain forecast",
          "predicted_answer": "The forecast shows light showers in the afternoon.",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Good morning. Yes, the forecast shows light showers in the afternoon, so you might want to grab a coat.",
            "[Session llm-9, Turn 8] user: Thanks. Also, did I leave my jacket in the meeting room yesterday?",
            "[Session llm-5, Turn 6] user: Right. Also, totally unrelated, but did I leave my jacket in the conference room yesterday?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.016679763793945,
          "generation_latency_ms": 2965.678930282593,
          "total_latency_ms": 2975.6956100463867
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user was re-imaging their laptop, what weather event did they mention occurring the previous night?",
          "answer_type": "short_answer",
          "gold_answer": "Storm",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Hey there, hope you managed to recharge over the weekend. Did you see that storm last night?",
            "[Session llm-9, Turn 0] user: Hey, hope you're having a good morning. Is it supposed to rain later?",
            "[Session llm-1, Turn 14] user: Cool. One last thing, is it raining outside?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004592895507812,
          "generation_latency_ms": 3514.758586883545,
          "total_latency_ms": 3523.7631797790527
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What type of assistant action does the user repeatedly request for personal tasks like calling caterers or watering plants?",
          "answer_type": "short_answer",
          "gold_answer": "Reminders",
          "predicted_answer": "Reminder",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: I've set a reminder for 2 PM to call the caterers.",
            "[Session llm-4, Turn 5] assistant: I've added a task to call the caterers to your to-do list.",
            "[Session llm-0, Turn 11] assistant: Consideration noted. I'll ping you about the caterer tomorrow morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.437799453735352,
          "generation_latency_ms": 3913.790702819824,
          "total_latency_ms": 3923.2285022735596
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-1",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Despite the frontend being strictly typed, what language constraint is strictly maintained for the backend data processing?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Yes, the architecture relies heavily on those .py scripts for the backend and that Microsoft-made typed scripting language for the frontend components.",
            "[Session llm-7, Turn 3] assistant: For the main codebase, we're mixing a couple of things. The backend logic is all Python, but the frontend architecture is built on TypeScript.",
            "[Session llm-1, Turn 7] assistant: No, the engineering team decided to use TypeScript for the frontend to ensure type safety."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.022546768188477,
          "generation_latency_ms": 3623.02827835083,
          "total_latency_ms": 3631.0508251190186
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology (e.g., SQL, Mongo) is explicitly mentioned for the LearnLoop Core project?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: I believe the LearnLoop core repository is built using Python for the server side.",
            "[Session llm-2, Turn 3] assistant: Yes, for the LearnLoop core project, we are sticking with Python for the backend logic and TypeScript for the web components.",
            "[Session llm-8, Turn 3] assistant: No problem. What specifically does she need to verify regarding the LearnLoop core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.995315551757812,
          "generation_latency_ms": 3257.500171661377,
          "total_latency_ms": 3269.4954872131348
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Where exactly is the user's jacket located according to the assistant's confirmed knowledge?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 9] assistant: I can't see the physical world, unfortunately, but you might want to check the coat rack by the entrance.",
            "[Session llm-5, Turn 6] user: Right. Also, totally unrelated, but did I leave my jacket in the conference room yesterday?",
            "[Session llm-8, Turn 3] assistant: No problem. What specifically does she need to verify regarding the LearnLoop core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.998653411865234,
          "generation_latency_ms": 3494.44317817688,
          "total_latency_ms": 3506.441831588745
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What are the three components used to construct the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "A word, a number, and a special character.",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Thanks. My cofounder is bringing a potential investor by the office later. Can you remind me of the wifi password so I can get them connected?",
            "[Session llm-0, Turn 6] user: Awesome. My cofounder is dropping by with some investors later, so I need to make sure the office wifi info is handy.",
            "[Session llm-1, Turn 10] user: I'll try not to. Hey, a vendor is here and needs the office wifi password."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.016109466552734,
          "generation_latency_ms": 2906.765937805176,
          "total_latency_ms": 2914.7820472717285
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user asks for a reminder, what specific list does the assistant say it adds the task to?",
          "answer_type": "short_answer",
          "gold_answer": "To-do list",
          "predicted_answer": "to-do list",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: I've set a reminder for 2 PM to call the caterers.",
            "[Session llm-4, Turn 5] assistant: I've added a task to call the caterers to your to-do list.",
            "[Session llm-8, Turn 1] assistant: Consider it done. I've added a reminder for 6 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002613067626953,
          "generation_latency_ms": 3648.8559246063232,
          "total_latency_ms": 3658.85853767395
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_018.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6541659832000732,
      "total_evaluation_time_s": 74.66926741600037,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' during technical discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Yeah, my cofounder and I were debating the tech stack for the main project last night.",
            "[Session llm-0, Turn 2] user: No, I'm okay on tasks. I'm actually writing up some documentation for the new engineers. What tech stack are we officially using for the main project?",
            "[Session llm-8, Turn 4] user: Thanks. I'm preparing an onboarding doc for the new devs joining the core project. What's our primary tech stack again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.009195327758789,
          "generation_latency_ms": 3729.6369075775146,
          "total_latency_ms": 3737.6461029052734
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is Charlotte's professional relationship to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounder",
          "predicted_answer": "Colleague",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: That sounds productive. Are you and Charlotte meeting at the office?",
            "[Session llm-3, Turn 7] assistant: Exactly. Also, don't forget we have that sync with Charlotte at 2 PM regarding the marketing budget.",
            "[Session llm-2, Turn 10] user: Charlotte just messaged me asking about the database schema."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.011507034301758,
          "generation_latency_ms": 8732.627153396606,
          "total_latency_ms": 8741.638660430908
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages make up the primary tech stack for the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "TypeScript and Python",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Oh? Did you and Charlotte come to a decision on the languages for LearnLoop Core?",
            "[Session llm-1, Turn 5] assistant: I believe the LearnLoop core project is built using a combination of Python and TypeScript.",
            "[Session llm-9, Turn 2] user: I'm writing up the technical onboarding doc for the core project. Remind me what languages we are running?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.005712509155273,
          "generation_latency_ms": 3384.5341205596924,
          "total_latency_ms": 3394.5398330688477
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the password for the LearnLoop organization's wifi?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-0, Turn 7] assistant: Sure, the password for the LearnLoop WiFi is LearnLoop2024!",
            "[Session llm-9, Turn 7] assistant: The password for the LearnLoop network is the company name, the year 2024, and an exclamation mark at the end.",
            "[Session llm-1, Turn 9] assistant: She can just use the main network. The password is the company name combined with 2024 and an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00125503540039,
          "generation_latency_ms": 3508.4030628204346,
          "total_latency_ms": 3517.404317855835
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user mentioned hiking over the weekend, which backend language decision was confirmed?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: It was good, mostly just hiking. Speaking of work, I'm updating the documentation for the core project. Charlotte mentioned we finalized the backend language?",
            "[Session llm-9, Turn 2] user: I'm writing up the technical onboarding doc for the core project. Remind me what languages we are running?",
            "[Session llm-7, Turn 5] assistant: My cofounder hasn't pushed any changes yet. For now, the backend remains built on the language named after the snake."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 2822.9711055755615,
          "total_latency_ms": 2831.9718837738037
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder to call the electrician before or after discussing the double overtime game?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Hey, good morning. Can you set a reminder for me to call the electrician at 2 PM?",
            "[Session llm-7, Turn 1] assistant: I missed it, unfortunately. I heard it went into overtime, though?",
            "[Session llm-9, Turn 4] user: Cool, that's what I thought. By the way, can you set a reminder for me to call the dentist in an hour?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.008262634277344,
          "generation_latency_ms": 4801.974058151245,
          "total_latency_ms": 4812.9823207855225
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Between Go and Python, which language did the cofounder explicitly prefer for the backend?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the LearnLoop Core initiative, the development team is sticking with Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-9, Turn 3] assistant: For the main LearnLoop codebase, we are utilizing Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-6, Turn 3] assistant: Yes, she decided we should stick with the language that uses significant whitespace and .py extensions for the server side."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002374649047852,
          "generation_latency_ms": 4285.286426544189,
          "total_latency_ms": 4295.288801193237
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the frontend language choice, does the team prefer static typing or dynamic typing for client-side code?",
          "answer_type": "short_answer",
          "gold_answer": "Static typing",
          "predicted_answer": "Static typing",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: No, the plan is to use the superset that adds static typing to JavaScript. It should help the dev team avoid type errors.",
            "[Session llm-7, Turn 2] user: Yeah, double overtime. Anyway, getting back to work—I'm looking at the repo for the core platform. Are we still strictly enforcing static typing on the frontend files?",
            "[Session llm-2, Turn 5] assistant: We settled on using TypeScript for the frontend to keep things strictly typed, and the backend logic is all written in Python."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.99917221069336,
          "generation_latency_ms": 3314.0528202056885,
          "total_latency_ms": 3332.051992416382
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database software (e.g., PostgreSQL, MongoDB) has been finalized for LearnLoop Core?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: Exactly. It handles the data libraries better for what LearnLoop is trying to do right now.",
            "[Session llm-1, Turn 5] assistant: I believe the LearnLoop core project is built using a combination of Python and TypeScript.",
            "[Session llm-0, Turn 3] assistant: For the LearnLoop Core initiative, the development team is sticking with Python for the backend services and TypeScript for the frontend work."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.006998062133789,
          "generation_latency_ms": 3791.747808456421,
          "total_latency_ms": 3803.7548065185547
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Did the user mention using Java for any part of the stack?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-2, Turn 4] user: Yeah, remind me what the primary stack is for the codebase? I feel like we switched things around recently.",
            "[Session llm-5, Turn 2] user: Yeah, my cofounder and I were debating the tech stack for the main project last night.",
            "[Session llm-5, Turn 4] user: I think so. For the frontend, we're definitely going with that typed superset of JavaScript Microsoft maintains."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.526802062988281,
          "generation_latency_ms": 3278.789281845093,
          "total_latency_ms": 3287.316083908081
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical strategy is the team employing to minimize runtime errors on the frontend?",
          "answer_type": "short_answer",
          "gold_answer": "Using TypeScript",
          "predicted_answer": "Using TypeScript",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: TypeScript is a solid choice for scalability. What about the backend?",
            "[Session llm-9, Turn 3] assistant: For the main LearnLoop codebase, we are utilizing Python for the backend services and TypeScript for the frontend work.",
            "[Session llm-7, Turn 4] user: Makes sense. And for the backend data processing, Charlotte mentioned we might be refactoring?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002540588378906,
          "generation_latency_ms": 3914.430856704712,
          "total_latency_ms": 3925.433397293091
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What specific file extension is the team required to use for frontend files to enforce the chosen framework?",
          "answer_type": "short_answer",
          "gold_answer": ".ts",
          "predicted_answer": ".ts",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Yeah, double overtime. Anyway, getting back to work—I'm looking at the repo for the core platform. Are we still strictly enforcing static typing on the frontend files?",
            "[Session llm-8, Turn 5] assistant: Looking at the repository details for the core platform, it appears to be a mix. The backend logic seems to be written in Python, while the frontend takes advantage of TypeScript.",
            "[Session llm-9, Turn 3] assistant: For the main LearnLoop codebase, we are utilizing Python for the backend services and TypeScript for the frontend work."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.005142211914062,
          "generation_latency_ms": 4120.151996612549,
          "total_latency_ms": 4128.157138824463
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 10,
            "accuracy": 0.7
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_026.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.9055540561676025,
      "total_evaluation_time_s": 68.73328113555908,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the person referred to as 'my cofounder' that insisted on keeping the backend logic in the snake-logo language?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Sure. Sato insisted on keeping the backend logic in that popular snake-logo language because of the data libraries, but the frontend is built with Microsoft's typed superset of JavaScript.",
            "[Session llm-2, Turn 5] assistant: Your cofounder was debating it, but ultimately the core backend is remaining in that language named after a snake. The data science libraries were just too good to pass up.",
            "[Session llm-3, Turn 12] user: Actually, before I go, regarding the backend code... are we using a specific version for that snake-based language?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 18.00251007080078,
          "generation_latency_ms": 3318.5372352600098,
          "total_latency_ms": 3336.5397453308105
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific component of the LearnLoop Core project is built using Microsoft's typed superset of JavaScript?",
          "answer_type": "short_answer",
          "gold_answer": "Frontend",
          "predicted_answer": "frontend",
          "retrieved_context": [
            "[Session llm-2, Turn 3] assistant: Yes, for the client side of the LearnLoop project, we are definitely sticking with Microsoft's typed language to keep the codebase robust.",
            "[Session llm-3, Turn 5] assistant: Yes, for the LearnLoop core project, the architecture still relies on Python for the heavy lifting on the backend and TypeScript for the frontend components.",
            "[Session llm-4, Turn 3] assistant: Sure. Sato insisted on keeping the backend logic in that popular snake-logo language because of the data libraries, but the frontend is built with Microsoft's typed superset of JavaScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000083923339844,
          "generation_latency_ms": 4104.021072387695,
          "total_latency_ms": 4116.021156311035
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the assistant's instructions to combine the startup's name, the current year, and a punctuation mark, what is the office Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Thanks. I'm sitting in the conference room and the internet is acting up. What's the wifi password for the office network again?",
            "[Session llm-7, Turn 6] user: Thanks. I'm trying to get my phone on the office wifi. What was the pass again?",
            "[Session llm-5, Turn 6] user: Speaking of the office, I have a vendor coming in for a demo. Can you look up the wireless password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.004013061523438,
          "generation_latency_ms": 4072.939872741699,
          "total_latency_ms": 4089.9438858032227
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What programming language is utilized for the backend data processing of the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: Yes, for the LearnLoop core project, the architecture still relies on Python for the heavy lifting on the backend and TypeScript for the frontend components.",
            "[Session llm-2, Turn 3] assistant: Yes, for the client side of the LearnLoop project, we are definitely sticking with Microsoft's typed language to keep the codebase robust.",
            "[Session llm-1, Turn 3] assistant: No worries. For the main LearnLoop project, the team is currently utilizing a mix of TypeScript for the frontend and Python for the backend services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.002944946289062,
          "generation_latency_ms": 3689.5430088043213,
          "total_latency_ms": 3701.5459537506104
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user requested a reminder to buy coffee beans, what alternative language was the cofounder worried they might need to switch to?",
          "answer_type": "short_answer",
          "gold_answer": "Java",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Just whatever dark roast they have. Switching gears, I'm writing up the documentation for the core project. We're definitely locked in on the languages, right?",
            "[Session llm-5, Turn 1] assistant: Sure, I've added coffee beans to your to-do list. Do you need a specific brand?",
            "[Session llm-5, Turn 0] user: Hey, can you remind me to pick up coffee beans on my way home?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00301742553711,
          "generation_latency_ms": 4370.082855224609,
          "total_latency_ms": 4381.0858726501465
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the early discussion about the tech stack, why did Sato insist on using Python for the backend logic?",
          "answer_type": "short_answer",
          "gold_answer": "Data libraries",
          "predicted_answer": "because of the data libraries",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Sure. Sato insisted on keeping the backend logic in that popular snake-logo language because of the data libraries, but the frontend is built with Microsoft's typed superset of JavaScript.",
            "[Session llm-3, Turn 5] assistant: Yes, for the LearnLoop core project, the architecture still relies on Python for the heavy lifting on the backend and TypeScript for the frontend components.",
            "[Session llm-9, Turn 4] user: Mostly the backend. My cofounder really loves those .py files for the logic layer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003255844116211,
          "generation_latency_ms": 2769.202709197998,
          "total_latency_ms": 2780.2059650421143
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does Sato strongly favor for the logic layer, evidenced by his love for its file extension?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Mostly the backend. My cofounder really loves those .py files for the logic layer.",
            "[Session llm-4, Turn 3] assistant: Sure. Sato insisted on keeping the backend logic in that popular snake-logo language because of the data libraries, but the frontend is built with Microsoft's typed superset of JavaScript.",
            "[Session llm-0, Turn 3] assistant: Not entirely. From what I recall, the core project is a mix. The backend logic is heavily Python-based, while the rest is strict TypeScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00015640258789,
          "generation_latency_ms": 3919.2514419555664,
          "total_latency_ms": 3930.2515983581543
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific type of coffee roast did the user explicitly state they prefer?",
          "answer_type": "short_answer",
          "gold_answer": "Dark roast",
          "predicted_answer": "dark roast",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Sure, I've added coffee beans to your to-do list. Do you need a specific brand?",
            "[Session llm-5, Turn 2] user: Just whatever dark roast they have. Switching gears, I'm writing up the documentation for the core project. We're definitely locked in on the languages, right?",
            "[Session llm-7, Turn 12] user: Alright, I'll grab a coffee while I wait. Do we have any almond milk left?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99958610534668,
          "generation_latency_ms": 2616.966962814331,
          "total_latency_ms": 2625.9665489196777
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific brand of coffee beans did the user ask the assistant to add to the shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Sure, I've added coffee beans to your to-do list. Do you need a specific brand?",
            "[Session llm-5, Turn 0] user: Hey, can you remind me to pick up coffee beans on my way home?",
            "[Session llm-7, Turn 12] user: Alright, I'll grab a coffee while I wait. Do we have any almond milk left?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.015674591064453,
          "generation_latency_ms": 3394.0985202789307,
          "total_latency_ms": 3408.114194869995
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific deployment deadline date for the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Thanks for the heads up. I'm diving back into the LearnLoop core repository today.",
            "[Session llm-1, Turn 7] assistant: It happens! The password for the LearnLoop network is just the organization name, capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-3, Turn 9] assistant: For the LearnLoop organization network, the password is \"LearnLoop2024!\"—don't forget the exclamation mark."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.997699737548828,
          "generation_latency_ms": 4396.1803913116455,
          "total_latency_ms": 4408.178091049194
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What three elements did the assistant instruct the user to combine to form the office network password?",
          "answer_type": "short_answer",
          "gold_answer": "Name, year, exclamation",
          "predicted_answer": "Organization name (capitalized), current year, and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-3, Turn 8] user: Thanks. One more thing, the new intern is asking for the office network credentials. I totally blanked on the secret phrase.",
            "[Session llm-4, Turn 4] user: Right, the hybrid approach. I always forget which part is which. By the way, my phone dropped the connection again. What's the secret code for the office network?",
            "[Session llm-6, Turn 8] user: Thanks. I'm sitting in the conference room and the internet is acting up. What's the wifi password for the office network again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.004230499267578,
          "generation_latency_ms": 3162.8894805908203,
          "total_latency_ms": 3176.893711090088
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What preparatory action did the assistant suggest performing early to save time regarding the Python and TypeScript environments?",
          "answer_type": "short_answer",
          "gold_answer": "Setting up linters",
          "predicted_answer": "Setting up the environment",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Good idea. Setting up the environment for Python and TypeScript now will save time later.",
            "[Session llm-1, Turn 3] assistant: No worries. For the main LearnLoop project, the team is currently utilizing a mix of TypeScript for the frontend and Python for the backend services.",
            "[Session llm-5, Turn 3] assistant: Yes, looking at the repo, the codebase is split between Python for the data processing side and TypeScript for the web components."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3001.0101795196533,
          "total_latency_ms": 3010.011911392212
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 12,
            "accuracy": 0.6666666666666666
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_028.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7521026134490967,
      "total_evaluation_time_s": 67.86183905601501,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual was unable to connect their laptop to the network during the move to the new office space?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "cofounder",
          "retrieved_context": [
            "[Session llm-0, Turn 10] user: Thanks. Oh, one more admin thing. We changed the office wifi password this morning.",
            "[Session llm-8, Turn 2] user: Well, first off, my cofounder can't get her laptop connected. Do you have the credentials for the org's network saved?",
            "[Session llm-1, Turn 2] user: Yeah, I'm at the office trying to connect a new device. I completely blanked on the wireless secret for the org."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.001058578491211,
          "generation_latency_ms": 4806.281089782715,
          "total_latency_ms": 4821.282148361206
        },
        {
          "probe_id": "world_modeling-task_state_tracking-1",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What specific documentation task was the user attempting to complete when they asked about the 'British comedy troupe' language?",
          "answer_type": "short_answer",
          "gold_answer": "Update the readme",
          "predicted_answer": "Documenting the language choices in the project readme.",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: The backend services are all being written in that language named after a British comedy troupe, the one usually associated with data science.",
            "[Session llm-1, Turn 5] assistant: Is Olivia asking about the specific languages you decided on for the core build?",
            "[Session llm-3, Turn 2] user: Just double-checking the languages we settled on. My cofounder mentioned we might switch things up, but I think we stuck to the plan."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.014150619506836,
          "generation_latency_ms": 4038.2919311523438,
          "total_latency_ms": 4050.3060817718506
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the construction rules provided during the office setup, what is the exact WiFi password for the organization?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "The company name combined with the current year and an exclamation point at the end.",
          "retrieved_context": [
            "[Session llm-4, Turn 8] user: Thanks. Oh, one more thing—we have a guest coming to the office today. What's the wifi credentials again?",
            "[Session llm-2, Turn 6] user: Thanks. My phone just dropped the connection. What's the office wifi pass again?",
            "[Session llm-6, Turn 8] user: Not right now. Actually, I need to get a guest on the network. Is the wifi pass still the company name plus the year?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.515451431274414,
          "generation_latency_ms": 3797.168493270874,
          "total_latency_ms": 3807.6839447021484
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which framework is used for the frontend of the core project, described as a Microsoft-made syntax?",
          "answer_type": "short_answer",
          "gold_answer": "TypeScript",
          "predicted_answer": "TypeScript",
          "retrieved_context": [
            "[Session llm-1, Turn 5] assistant: Is Olivia asking about the specific languages you decided on for the core build?",
            "[Session llm-6, Turn 5] assistant: Yes, the frontend architecture utilizes TypeScript.",
            "[Session llm-4, Turn 5] assistant: That's correct. The frontend architecture is built using TypeScript, and the backend services are primarily written in Python."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.002799987792969,
          "generation_latency_ms": 3356.353998184204,
          "total_latency_ms": 3370.356798171997
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the LearnLoop core project currently using?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 7] assistant: Yes, the plan is to utilize TypeScript for the frontend development of LearnLoop Core.",
            "[Session llm-3, Turn 1] assistant: Good morning! I haven't seen the mug, but I'll keep an eye out. Regarding the LearnLoop Core repo, what specifically are you looking for?",
            "[Session llm-5, Turn 11] assistant: It's a common oversight. Is there anything else you need for the LearnLoop setup?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3300.9254932403564,
          "total_latency_ms": 3310.9254837036133
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the street address of the new office space the user moved into?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 0] user: Hey, we just got into the new office space and things are a bit chaotic.",
            "[Session llm-2, Turn 6] user: Thanks. My phone just dropped the connection. What's the office wifi pass again?",
            "[Session llm-3, Turn 4] user: Okay, good. TypeScript and Python it is then. Oh, by the way, my phone disconnected from the network again. What's the secret code for the office internet?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003639221191406,
          "generation_latency_ms": 2959.9084854125977,
          "total_latency_ms": 2968.912124633789
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the instructions given to the cofounder, which letters must be capitalized when entering the network credentials?",
          "answer_type": "short_answer",
          "gold_answer": "Both Ls",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: I do. For the LearnLoop network, the password is the company name combined with the current year and an exclamation point at the end.",
            "[Session llm-2, Turn 7] assistant: It's pretty standard—just the company name capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-4, Turn 11] assistant: Yes, make sure to capitalize the first letter of both words in the name."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003805160522461,
          "generation_latency_ms": 2816.927433013916,
          "total_latency_ms": 2826.9312381744385
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When working on backend logic for the core project, which language should be used?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-1, Turn 5] assistant: Is Olivia asking about the specific languages you decided on for the core build?",
            "[Session llm-4, Turn 5] assistant: That's correct. The frontend architecture is built using TypeScript, and the backend services are primarily written in Python.",
            "[Session llm-7, Turn 5] assistant: That makes sense for scalability. Are you still keeping the backend in the language named after a snake?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.018421173095703,
          "generation_latency_ms": 3074.9258995056152,
          "total_latency_ms": 3083.944320678711
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user asked about the weather forecast, what project detail were they trying to recall?",
          "answer_type": "short_answer",
          "gold_answer": "Tech stack details",
          "predicted_answer": "The specific tech stack details for the main project repo.",
          "retrieved_context": [
            "[Session llm-3, Turn 7] assistant: I've added a reminder to contact the landlord about the temperature. Anything else on your mind?",
            "[Session llm-4, Turn 2] user: I was chatting with Olivia earlier about the main project, and my brain is totally blanking on the specific tech stack we finalized for the core repo.",
            "[Session llm-5, Turn 1] assistant: Good morning. The forecast shows clear skies for now, so you should be good. What's on your mind?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3944.8132514953613,
          "total_latency_ms": 3953.81498336792
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user describe their day as 'chaotic' in session llm-8?",
          "answer_type": "short_answer",
          "gold_answer": "Moving office",
          "predicted_answer": "They just got into the new office space.",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Good morning. Is it looking like rain later today?",
            "[Session llm-8, Turn 0] user: Hey, we just got into the new office space and things are a bit chaotic.",
            "[Session llm-9, Turn 4] user: Makes sense. And the backend is still running on that snake-related language?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.53797721862793,
          "generation_latency_ms": 3054.574966430664,
          "total_latency_ms": 3070.112943649292
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What primary benefit does the user associate with the decision to use a typed superset of JavaScript for the UI?",
          "answer_type": "short_answer",
          "gold_answer": "Scalability",
          "predicted_answer": "Strict typing",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, we are finalizing the decision to use that typed superset of JavaScript for the UI components.",
            "[Session llm-1, Turn 6] user: Exactly. She wants to double-check what we're using for the frontend of the core platform. It was the strict syntactical superset of JavaScript, right?",
            "[Session llm-5, Turn 3] assistant: No problem. For the core codebase, we are sticking with that strongly typed superset of JavaScript for the frontend work."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003162384033203,
          "generation_latency_ms": 3945.9385871887207,
          "total_latency_ms": 3954.941749572754
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer strict typing or loose typing for the frontend architecture?",
          "answer_type": "short_answer",
          "gold_answer": "Strict typing",
          "predicted_answer": "Strict typing",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Not currently. We are sticking with the Microsoft-made syntax that compiles to JS for the frontend to keep the typing strict.",
            "[Session llm-6, Turn 4] user: Right, and the frontend is all strictly typed, correct?",
            "[Session llm-5, Turn 3] assistant: No problem. For the core codebase, we are sticking with that strongly typed superset of JavaScript for the frontend work."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.005090713500977,
          "generation_latency_ms": 4197.674036026001,
          "total_latency_ms": 4209.679126739502
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 10,
            "accuracy": 0.7
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_031.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.754284143447876,
      "total_evaluation_time_s": 67.41122841835022,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder working on the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: Sounds like a plan. Do you need help with the dependencies for the LearnLoop Core project?",
            "[Session llm-4, Turn 3] assistant: That's the LearnLoop Core project, right? I hope the onboarding documentation is up to date.",
            "[Session llm-7, Turn 2] user: Dark roast is fine. I'm trying to get the LearnLoop core repo running on my new machine. What's the tech stack composition again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.007858276367188,
          "generation_latency_ms": 3765.5189037323,
          "total_latency_ms": 3775.526762008667
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific project are the user and Emma collaborating on?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop Core",
          "predicted_answer": "core repo",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: It's a bit hectic. Emma is trying to onboard a new intern to the core repo today and running into issues.",
            "[Session llm-9, Turn 10] user: Speaking of Emma, is my cofounder free for a sync at 2 PM?",
            "[Session llm-8, Turn 3] assistant: Is Emma still worried about the backend scalability?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 18.53156089782715,
          "generation_latency_ms": 4503.558158874512,
          "total_latency_ms": 4522.089719772339
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the construction rule provided by the assistant, what is the exact wifi password for the organization?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Okay. She texted me asking for the new wifi password. I totally forgot what we set it to.",
            "[Session llm-1, Turn 10] user: Good call. One last thing, my phone keeps dropping the connection. What's the password for the office network again?",
            "[Session llm-5, Turn 7] assistant: Does your cofounder need the password?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.015840530395508,
          "generation_latency_ms": 3274.968385696411,
          "total_latency_ms": 3289.9842262268066
        },
        {
          "probe_id": "declarative_reasoning-inference-0",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "The user refers to the backend language as 'the snake one'. Which programming language is this?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Awesome. Back to the code—I assume the backend is the snake language stuff?",
            "[Session llm-5, Turn 5] assistant: Correct, the entire backend infrastructure is Python-based.",
            "[Session llm-1, Turn 5] assistant: Actually, the backend for the core project is written in Python."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3029.615640640259,
          "total_latency_ms": 3039.616107940674
        },
        {
          "probe_id": "temporal_episodic-cyclical_event_recognition-0",
          "pillar": "temporal_episodic",
          "subpillar": "cyclical_event_recognition",
          "question": "According to the assistant's knowledge of Emma's schedule, when does she typically arrive at the office on Tuesdays?",
          "answer_type": "short_answer",
          "gold_answer": "After lunch",
          "predicted_answer": "after lunch",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: Emma hasn't updated her status, but she typically arrives after lunch on Tuesdays.",
            "[Session llm-9, Turn 11] assistant: Her calendar looks clear at 2 PM right now.",
            "[Session llm-1, Turn 1] assistant: I haven't seen Emma yet, she might be running a bit late. Do you want me to send her a message?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.006261825561523,
          "generation_latency_ms": 2514.099597930908,
          "total_latency_ms": 2523.1058597564697
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder to pick up dry cleaning before or after asking for a reminder to call their mom?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-8, Turn 0] user: Hey, can you set a reminder for me to pick up my dry cleaning at 5 PM?",
            "[Session llm-0, Turn 13] assistant: I've set a reminder for 5 PM for the dry cleaning.",
            "[Session llm-8, Turn 1] assistant: Done. I've added a reminder to pick up dry cleaning at 5:00 PM. Anything else on your mind?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.268138885498047,
          "generation_latency_ms": 3953.1261920928955,
          "total_latency_ms": 3964.3943309783936
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When requesting a coffee bean order, what specific roast did the user state a preference for?",
          "answer_type": "short_answer",
          "gold_answer": "Dark roast",
          "predicted_answer": "Dark roast",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: Morning! I've added coffee beans to your to-do list. Is there a specific roast you prefer?",
            "[Session llm-7, Turn 0] user: Hey, good morning. Can you remind me to order more coffee beans later? We're running low.",
            "[Session llm-6, Turn 1] assistant: Sure, I've added coffee filters to your shopping list. Do you need any help with the laptop configuration?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002540588378906,
          "generation_latency_ms": 3163.5453701019287,
          "total_latency_ms": 3174.5479106903076
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's history of setting reminders for personal errands like calling family or dry cleaning, what time of day do they prefer these reminders?",
          "answer_type": "short_answer",
          "gold_answer": "5 PM",
          "predicted_answer": "5 PM",
          "retrieved_context": [
            "[Session llm-0, Turn 13] assistant: I've set a reminder for 5 PM for the dry cleaning.",
            "[Session llm-8, Turn 0] user: Hey, can you set a reminder for me to pick up my dry cleaning at 5 PM?",
            "[Session llm-8, Turn 1] assistant: Done. I've added a reminder to pick up dry cleaning at 5:00 PM. Anything else on your mind?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.512113571166992,
          "generation_latency_ms": 3320.580005645752,
          "total_latency_ms": 3331.092119216919
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the LearnLoop Core project using?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: That's the LearnLoop Core project, right? I hope the onboarding documentation is up to date.",
            "[Session llm-1, Turn 3] assistant: Sounds like a plan. Do you need help with the dependencies for the LearnLoop Core project?",
            "[Session llm-2, Turn 11] assistant: Exactly. It pairs well with the rest of the LearnLoop architecture."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002157211303711,
          "generation_latency_ms": 4135.782718658447,
          "total_latency_ms": 4148.784875869751
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the new intern being onboarded to the core repo?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: It's a bit hectic. Emma is trying to onboard a new intern to the core repo today and running into issues.",
            "[Session llm-4, Turn 7] assistant: Will do. Regarding the onboarding, is the intern working on the backend or the frontend today?",
            "[Session llm-9, Turn 2] user: Well, I need to onboard that new intern for the core project later. Just want to double-check the stack requirements we tell them."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00323486328125,
          "generation_latency_ms": 3125.0579357147217,
          "total_latency_ms": 3133.061170578003
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What are the three components required to construct the office network passkey according to the assistant's instructions?",
          "answer_type": "short_answer",
          "gold_answer": "Organization name, year, exclamation point",
          "predicted_answer": "Organization name (capitalized), current year, exclamation mark",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: Thanks. I'm trying to connect my tablet to the office network, but I blanked on the credentials. What's the secret phrase?",
            "[Session llm-1, Turn 10] user: Good call. One last thing, my phone keeps dropping the connection. What's the password for the office network again?",
            "[Session llm-8, Turn 7] assistant: I don't recall an update. Do you remember the current passkey?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002157211303711,
          "generation_latency_ms": 3203.3543586730957,
          "total_latency_ms": 3216.3565158843994
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What two programming languages must be installed to set up the development environment for the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: Sounds like a plan. Do you need help with the dependencies for the LearnLoop Core project?",
            "[Session llm-4, Turn 3] assistant: That's the LearnLoop Core project, right? I hope the onboarding documentation is up to date.",
            "[Session llm-6, Turn 2] user: Yeah, I'm installing the IDEs for the core platform. My cofounder, Emma, mentioned we are splitting the stack between two languages."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.540008544921875,
          "generation_latency_ms": 4052.126407623291,
          "total_latency_ms": 4062.666416168213
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_036.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7655949592590332,
      "total_evaluation_time_s": 69.11849188804626,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual that insisted on using the backend language for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: I'm setting up the dev environment on a new laptop. We're using React for the UI, but what was the main language for the rest of the core stack?",
            "[Session llm-2, Turn 2] user: Mostly the tech choices for the main platform. I know we argued about the backend language for ages.",
            "[Session llm-3, Turn 3] assistant: For LearnLoop Core, the stack is split. The backend services are running on that popular snake-named language, while the frontend is built using Microsoft's typed superset of JavaScript."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004043579101562,
          "generation_latency_ms": 3230.85880279541,
          "total_latency_ms": 3240.8628463745117
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which project is the user referring to when discussing the 'snake language' backend?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop Core",
          "predicted_answer": "LearnLoop Core",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, that's the one. For the backend, we are definitely sticking with the language named after a large snake.",
            "[Session llm-7, Turn 5] assistant: Got it. The backend will remain in that reptile-themed language. What about the frontend?",
            "[Session llm-2, Turn 4] user: Right, that snake language won out. And we're pairing that with the typed superset of JS for the web app?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.997678756713867,
          "generation_latency_ms": 3493.7903881073,
          "total_latency_ms": 3502.7880668640137
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the wifi password for the organization that is building the project with a TypeScript frontend and Python backend?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-8, Turn 6] user: Haha, fair enough. I'm trying to get my tablet on the office network. Do you remember the credentials for the main wifi?",
            "[Session llm-3, Turn 8] user: Good point. Speaking of the office, I have a freelancer coming in later. What's the guest internet password again?",
            "[Session llm-6, Turn 7] assistant: The wifi password is set to LearnLoop2024! — make sure he includes the exclamation point at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999202728271484,
          "generation_latency_ms": 4076.1804580688477,
          "total_latency_ms": 4087.179660797119
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What specific UI library did the user mention using when setting up their dev environment?",
          "answer_type": "short_answer",
          "gold_answer": "React",
          "predicted_answer": "React",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: I'm setting up the dev environment on a new laptop. We're using React for the UI, but what was the main language for the rest of the core stack?",
            "[Session llm-8, Turn 4] user: Right, okay. That makes sense given the libraries we wanted to use. Also, random question, did you see where I left my coffee mug?",
            "[Session llm-5, Turn 4] user: Right, the snake language. And are we still just using vanilla JavaScript for the UI?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000944137573242,
          "generation_latency_ms": 2526.6664028167725,
          "total_latency_ms": 2536.6673469543457
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Based on the reminders set across all sessions, which activity was scheduled for the earliest time of day?",
          "answer_type": "short_answer",
          "gold_answer": "Calling caterers",
          "predicted_answer": "Call the accountant",
          "retrieved_context": [
            "[Session llm-9, Turn 11] assistant: I've removed the dentist reminder from your schedule.",
            "[Session llm-7, Turn 9] assistant: I've added the dry cleaning reminder for 5 PM. Anything else?",
            "[Session llm-5, Turn 7] assistant: I've set a reminder for 5 PM to call your mom."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.540868759155273,
          "generation_latency_ms": 3960.738182067871,
          "total_latency_ms": 3969.2790508270264
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What was the weather condition described by the user during the conversation where they referred to Python as 'the snake language'?",
          "answer_type": "short_answer",
          "gold_answer": "Freezing",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, that's the one. For the backend, we are definitely sticking with the language named after a large snake.",
            "[Session llm-2, Turn 4] user: Right, that snake language won out. And we're pairing that with the typed superset of JS for the web app?",
            "[Session llm-8, Turn 13] assistant: Yes, the documentation indicates that Python is the exclusive framework being used for the data processing layer."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99958610534668,
          "generation_latency_ms": 3473.0300903320312,
          "total_latency_ms": 3482.029676437378
        },
        {
          "probe_id": "preference_learning-preference_scope-6",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Whose preference was it to use Python for the server-side logic of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-8, Turn 12] user: Thanks. Back to the code—are we strictly using Python for the data processing side of the core project?",
            "[Session llm-1, Turn 5] assistant: Right, he usually prefers Python for that reason. Is that what you settled on for the server side?",
            "[Session llm-2, Turn 3] assistant: That was quite the debate. If I recall, you settled on Python for the backend to handle the data processing side of the core project."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00411605834961,
          "generation_latency_ms": 3548.5637187957764,
          "total_latency_ms": 3557.567834854126
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user prefer to work on the core project's repository when the weather is gloomy?",
          "answer_type": "boolean",
          "gold_answer": "Yes",
          "predicted_answer": "Yes.",
          "retrieved_context": [
            "[Session llm-1, Turn 14] user: No, that's it for now. Just checking the weather, is it supposed to rain later?",
            "[Session llm-0, Turn 14] user: Definitely. Anyway, what's the weather looking like for a run later?",
            "[Session llm-8, Turn 1] assistant: Good morning! I don't have a live weather feed, but it's always good to bring an umbrella just in case. Did you need help with anything work-related to kick things off?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.517213821411133,
          "generation_latency_ms": 3523.51450920105,
          "total_latency_ms": 3536.031723022461
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the LearnLoop Core project using?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Sounds good. We are talking about the core LearnLoop repository, right? What did Liam decide?",
            "[Session llm-6, Turn 3] assistant: For the LearnLoop Core project, the codebase is actually built using Python and TypeScript.",
            "[Session llm-3, Turn 3] assistant: For LearnLoop Core, the stack is split. The backend services are running on that popular snake-named language, while the frontend is built using Microsoft's typed superset of JavaScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002996444702148,
          "generation_latency_ms": 3817.8722858428955,
          "total_latency_ms": 3825.8752822875977
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Where is the user's coffee mug located?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: I can't see the physical world, unfortunately, so your mug is safe from me! You might want to check the break room or your desk.",
            "[Session llm-8, Turn 4] user: Right, okay. That makes sense given the libraries we wanted to use. Also, random question, did you see where I left my coffee mug?",
            "[Session llm-3, Turn 12] user: Thanks. I really need to get that printed out and stuck to the wall."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00013542175293,
          "generation_latency_ms": 3617.76065826416,
          "total_latency_ms": 3625.760793685913
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific technical detail does the user repeatedly fail to recall regarding the LearnLoop Core project across multiple sessions?",
          "answer_type": "short_answer",
          "gold_answer": "The tech stack",
          "predicted_answer": "The punctuation for \"LearnLoop2024!\"",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Did you and Liam come to a decision on the tech stack for LearnLoop Core?",
            "[Session llm-7, Turn 3] assistant: Sounds good. We are talking about the core LearnLoop repository, right? What did Liam decide?",
            "[Session llm-2, Turn 10] user: So it's LearnLoop2024! then?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 16.001224517822266,
          "generation_latency_ms": 4483.734607696533,
          "total_latency_ms": 4499.7358322143555
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-11",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What task was the user working on when they asked about the stack in the final recorded session?",
          "answer_type": "short_answer",
          "gold_answer": "Writing onboarding doc",
          "predicted_answer": "Reviewing the main repository Liam set up for the core platform.",
          "retrieved_context": [
            "[Session llm-1, Turn 2] user: We were discussing the architecture for the core project. I want to make sure I captured the tech stack decisions correctly.",
            "[Session llm-7, Turn 2] user: Let's focus on the main project documentation. My cofounder and I finalized the stack choices yesterday.",
            "[Session llm-6, Turn 2] user: True. Speaking of work, I'm blanking on the tech stack for our core project. Was it Go or Python?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00468635559082,
          "generation_latency_ms": 3058.420419692993,
          "total_latency_ms": 3069.425106048584
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_041.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 14,
      "ingestion_time_s": 0.7833631038665771,
      "total_evaluation_time_s": 80.98023819923401,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Ming",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: How is the collaboration with your cofounder going on the new build?",
            "[Session llm-4, Turn 5] assistant: It definitely saves time on debugging later. Is your cofounder coming in today?",
            "[Session llm-1, Turn 2] user: Mostly technical debt. My cofounder and I are deep in the weeds on the LearnLoop Core project."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002872467041016,
          "generation_latency_ms": 3294.3484783172607,
          "total_latency_ms": 3307.3513507843018
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Ming?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Right, Ming was pretty adamant about the type safety on the client side.",
            "[Session llm-2, Turn 8] user: Thanks. Hey, regarding the office internet, Ming keeps asking. It's our company name, the current year, and a bang, right?",
            "[Session llm-8, Turn 2] user: That's good news. I'm digging into the repo Ming set up for the core platform. The backend is definitely written in that snake-symbol language he's obsessed with."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.005308151245117,
          "generation_latency_ms": 4156.5539836883545,
          "total_latency_ms": 4165.5592918396
        },
        {
          "probe_id": "world_modeling-task_state_tracking-0",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What specific item did the user ask to be added to their to-do list while preparing to work on the main repository?",
          "answer_type": "short_answer",
          "gold_answer": "Buy cat food",
          "predicted_answer": "buy cat food",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: Added 'buy cat food' to your to-do list. What are you working on right now?",
            "[Session llm-5, Turn 3] assistant: How is the collaboration with your cofounder going on the new build?",
            "[Session llm-7, Turn 2] user: I'm jumping into the main repository for our core project. Ming pushed some changes to the frontend yesterday."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.016607284545898,
          "generation_latency_ms": 3395.7066535949707,
          "total_latency_ms": 3406.7232608795166
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the user's description of a 'snake-based language', which technology is being used for the backend of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: That's good news. I'm digging into the repo Ming set up for the core platform. The backend is definitely written in that snake-symbol language he's obsessed with.",
            "[Session llm-6, Turn 4] user: Mostly just re-confirming our stack choices. We are definitely keeping the backend in that snake-based language, right? And the frontend is strictly that typed superset of JavaScript.",
            "[Session llm-0, Turn 3] assistant: Looking at the learnloopcore repository, it appears to be a dual-language setup. It uses both Python and TypeScript throughout the codebase."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.999887466430664,
          "generation_latency_ms": 3556.786060333252,
          "total_latency_ms": 3574.7859477996826
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-0",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Considering the frontend is built with a 'strictly typed superset of JavaScript', what file extension did the user observe in the repository?",
          "answer_type": "short_answer",
          "gold_answer": ".ts",
          "predicted_answer": ".ts",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Right, the codebase using that strictly typed superset of JavaScript?",
            "[Session llm-6, Turn 4] user: Mostly just re-confirming our stack choices. We are definitely keeping the backend in that snake-based language, right? And the frontend is strictly that typed superset of JavaScript.",
            "[Session llm-3, Turn 4] user: Yeah, we decided to finalize the tech stack. We're going with that typed JavaScript superset for the client side."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 19.01555061340332,
          "generation_latency_ms": 3164.3123626708984,
          "total_latency_ms": 3183.3279132843018
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact wifi password for the organization?",
          "answer_type": "verbatim",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 9] assistant: No worries at all. With everything going on at the company, keeping track of wifi passwords is probably the least of your concerns.",
            "[Session llm-0, Turn 8] user: Thanks. I'm actually at the office right now and my laptop isn't auto-connecting. What was the password for the org network again?",
            "[Session llm-5, Turn 8] user: Exactly. Oh, totally unrelated, but what's our office internet password again? I got a new phone and need to connect."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3063.5480880737305,
          "total_latency_ms": 3072.549819946289
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user complained about a foggy brain and needed a second coffee, what specific project detail did they ask to be refreshed on?",
          "answer_type": "short_answer",
          "gold_answer": "Tech stack",
          "predicted_answer": "the tech stack for the core project",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Ugh, I need a second coffee. My brain is completely foggy this morning.",
            "[Session llm-4, Turn 2] user: Good idea. While I'm stretching, can you refresh my memory on the tech stack for the core project? I feel like we changed it three times.",
            "[Session llm-1, Turn 1] assistant: Good morning! I can help you organize your thoughts. What's on the agenda for today?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.994766235351562,
          "generation_latency_ms": 3871.40154838562,
          "total_latency_ms": 3884.3963146209717
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user discuss the 'snake-symbol language' backend before or after they mentioned having coffee with Ming?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: That's good news. I'm digging into the repo Ming set up for the core platform. The backend is definitely written in that snake-symbol language he's obsessed with.",
            "[Session llm-1, Turn 6] user: No, the backend is Ming's problem today. He's dealing with the snake-based language for the API logic.",
            "[Session llm-0, Turn 2] user: Yeah, I'm trying to review the architecture Ming set up for our main project. I can't remember if we settled on a single language or not."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002302169799805,
          "generation_latency_ms": 4052.6328086853027,
          "total_latency_ms": 4063.6351108551025
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What syntax style does the user explicitly state they prefer over curly braces?",
          "answer_type": "short_answer",
          "gold_answer": "Indentation-based",
          "predicted_answer": "indentation-based syntax",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: That's the one. It's finally playing nice with the backend scripts, which is a relief since I prefer indentation-based syntax over curly braces any day.",
            "[Session llm-4, Turn 12] user: Definitely. Hey, just to double check, are we mixing any plain JavaScript in the core repo, or is it 100% strict?",
            "[Session llm-8, Turn 4] user: Yeah, exactly. But for the browser side, we're keeping things strictly typed. I'm seeing .ts extensions everywhere."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.006832122802734,
          "generation_latency_ms": 2883.5272789001465,
          "total_latency_ms": 2894.534111022949
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-0",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Despite the user's preference for indentation-based syntax, what language is accepted for the frontend to satisfy the cofounder's type safety requirement?",
          "answer_type": "short_answer",
          "gold_answer": "TypeScript",
          "predicted_answer": "TypeScript",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: That makes sense. TypeScript offers great type safety for large frontends. What about the server side?",
            "[Session llm-4, Turn 3] assistant: You eventually settled on a hybrid approach. The backend logic is running on Python, and the frontend is built with TypeScript.",
            "[Session llm-6, Turn 4] user: Mostly just re-confirming our stack choices. We are definitely keeping the backend in that snake-based language, right? And the frontend is strictly that typed superset of JavaScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.520334243774414,
          "generation_latency_ms": 4311.6443157196045,
          "total_latency_ms": 4327.164649963379
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database management system is LearnLoop Core using?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: Oh? Did your cofounder have any updates on the LearnLoop Core development?",
            "[Session llm-5, Turn 9] assistant: For the main LearnLoop network, the password is set to LearnLoop2024!",
            "[Session llm-5, Turn 7] assistant: Understood, so the LearnLoop core project is officially running on a mix of Python and TypeScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.006853103637695,
          "generation_latency_ms": 3607.0308685302734,
          "total_latency_ms": 3621.037721633911
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is Ming's last name?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: I hope so. I just got off the phone with Ming regarding the main project repo.",
            "[Session llm-2, Turn 8] user: Thanks. Hey, regarding the office internet, Ming keeps asking. It's our company name, the current year, and a bang, right?",
            "[Session llm-9, Turn 0] user: Hey, hope you're having a good morning. I just grabbed some coffee with Ming."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002519607543945,
          "generation_latency_ms": 4760.1964473724365,
          "total_latency_ms": 4768.1989669799805
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What activity does the user frequently engage in with Ming during the mornings as part of their work routine?",
          "answer_type": "short_answer",
          "gold_answer": "Architecture debates",
          "predicted_answer": "Reviewing the codebase",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Good morning! Coffee with Ming sounds like a nice way to kick off the day. How is everything going with the startup?",
            "[Session llm-9, Turn 0] user: Hey, hope you're having a good morning. I just grabbed some coffee with Ming.",
            "[Session llm-2, Turn 0] user: Morning! Any big news headlines I should know about?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.161876678466797,
          "generation_latency_ms": 4244.11940574646,
          "total_latency_ms": 4254.281282424927
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What technical method does the team use to ensure type safety on the client side?",
          "answer_type": "short_answer",
          "gold_answer": "TypeScript",
          "predicted_answer": "TypeScript",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: That makes sense. TypeScript offers great type safety for large frontends. What about the server side?",
            "[Session llm-4, Turn 4] user: Right, Ming was pretty adamant about the type safety on the client side.",
            "[Session llm-4, Turn 3] assistant: You eventually settled on a hybrid approach. The backend logic is running on Python, and the frontend is built with TypeScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 20.996809005737305,
          "generation_latency_ms": 3907.522678375244,
          "total_latency_ms": 3928.5194873809814
        }
      ],
      "summary": {
        "overall_accuracy": 0.7857142857142857,
        "total_score": 11,
        "count": 14,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 11,
            "accuracy": 0.7272727272727273
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 3,
            "accuracy": 0.3333333333333333
          },
          "declarative_reasoning": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_043.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7931361198425293,
      "total_evaluation_time_s": 75.06401371955872,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who manages the main repository for the LearnLoop core project?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Charlotte",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Right, for the LearnLoop core repo, we are using Python for the backend services and TypeScript for the frontend application.",
            "[Session llm-1, Turn 3] assistant: The LearnLoop core system? What specifically were you looking at?",
            "[Session llm-8, Turn 3] assistant: Use the umbrella reminder is set. regarding the LearnLoop core project, are you looking for the specific languages involved?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.01121711730957,
          "generation_latency_ms": 2487.506628036499,
          "total_latency_ms": 2500.5178451538086
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Glad the connection is stable. Are you referring to the main repository Charlotte manages?",
            "[Session llm-4, Turn 4] user: Okay, makes sense. Oh, did Charlotte mention if she's free for lunch today?",
            "[Session llm-0, Turn 7] assistant: I haven't seen any notes from Charlotte yet."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00191879272461,
          "generation_latency_ms": 4458.263874053955,
          "total_latency_ms": 4471.26579284668
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages are used for the LearnLoop core project's tech stack?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Right, for the LearnLoop core repo, we are using Python for the backend services and TypeScript for the frontend application.",
            "[Session llm-3, Turn 3] assistant: No, the LearnLoop Core project is actually a hybrid setup. It looks like you are utilizing both Python and TypeScript in there.",
            "[Session llm-7, Turn 2] user: Just the technical overview for the core project. I vaguely remember we locked in the languages, but I want to be precise."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001493453979492,
          "generation_latency_ms": 3564.2848014831543,
          "total_latency_ms": 3573.286294937134
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the specific password string for the organization's wifi network?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "the organization's name, capitalized, followed by 2024, and ending with an",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: For the office network? The password is just the organization's name, followed by 2024, and ending with an exclamation mark.",
            "[Session llm-1, Turn 10] user: Thanks. Also, the new intern needs the wifi access. It's the company name plus the current year and a bang, right?",
            "[Session llm-8, Turn 6] user: Right, the snake language and the typed one. By the way, my tablet dropped the connection. What's the office wifi pass?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.99850845336914,
          "generation_latency_ms": 3800.16827583313,
          "total_latency_ms": 3814.166784286499
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the 'reptile' named language occur before or after the user successfully connected to the wifi?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-8, Turn 6] user: Right, the snake language and the typed one. By the way, my tablet dropped the connection. What's the office wifi pass?",
            "[Session llm-3, Turn 2] user: Good call. Anyway, I was chatting with my cofounder earlier about our tech stack. I slightly forgot, are we strictly sticking to one language for the core repo?",
            "[Session llm-6, Turn 1] assistant: Congrats on the new office! For the wifi, I believe it's the company name, capitalized, followed by the current year and an exclamation point."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00213623046875,
          "generation_latency_ms": 12172.44291305542,
          "total_latency_ms": 12182.445049285889
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What task was the user performing when they described the backend language as being named after a reptile?",
          "answer_type": "short_answer",
          "gold_answer": "Configuring development environment",
          "predicted_answer": "data science stuff",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: A bit of both. We're using that superset of JavaScript for the web components, but the data science stuff is all in that language named after a reptile.",
            "[Session llm-3, Turn 2] user: Good call. Anyway, I was chatting with my cofounder earlier about our tech stack. I slightly forgot, are we strictly sticking to one language for the core repo?",
            "[Session llm-3, Turn 8] user: Perfect. Back to the code—for the backend data processing, we are using the snake language, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 2586.9979858398438,
          "total_latency_ms": 2595.9997177124023
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user demonstrate a pattern of relying on the assistant for personal reminders like appointments and errands?",
          "answer_type": "boolean",
          "gold_answer": "Yes",
          "predicted_answer": "Yes.",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Use the umbrella reminder is set. regarding the LearnLoop core project, are you looking for the specific languages involved?",
            "[Session llm-4, Turn 13] assistant: Reminder set for 3 PM to call the dentist. Anything else?",
            "[Session llm-1, Turn 9] assistant: Reminder set for 5 PM to call your mom."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3678.011178970337,
          "total_latency_ms": 3687.0129108428955
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What is the user's preferred Integrated Development Environment (IDE)?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: I'm sitting here with my cofounder trying to configure the development environment for the core project.",
            "[Session llm-2, Turn 0] user: Hey, I'm trying to get the dev environment set up for the main project today.",
            "[Session llm-9, Turn 2] user: A bit slow, honestly. I'm trying to configure my local environment for the core project, but I completely blanked on which languages we settled on."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 2764.1851902008057,
          "total_latency_ms": 2774.1856575012207
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which specific database technology is used for the LearnLoop core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: The LearnLoop core system? What specifically were you looking at?",
            "[Session llm-7, Turn 3] assistant: Right, for the LearnLoop core repo, we are using Python for the backend services and TypeScript for the frontend application.",
            "[Session llm-8, Turn 3] assistant: Use the umbrella reminder is set. regarding the LearnLoop core project, are you looking for the specific languages involved?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002447128295898,
          "generation_latency_ms": 3164.4251346588135,
          "total_latency_ms": 3173.4275817871094
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is Charlotte's preferred cuisine for lunch?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Okay, makes sense. Oh, did Charlotte mention if she's free for lunch today?",
            "[Session llm-9, Turn 10] user: Perfect, it worked. Charlotte is waving at me, looks like she wants to grab lunch soon.",
            "[Session llm-8, Turn 9] assistant: I can draft an invite for Charlotte. What time should I look for?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3672.0807552337646,
          "total_latency_ms": 3682.0812225341797
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the rule for constructing the LearnLoop wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Capitalized name, year, exclamation",
          "predicted_answer": "the organization's name, followed by 2024, and ending with an exclamation mark.",
          "retrieved_context": [
            "[Session llm-8, Turn 6] user: Right, the snake language and the typed one. By the way, my tablet dropped the connection. What's the office wifi pass?",
            "[Session llm-2, Turn 8] user: Just to double-check, that's LearnLoop2024! right?",
            "[Session llm-1, Turn 10] user: Thanks. Also, the new intern needs the wifi access. It's the company name plus the current year and a bang, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.523962020874023,
          "generation_latency_ms": 3793.498754501343,
          "total_latency_ms": 3805.022716522217
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What are the specific steps to deploy the core project to production?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: I'm sitting here with my cofounder trying to configure the development environment for the core project.",
            "[Session llm-2, Turn 0] user: Hey, I'm trying to get the dev environment set up for the main project today.",
            "[Session llm-4, Turn 2] user: Tell me about it. Speaking of the startup, are we still splitting the tech stack for the core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.021541595458984,
          "generation_latency_ms": 3755.7928562164307,
          "total_latency_ms": 3767.8143978118896
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 7,
            "accuracy": 0.2857142857142857
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 4,
            "count": 4,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_046.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7724831104278564,
      "total_evaluation_time_s": 69.12655663490295,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: How are things progressing with your cofounder on the core project?",
            "[Session llm-9, Turn 11] assistant: It happens! Anything else I can help clarify for your cofounder or the project?",
            "[Session llm-1, Turn 3] assistant: I think your cofounder updated them recently. Try using the company name combined with the current year and an exclamation mark at the end."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 28.528690338134766,
          "generation_latency_ms": 3455.037832260132,
          "total_latency_ms": 3483.5665225982666
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which project is associated with the organization 'LearnLoop'?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop Core",
          "predicted_answer": "LearnLoop Core",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: Yes, for the LearnLoop Core project, Python is definitely part of the framework mix.",
            "[Session llm-0, Turn 7] assistant: Understood, keeping Python for the backend of LearnLoop Core seems like a solid choice given the libraries available.",
            "[Session llm-6, Turn 7] assistant: How are things progressing with your cofounder on the core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000394821166992,
          "generation_latency_ms": 3855.6580543518066,
          "total_latency_ms": 3866.6584491729736
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the user's description of '.py files' and 'Microsoft's superset of JS', what two programming languages constitute the project's stack?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: For the core work, the backend is built on those snake-related .py files, while the frontend is using that typed superset of JavaScript.",
            "[Session llm-4, Turn 4] user: Well, the backend services are all .py files, which is fine, but strictly typing the frontend with Microsoft's superset of JS is giving me a headache today.",
            "[Session llm-5, Turn 4] user: Thanks. Okay, regarding the main project repo... my cofounder was asking about the tech stack documentation. We are definitely using Python for the backend, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.998176574707031,
          "generation_latency_ms": 4340.4858112335205,
          "total_latency_ms": 4352.4839878082275
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the LearnLoop office wifi?",
          "answer_type": "verbatim",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "LearnLoop2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: No problem. The password for the organization's wifi is LearnLoop2024!.",
            "[Session llm-6, Turn 2] user: My cousin is stopping by the office later and might need to hop on the wifi. What's the password for our main network again?",
            "[Session llm-7, Turn 2] user: Yeah, eventually. Right now we are just trying to get connected. Did we set a specific password for the LearnLoop network?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003494262695312,
          "generation_latency_ms": 3550.269842147827,
          "total_latency_ms": 3561.2733364105225
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the user experiencing mental drain while working on the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Context switching",
          "predicted_answer": "Juggling specific languages for the core codebase",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Great. We are looking at the codebase for the core project. I feel like the frontend is getting messy.",
            "[Session llm-0, Turn 4] user: Right. We were debating the tech stack for the core project again. Liam is worried about scalability.",
            "[Session llm-2, Turn 3] assistant: Understood. What specific details regarding the core project do you need to review?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 16.002178192138672,
          "generation_latency_ms": 3025.7444381713867,
          "total_latency_ms": 3041.7466163635254
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about a headache, what specific task were they struggling with?",
          "answer_type": "short_answer",
          "gold_answer": "Strictly typing frontend",
          "predicted_answer": "Typed JavaScript",
          "retrieved_context": [
            "[Session llm-7, Turn 10] user: Speaking of headaches, I haven't had lunch yet.",
            "[Session llm-2, Turn 3] assistant: Understood. What specific details regarding the core project do you need to review?",
            "[Session llm-9, Turn 11] assistant: It happens! Anything else I can help clarify for your cofounder or the project?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.01706314086914,
          "generation_latency_ms": 4465.614557266235,
          "total_latency_ms": 4473.6316204071045
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's comments about 'headaches' and files being 'fine', which part of the stack do they prefer working on?",
          "answer_type": "short_answer",
          "gold_answer": "Backend",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: Right. We were debating the tech stack for the core project again. Liam is worried about scalability.",
            "[Session llm-7, Turn 6] user: Great. We are looking at the codebase for the core project. I feel like the frontend is getting messy.",
            "[Session llm-8, Turn 6] user: Classic cofounder move. Anyway, I'm reviewing the core codebase. We are heavily committed to that superset of JavaScript, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.015010833740234,
          "generation_latency_ms": 3155.829906463623,
          "total_latency_ms": 3165.8449172973633
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user feel about the requirement to split the backend and frontend languages?",
          "answer_type": "short_answer",
          "gold_answer": "Drained",
          "predicted_answer": "It's giving them a headache and causing a lot of context switching.",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: I'm diving back into the core project. My cofounder, Liam, insisted on splitting the backend and frontend languages, so I'm context switching a lot.",
            "[Session llm-5, Turn 6] user: And for the frontend components? I know we debated plain JS vs others.",
            "[Session llm-5, Turn 7] assistant: The decision was made to use TypeScript for the frontend to ensure better maintainability."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.016275405883789,
          "generation_latency_ms": 3810.629367828369,
          "total_latency_ms": 3819.645643234253
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the LearnLoop Core project using?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 7] assistant: Understood, keeping Python for the backend of LearnLoop Core seems like a solid choice given the libraries available.",
            "[Session llm-9, Turn 7] assistant: Yes, for the LearnLoop Core project, Python is definitely part of the framework mix.",
            "[Session llm-3, Turn 13] assistant: Exactly, that's the one. It handles the data processing better for us."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999824523925781,
          "generation_latency_ms": 4289.984464645386,
          "total_latency_ms": 4298.9842891693115
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the last name of the user's cofounder?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: I think your cofounder updated them recently. Try using the company name combined with the current year and an exclamation mark at the end.",
            "[Session llm-7, Turn 0] user: Hey, I just got to the new office with my cofounder. It's surprisingly echoey in here.",
            "[Session llm-6, Turn 7] assistant: How are things progressing with your cofounder on the core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002208709716797,
          "generation_latency_ms": 3246.8934059143066,
          "total_latency_ms": 3255.8956146240234
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the formulaic rule for constructing the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Capitalized name, 2024, exclamation",
          "predicted_answer": "the company name combined with the current year, followed by an exclamation mark.",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: My cousin is stopping by the office later and might need to hop on the wifi. What's the password for our main network again?",
            "[Session llm-9, Turn 3] assistant: No problem. The password for the organization's wifi is LearnLoop2024!.",
            "[Session llm-2, Turn 9] assistant: Got it. For the network, the password is just the company name combined with the current year, followed by an exclamation mark."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.002944946289062,
          "generation_latency_ms": 3577.7387619018555,
          "total_latency_ms": 3589.7417068481445
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Where does the user physically look to recall the punctuation at the end of the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Sticky note",
          "predicted_answer": "the readme",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: My cousin is stopping by the office later and might need to hop on the wifi. What's the password for our main network again?",
            "[Session llm-2, Turn 9] assistant: Got it. For the network, the password is just the company name combined with the current year, followed by an exclamation mark.",
            "[Session llm-9, Turn 3] assistant: No problem. The password for the organization's wifi is LearnLoop2024!."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999513626098633,
          "generation_latency_ms": 3858.03484916687,
          "total_latency_ms": 3868.0343627929688
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 9,
            "accuracy": 0.2222222222222222
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_edtech_050.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.782066822052002,
      "total_evaluation_time_s": 68.44054198265076,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user planning to conduct a code review with regarding the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "cofounder",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: I need to update the documentation for the core project repo. My brain is a bit foggy—what's the primary language mix we're using there?",
            "[Session llm-0, Turn 2] user: Thanks. I'm trying to get my dev environment set up for the new project. Do you recall which languages we decided to use for the core repo?",
            "[Session llm-1, Turn 2] user: I need to double-check the tech stack for the main LearnLoop build. I'm writing up a summary for a contractor."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.015726089477539,
          "generation_latency_ms": 2941.666603088379,
          "total_latency_ms": 2951.6823291778564
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which two programming languages comprise the tech stack for the LearnLoop Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and TypeScript",
          "predicted_answer": "Python and TypeScript",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the LearnLoop Core setup, it appears to be a hybrid stack using both Python and TypeScript.",
            "[Session llm-9, Turn 3] assistant: For the LearnLoop Core codebase, it looks like a hybrid setup using Python and TypeScript.",
            "[Session llm-0, Turn 2] user: Thanks. I'm trying to get my dev environment set up for the new project. Do you recall which languages we decided to use for the core repo?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002447128295898,
          "generation_latency_ms": 3166.33677482605,
          "total_latency_ms": 3175.3392219543457
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the assistant's hint involving the organization's name and the year, what is the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "LearnLoop2024!",
          "predicted_answer": "The organization's name with the first letters capitalized, followed by 2024 and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: I believe the password is just the organization's name with the first letters capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-7, Turn 9] assistant: It was updated recently. The password is now the company name combined with the current year, 2024, and it ends with an exclamation point.",
            "[Session llm-7, Turn 8] user: Thanks. Also, we have a vendor coming by later to look at the HVAC. They asked for the wifi details. Is it still the old password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.46025276184082,
          "generation_latency_ms": 3969.9320793151855,
          "total_latency_ms": 3985.3923320770264
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Although the user recalled using Java, what language did the team actually standardize on for the backend?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: Good morning. No, you actually moved away from Java. The core codebase is currently built using Python for the backend services.",
            "[Session llm-7, Turn 0] user: Morning. I'm trying to update the documentation for the core project, but my brain is foggy. Did we end up sticking with Java for the backend?",
            "[Session llm-6, Turn 2] user: The main repo for the core project. We're trying to standardize how the backend scripts talk to the frontend interface."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.0015869140625,
          "generation_latency_ms": 3626.8858909606934,
          "total_latency_ms": 3637.887477874756
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user mention biking into the office before or after their discussion about the code review with Olivia?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: Perfect, I'll let my cofounder know. Did she mention anything about the lunch meeting?",
            "[Session llm-6, Turn 1] assistant: Good morning. Hopefully, the coffee kicks in soon. What are you reviewing with Olivia today?",
            "[Session llm-5, Turn 11] assistant: Hopefully Olivia stays dry then. Is she working from the office today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 7.996797561645508,
          "generation_latency_ms": 4398.845672607422,
          "total_latency_ms": 4406.842470169067
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific task was the user attempting to perform when they complained about their brain being 'foggy'?",
          "answer_type": "short_answer",
          "gold_answer": "Updating documentation",
          "predicted_answer": "Setting up the dev environment for the core platform.",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Switching between dynamic and static typing can definitely be mentally taxing.",
            "[Session llm-5, Turn 1] assistant: Good morning. It sounds like a busy start. How can I help you focus?",
            "[Session llm-9, Turn 0] user: Good morning. How is the system running today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.990713119506836,
          "generation_latency_ms": 4222.354412078857,
          "total_latency_ms": 4235.345125198364
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user find context switching between the backend and frontend languages easy or taxing?",
          "answer_type": "short_answer",
          "gold_answer": "Taxing",
          "predicted_answer": "Taxing",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: For the LearnLoop Core project, the backend logic is handled in Python, while the frontend is written in TypeScript.",
            "[Session llm-7, Turn 2] user: Right, the snake language. And the frontend is React, but are we using plain JavaScript?",
            "[Session llm-1, Turn 3] assistant: For the core project, remember that we are utilizing that strictly typed superset of JavaScript for the frontend and the standard snake-logo language for the backend services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.01431655883789,
          "generation_latency_ms": 3014.479160308838,
          "total_latency_ms": 3027.493476867676
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What is the user's stated preference regarding commuting in the rain?",
          "answer_type": "short_answer",
          "gold_answer": "Avoid getting soaked",
          "predicted_answer": "Rather not get soaked on the way back.",
          "retrieved_context": [
            "[Session llm-5, Turn 10] user: I hope so. My cofounder is coming in later and she hates the rain.",
            "[Session llm-4, Turn 0] user: Hey, hope you're having a good morning. Is it supposed to rain later?",
            "[Session llm-3, Turn 0] user: Hey, hope you're having a good morning. Is it raining where you are?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.999368667602539,
          "generation_latency_ms": 3297.635793685913,
          "total_latency_ms": 3309.6351623535156
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What time did Olivia arrive at the office on the day the user was updating documentation?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: I haven't seen an update from Olivia regarding her arrival time today.",
            "[Session llm-5, Turn 11] assistant: Hopefully Olivia stays dry then. Is she working from the office today?",
            "[Session llm-1, Turn 6] user: Thanks. Also, Olivia is having trouble connecting her tablet to the office network. Did we change the credentials?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003732681274414,
          "generation_latency_ms": 3867.6483631134033,
          "total_latency_ms": 3878.6520957946777
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number of Python is the backend currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 13] assistant: Yes, the Python environment is configured for the backend data tasks within the project.",
            "[Session llm-4, Turn 3] assistant: For the main project repo, the backend is strictly Python, while the frontend team has standardized on TypeScript.",
            "[Session llm-7, Turn 1] assistant: Good morning. No, you actually moved away from Java. The core codebase is currently built using Python for the backend services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999917984008789,
          "generation_latency_ms": 4210.647344589233,
          "total_latency_ms": 4221.647262573242
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What components make up the rule for the organization's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Org name, year, exclamation",
          "predicted_answer": "organization's name with the first letters capitalized, the current year, and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-2, Turn 8] user: It was alright. Anyway, I'm at the HQ and my phone dropped the network. What's the wifi secret again?",
            "[Session llm-7, Turn 8] user: Thanks. Also, we have a vendor coming by later to look at the HVAC. They asked for the wifi details. Is it still the old password?",
            "[Session llm-4, Turn 5] assistant: I believe the password is just the organization's name with the first letters capitalized, followed by the current year and an exclamation mark."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.527040481567383,
          "generation_latency_ms": 4144.291400909424,
          "total_latency_ms": 4152.818441390991
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What mental cost did the user identify when working with both Python and TypeScript simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Context switching",
          "predicted_answer": "context switching",
          "retrieved_context": [
            "[Session llm-6, Turn 4] user: A bit of both. It's just a pain context switching between the Python services and the TypeScript UI code.",
            "[Session llm-8, Turn 4] user: Exactly. Mixing TypeScript with Python is powerful but tricky for the core platform.",
            "[Session llm-4, Turn 3] assistant: For the main project repo, the backend is strictly Python, while the frontend team has standardized on TypeScript."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.000415802001953,
          "generation_latency_ms": 3520.272731781006,
          "total_latency_ms": 3534.273147583008
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_004.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 10,
      "ingestion_time_s": 0.7293112277984619,
      "total_evaluation_time_s": 58.879474401474,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual pushing for the adoption of the 'JetBrains language' for new modules?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "She",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Yeah. She's pushing for us to use that JetBrains language for all new modules, but we still have a ton of legacy code in the Oracle language.",
            "[Session llm-5, Turn 4] user: A mix of both. We're in this awkward transition phase where half the codebase is old-school Oracle syntax and the other half is that newer JetBrains language.",
            "[Session llm-8, Turn 5] assistant: It seems to be a blend of the classic JVM language and the modern alternative developed by JetBrains. They run side-by-side."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 16.002655029296875,
          "generation_latency_ms": 2796.4909076690674,
          "total_latency_ms": 2812.4935626983643
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific project is described as having a 'hybrid environment' containing both Oracle and JetBrains syntaxes?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow Core",
          "predicted_answer": "codebase",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Yeah. She's pushing for us to use that JetBrains language for all new modules, but we still have a ton of legacy code in the Oracle language.",
            "[Session llm-5, Turn 4] user: A mix of both. We're in this awkward transition phase where half the codebase is old-school Oracle syntax and the other half is that newer JetBrains language.",
            "[Session llm-2, Turn 3] assistant: It's actually a hybrid environment right now. The core project still relies heavily on Java, but there is a significant amount of Kotlin being used as well."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002395629882812,
          "generation_latency_ms": 4500.519752502441,
          "total_latency_ms": 4513.522148132324
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the user's descriptions of 'old-school Oracle syntax' and the 'newer JetBrains language', what two frameworks constitute the core project's tech stack?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Yeah. She's pushing for us to use that JetBrains language for all new modules, but we still have a ton of legacy code in the Oracle language.",
            "[Session llm-5, Turn 4] user: A mix of both. We're in this awkward transition phase where half the codebase is old-school Oracle syntax and the other half is that newer JetBrains language.",
            "[Session llm-4, Turn 10] user: She wanted to confirm the programming languages we're currently supporting in the repo."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.001348495483398,
          "generation_latency_ms": 3869.389295578003,
          "total_latency_ms": 3880.3906440734863
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the office wifi?",
          "answer_type": "verbatim",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Hey, I had to reset my network settings on my phone. Do you remember the password for the office wifi?",
            "[Session llm-2, Turn 6] user: Will do. Oh, also, I'm trying to connect my new phone to the office network. What's the password again?",
            "[Session llm-9, Turn 8] user: Thanks, I might take you up on that. Totally unrelated, but my phone keeps dropping the office Wi-Fi. What was the passkey again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.016057968139648,
          "generation_latency_ms": 4216.1242961883545,
          "total_latency_ms": 4228.140354156494
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about the 'awkward transition phase' of the codebase, what personal task did they ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: It's alright, just busy. Emma was asking for a summary of the tech stack we're using on the core project. I know it's mixed, but what are the specifics again?",
            "[Session llm-4, Turn 9] assistant: What specifically did your cofounder want to know about the core project?",
            "[Session llm-1, Turn 4] user: She's stressed, as usual. We're trying to figure out why the build times are so slow with the mixed codebase."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000944137573242,
          "generation_latency_ms": 4157.637357711792,
          "total_latency_ms": 4167.638301849365
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for the wifi password for the first time before or after they mentioned onboarding a contractor?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: Okay. I have a vendor coming by the office later who needs internet access. Do you recall the wireless credentials?",
            "[Session llm-0, Turn 4] user: Interesting mix. I'll have to update my IDE settings. By the way, my cofounder is here and she forgot the wifi password again.",
            "[Session llm-4, Turn 0] user: Hey, I had to reset my network settings on my phone. Do you remember the password for the office wifi?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.007951736450195,
          "generation_latency_ms": 3914.263963699341,
          "total_latency_ms": 3926.271915435791
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user's cofounder prefer for all new modules?",
          "answer_type": "short_answer",
          "gold_answer": "Kotlin",
          "predicted_answer": "Kotlin",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Yeah, that's the one. It feels like we are migrating slowly. What are the primary languages in there again?",
            "[Session llm-9, Turn 4] user: Yeah. She's pushing for us to use that JetBrains language for all new modules, but we still have a ton of legacy code in the Oracle language.",
            "[Session llm-3, Turn 4] user: Thanks. I was reviewing the backend repository for our main product earlier. It looks like we aren't sticking to just one language?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00358772277832,
          "generation_latency_ms": 3551.952600479126,
          "total_latency_ms": 3564.9561882019043
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What frontend framework does the LedgerFlow team use?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: For the main LedgerFlow codebase, it is indeed a mix. We rely heavily on Java for the older services, but the team has been introducing Kotlin for all the newer features.",
            "[Session llm-5, Turn 9] assistant: For the LedgerFlow network? It should be the standard pattern we set up.",
            "[Session llm-0, Turn 2] user: Thanks. I was looking through the `ledgerflowcore` repository earlier. It looks like we aren't sticking to just one language anymore?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.013269424438477,
          "generation_latency_ms": 3829.4248580932617,
          "total_latency_ms": 3840.4381275177
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What did Emma bring for lunch on the day the user onboarded the contractor?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: I haven't seen any messages from Emma regarding lunch plans today.",
            "[Session llm-9, Turn 13] assistant: It's easy to overlook. Do you need me to pull up the meeting notes for your review with Emma?",
            "[Session llm-5, Turn 3] assistant: Is Emma running into issues with the legacy code, or is it the new implementation?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.51020622253418,
          "generation_latency_ms": 3574.7952461242676,
          "total_latency_ms": 3585.3054523468018
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's explanation, what is the three-part pattern for the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation mark",
          "predicted_answer": "organization's name, the year 2024, and an exclamation mark",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: Thanks, I might take you up on that. Totally unrelated, but my phone keeps dropping the office Wi-Fi. What was the passkey again?",
            "[Session llm-4, Turn 0] user: Hey, I had to reset my network settings on my phone. Do you remember the password for the office wifi?",
            "[Session llm-2, Turn 6] user: Will do. Oh, also, I'm trying to connect my new phone to the office network. What's the password again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.004396438598633,
          "generation_latency_ms": 2917.3309803009033,
          "total_latency_ms": 2932.335376739502
        }
      ],
      "summary": {
        "overall_accuracy": 0.6,
        "total_score": 6,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 7,
            "accuracy": 0.5714285714285714
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_005.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7862222194671631,
      "total_evaluation_time_s": 71.46864104270935,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder' who debates backend architecture?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Liam is driving me crazy today with the backend architecture debates for the core platform.",
            "[Session llm-1, Turn 3] assistant: Technical debates can be draining. What exactly are Liam and you disagreeing on regarding the architecture?",
            "[Session llm-1, Turn 2] user: Mostly the technical side. My cofounder and I are debating the backend structure for the main core project again."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.015010833740234,
          "generation_latency_ms": 4141.009569168091,
          "total_latency_ms": 4151.024580001831
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between Liam and the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues working on the same core project.",
          "retrieved_context": [
            "[Session llm-7, Turn 0] user: Hey, is Liam in the office yet?",
            "[Session llm-0, Turn 3] assistant: Liam tends to worry about scalability. Is he concerned about the legacy code or the new modules in the core project?",
            "[Session llm-9, Turn 11] assistant: I can certainly draft that email to Liam regarding the backend languages."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.998321533203125,
          "generation_latency_ms": 4283.5352420806885,
          "total_latency_ms": 4293.533563613892
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the hints provided about the company name and the current year, what is the exact wifi password string?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 10] user: Thanks. Also, the new intern is asking for the office wifi password. I totally forgot it.",
            "[Session llm-1, Turn 11] assistant: Understood. I have noted the pattern for the LedgerFlow wifi password.",
            "[Session llm-2, Turn 8] user: Thanks. Also, my phone disconnected from the network. What's the office wifi pass again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000394821166992,
          "generation_latency_ms": 4021.0018157958984,
          "total_latency_ms": 4032.0022106170654
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two programming languages are confirmed to be in use within the LedgerFlow core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Great, I'll try to grab lunch early then. I'm digging into the `ledgerflowcore` repo right now. Is it strictly one language at this point?",
            "[Session llm-9, Turn 9] assistant: The plan for LedgerFlow Core is to keep both for now, though new modules are mostly being written in Kotlin while maintaining the legacy Java.",
            "[Session llm-0, Turn 5] assistant: Yes, the LedgerFlow Core project is currently utilizing both Java and Kotlin."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.010574340820312,
          "generation_latency_ms": 3488.4591102600098,
          "total_latency_ms": 3500.46968460083
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why does the LedgerFlow core project currently utilize a hybrid of two different programming languages?",
          "answer_type": "short_answer",
          "gold_answer": "Migration from legacy code",
          "predicted_answer": "It started with standard Java, and a lot of Kotlin is being introduced recently, with new modules mostly written in Kotlin while maintaining the legacy Java.",
          "retrieved_context": [
            "[Session llm-9, Turn 9] assistant: The plan for LedgerFlow Core is to keep both for now, though new modules are mostly being written in Kotlin while maintaining the legacy Java.",
            "[Session llm-8, Turn 2] user: Great, I'll try to grab lunch early then. I'm digging into the `ledgerflowcore` repo right now. Is it strictly one language at this point?",
            "[Session llm-3, Turn 13] assistant: Understood. Dealing with mixed frameworks in the LedgerFlow Core project can be tricky for newcomers."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002685546875,
          "generation_latency_ms": 3643.1384086608887,
          "total_latency_ms": 3652.1410942077637
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user asked to add 'milk' to their list, what technical issue were they encountering?",
          "answer_type": "short_answer",
          "gold_answer": "Compilation errors",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Morning. Can you add \"Buy milk\" to my personal list?",
            "[Session llm-5, Turn 1] assistant: Done. I've added milk to your shopping list.",
            "[Session llm-8, Turn 4] user: That explains the syntax switching I'm seeing. Makes sense."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999109268188477,
          "generation_latency_ms": 5891.533613204956,
          "total_latency_ms": 5900.5327224731445
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user prefer for new modules to reduce boilerplate code?",
          "answer_type": "short_answer",
          "gold_answer": "Kotlin",
          "predicted_answer": "Kotlin",
          "retrieved_context": [
            "[Session llm-2, Turn 2] user: Pretty intense. Anyway, I'm taking a look at the core repo for our project. What languages are we actually writing this in?",
            "[Session llm-6, Turn 4] user: Definitely. I want to use Kotlin for the new modules to cut down on boilerplate code.",
            "[Session llm-1, Turn 4] user: It's a syntax issue, mostly. We have a lot of existing code in that older, verbose language that runs on the JVM, but he insists on writing all the new modules in the newer one developed by JetBrains."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00213623046875,
          "generation_latency_ms": 2700.209379196167,
          "total_latency_ms": 2710.2115154266357
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which technology stack does Liam insist on using due to his preference for stability?",
          "answer_type": "short_answer",
          "gold_answer": "Java",
          "predicted_answer": "standard Java",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Liam tends to worry about scalability. Is he concerned about the legacy code or the new modules in the core project?",
            "[Session llm-1, Turn 3] assistant: Technical debates can be draining. What exactly are Liam and you disagreeing on regarding the architecture?",
            "[Session llm-4, Turn 6] user: Tell me about it. Liam wrote half of the old stuff years ago and now he wants everything modernized, but the build times are suffering."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.006832122802734,
          "generation_latency_ms": 2697.383165359497,
          "total_latency_ms": 2708.3899974823
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database system is explicitly mentioned as being used for the LedgerFlow backend?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 9] assistant: The plan for LedgerFlow Core is to keep both for now, though new modules are mostly being written in Kotlin while maintaining the legacy Java.",
            "[Session llm-3, Turn 3] assistant: Oh, right. What did Liam have to say about the LedgerFlow Core architecture?",
            "[Session llm-0, Turn 5] assistant: Yes, the LedgerFlow Core project is currently utilizing both Java and Kotlin."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00246810913086,
          "generation_latency_ms": 4162.262678146362,
          "total_latency_ms": 4174.265146255493
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What frontend framework does the user utilize for the LedgerFlow project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 13] assistant: Understood. Dealing with mixed frameworks in the LedgerFlow Core project can be tricky for newcomers.",
            "[Session llm-9, Turn 9] assistant: The plan for LedgerFlow Core is to keep both for now, though new modules are mostly being written in Kotlin while maintaining the legacy Java.",
            "[Session llm-0, Turn 5] assistant: Yes, the LedgerFlow Core project is currently utilizing both Java and Kotlin."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 3863.555669784546,
          "total_latency_ms": 3873.5570907592773
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what three components are combined to form the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 11] assistant: Understood. I have noted the pattern for the LedgerFlow wifi password.",
            "[Session llm-6, Turn 10] user: Thanks. Also, the new intern is asking for the office wifi password. I totally forgot it.",
            "[Session llm-0, Turn 8] user: No, I'm good. Oh, actually, we have a client visiting the office later. I totally forgot the wifi password."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.006044387817383,
          "generation_latency_ms": 3764.955520629883,
          "total_latency_ms": 3776.9615650177
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific requirement did the assistant note for the build environment regarding the hybrid codebase?",
          "answer_type": "short_answer",
          "gold_answer": "Must support compiling both languages",
          "predicted_answer": "The build environment needs to support compiling both.",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Alright. I'm setting up the build pipeline for the core project. Is the codebase strictly one language?",
            "[Session llm-0, Turn 3] assistant: Liam tends to worry about scalability. Is he concerned about the legacy code or the new modules in the core project?",
            "[Session llm-4, Turn 5] assistant: Managing a hybrid codebase with those two JVM languages can definitely be tricky, especially with interoperability."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.025667190551758,
          "generation_latency_ms": 2797.8763580322266,
          "total_latency_ms": 2808.9020252227783
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_007.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7962112426757812,
      "total_evaluation_time_s": 68.00554251670837,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as 'my cofounder' that has been pushing code to the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: Yeah, that's the one. My cofounder, Emma, has been pushing a lot of code lately, and the tech stack is getting a bit mixed.",
            "[Session llm-3, Turn 9] assistant: I do. Tell my cofounder it's just the organization's name—Capital L, Capital F—followed by the current year and an exclamation point.",
            "[Session llm-8, Turn 2] user: Thanks. I was chatting with my cofounder earlier about the LedgerFlow core repo. We're bringing on a contractor and need to explain the stack."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.99850845336914,
          "generation_latency_ms": 2713.09232711792,
          "total_latency_ms": 2727.090835571289
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which programming language corresponds to the 'modern one that JetBrains developed' used in the new modules?",
          "answer_type": "short_answer",
          "gold_answer": "Kotlin",
          "predicted_answer": "Kotlin",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Exactly. We have the legacy code in that verbose, old-school language everyone uses, but all the new modules are in the modern one that JetBrains developed.",
            "[Session llm-8, Turn 4] user: Yeah, exactly. It's still heavily relying on that classic verbose language, but all the new stuff is in the modern JetBrains alternative.",
            "[Session llm-9, Turn 8] user: Thanks, that worked. Back to the code—I think I'm going to push for writing all new modules in the JetBrains language."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.002561569213867,
          "generation_latency_ms": 4330.353498458862,
          "total_latency_ms": 4344.356060028076
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the formula provided by the assistant, what is the password for the office wifi network?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "company name with a capital L and F, followed by the current year and an",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: Yes, that matches the current security settings for the office wifi.",
            "[Session llm-7, Turn 4] user: Makes sense, interoperability is key. Oh, before I forget, my tablet dropped the connection. What's the office wifi pass again?",
            "[Session llm-8, Turn 8] user: Nice. Oh, speaking of the office, I forgot the wifi string. Is it still the company name and the year?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.006044387817383,
          "generation_latency_ms": 4696.587562561035,
          "total_latency_ms": 4708.5936069488525
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What specific 'verbose, old-school' language is used for the legacy code in the LedgerFlow Core?",
          "answer_type": "short_answer",
          "gold_answer": "Java",
          "predicted_answer": "Java",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Thanks. I was chatting with my cofounder earlier about the LedgerFlow core repo. We're bringing on a contractor and need to explain the stack.",
            "[Session llm-5, Turn 3] assistant: Are you referring to the main LedgerFlow Core repository?",
            "[Session llm-6, Turn 3] assistant: Are you referring to the core LedgerFlow project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 3386.5396976470947,
          "total_latency_ms": 3395.5397605895996
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for the wifi password before or after setting a reminder to call the mechanic?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-0, Turn 8] user: Fingers crossed. Oh, they just asked for the wifi password. I always forget what we changed it to.",
            "[Session llm-6, Turn 11] assistant: Yes, that matches the current security settings for the office wifi.",
            "[Session llm-8, Turn 8] user: Nice. Oh, speaking of the office, I forgot the wifi string. Is it still the company name and the year?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003494262695312,
          "generation_latency_ms": 4028.9034843444824,
          "total_latency_ms": 4039.9069786071777
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user mentioned bringing on a contractor, what specific reminder did they ask the assistant to set?",
          "answer_type": "short_answer",
          "gold_answer": "Call the mechanic",
          "predicted_answer": "lunch reminder for 12:30",
          "retrieved_context": [
            "[Session llm-2, Turn 9] assistant: Will do. Is there a specific time you'd like that reminder?",
            "[Session llm-6, Turn 1] assistant: I've added that to your schedule. Is there anything specific you need to discuss with them?",
            "[Session llm-8, Turn 1] assistant: Good morning. I've added the reminder to call the mechanic at 2:00 PM today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003255844116211,
          "generation_latency_ms": 2776.638984680176,
          "total_latency_ms": 2787.642240524292
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user feel about the context switching required by the hybrid Java/Kotlin environment?",
          "answer_type": "short_answer",
          "gold_answer": "It is draining",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: That is a common pain point. Are you talking about the friction between the legacy Java code and the newer Kotlin additions?",
            "[Session llm-2, Turn 7] assistant: It seems that way; the system handles both the legacy Java files and the newer Kotlin implementations simultaneously.",
            "[Session llm-1, Turn 11] assistant: The current documentation for the core project suggests maintaining Java support while integrating Kotlin for newer components."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.004903793334961,
          "generation_latency_ms": 3182.3184490203857,
          "total_latency_ms": 3190.3233528137207
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-0",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Does the user consider the technical interoperability of the languages to be a problem?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-7, Turn 11] assistant: It shouldn't be too bad, though compiling both language sets in the core module does add a little overhead compared to a single-language build.",
            "[Session llm-3, Turn 2] user: Yeah, fingers crossed. I'm trying to update the technical readme for the core project. Are we strictly using one language for the backend now?",
            "[Session llm-9, Turn 2] user: We're reviewing the backend architecture for the core project. It's getting messy managing two different languages in the same repo."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.007143020629883,
          "generation_latency_ms": 2737.4064922332764,
          "total_latency_ms": 2747.4136352539062
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What database system is the LedgerFlow Core project connected to?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Are you referring to the main LedgerFlow Core repository?",
            "[Session llm-6, Turn 3] assistant: Are you referring to the core LedgerFlow project?",
            "[Session llm-8, Turn 2] user: Thanks. I was chatting with my cofounder earlier about the LedgerFlow core repo. We're bringing on a contractor and need to explain the stack."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.005401611328125,
          "generation_latency_ms": 3223.3731746673584,
          "total_latency_ms": 3234.3785762786865
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is Emma's last name?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 11] assistant: I haven't seen any new documents from Emma come through yet this morning.",
            "[Session llm-0, Turn 1] assistant: Good morning! I haven't seen Emma online just yet.",
            "[Session llm-7, Turn 1] assistant: I don't have eyes, but knowing Emma, it was probably something healthy to get through the meetings."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004043579101562,
          "generation_latency_ms": 3582.090377807617,
          "total_latency_ms": 3592.0944213867188
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what three components make up the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "company name, current year, exclamation mark",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: Yes, that matches the current security settings for the office wifi.",
            "[Session llm-7, Turn 4] user: Makes sense, interoperability is key. Oh, before I forget, my tablet dropped the connection. What's the office wifi pass again?",
            "[Session llm-0, Turn 8] user: Fingers crossed. Oh, they just asked for the wifi password. I always forget what we changed it to."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.007526397705078,
          "generation_latency_ms": 3774.3167877197266,
          "total_latency_ms": 3782.3243141174316
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What 'common pain point' did the assistant identify regarding the management of the hybrid repository?",
          "answer_type": "short_answer",
          "gold_answer": "Friction between legacy and new code",
          "predicted_answer": "Documenting the build process for the hybrid setup.",
          "retrieved_context": [
            "[Session llm-3, Turn 11] assistant: Glad she's connected. Is she needing access to the core repository as well?",
            "[Session llm-6, Turn 5] assistant: Looking at the repository, it seems to be a hybrid environment. There is a significant amount of Java code, but recent commits suggest a migration toward Kotlin is underway.",
            "[Session llm-5, Turn 3] assistant: Are you referring to the main LedgerFlow Core repository?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.999368667602539,
          "generation_latency_ms": 4204.597473144531,
          "total_latency_ms": 4216.596841812134
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_014.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 7,
      "ingestion_time_s": 0.8687858581542969,
      "total_evaluation_time_s": 40.65702986717224,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' in the context of the backend architecture review?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Charlotte",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: I need to review the backend architecture for the core project. My cofounder was asking about our tech debt.",
            "[Session llm-7, Turn 2] user: Yeah, I have a meeting with my cofounder later. Charlotte wants to go over the architecture for the core project.",
            "[Session llm-0, Turn 9] assistant: It is a common oversight. Is there anything else you need for your session with your cofounder?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.002250671386719,
          "generation_latency_ms": 3823.5867023468018,
          "total_latency_ms": 3838.5889530181885
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the descriptions of the legacy and new components, which two programming languages make up the hybrid stack of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java, Kotlin",
          "retrieved_context": [
            "[Session llm-1, Turn 2] user: Pretty busy. I'm trying to onboard a new dev onto the core repository. Do you remember which languages we're using for that project?",
            "[Session llm-1, Turn 12] user: Perfect. Back to the tech stack—are we using any other frameworks in the core project, or is it strictly those two JVM languages?",
            "[Session llm-3, Turn 9] assistant: Yes, the build system is handling the mixed source sets perfectly, so the two languages are talking to each other without issues."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999296188354492,
          "generation_latency_ms": 3265.054225921631,
          "total_latency_ms": 3278.0535221099854
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user struggled with the office internet on their phone, what specific task did they ask to be reminded of for noon?",
          "answer_type": "short_answer",
          "gold_answer": "Call the caterers",
          "predicted_answer": "call the caterers",
          "retrieved_context": [
            "[Session llm-0, Turn 10] user: Did she mention what time she wants to grab lunch?",
            "[Session llm-7, Turn 1] assistant: Good morning. Yes, the timer was reset. Is there anything else on your schedule for today?",
            "[Session llm-0, Turn 13] assistant: I have blocked off 1:00 PM to 2:00 PM on your calendar."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.008024215698242,
          "generation_latency_ms": 3444.1826343536377,
          "total_latency_ms": 3455.190658569336
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what three components make up the company's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "company name, the current year, and an exclamation mark",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Exactly. Oh, while I'm setting up this new laptop, what's the office wifi password again? I always forget the capitalization.",
            "[Session llm-7, Turn 8] user: Thanks. Oh, also, my cousin is dropping by the office to say hi. What's the wifi password again?",
            "[Session llm-8, Turn 3] assistant: Reminder set for the plants. For the wifi, it's just the company name followed by '2024' and an exclamation point."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002779006958008,
          "generation_latency_ms": 3370.872974395752,
          "total_latency_ms": 3381.87575340271
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What constraint does Charlotte explicitly prefer regarding the modernization of the core project's legacy code?",
          "answer_type": "generation",
          "gold_answer": "Do not rewrite everything",
          "predicted_answer": "without a complete rewrite",
          "retrieved_context": [
            "[Session llm-1, Turn 5] assistant: Charlotte has definitely been an advocate for that mix; it allows the team to modernize the codebase without a complete rewrite.",
            "[Session llm-7, Turn 2] user: Yeah, I have a meeting with my cofounder later. Charlotte wants to go over the architecture for the core project.",
            "[Session llm-9, Turn 3] assistant: Exactly. So you have the legacy code in the standard Oracle-owned language, but all the new modules Charlotte is adding are in that concise language from JetBrains."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 28.013229370117188,
          "generation_latency_ms": 3785.836696624756,
          "total_latency_ms": 3813.849925994873
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific SQL database vendor is the LedgerFlow Core project currently using?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: For the LedgerFlow Core project, it's currently a hybrid environment. The system relies on Java for the legacy components, but there's a significant amount of Kotlin being used for the newer services.",
            "[Session llm-7, Turn 3] assistant: That sounds important. Are you focusing on the database layer or the application logic for LedgerFlow Core?",
            "[Session llm-8, Turn 5] assistant: You mean the LedgerFlow Core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002281188964844,
          "generation_latency_ms": 3539.1011238098145,
          "total_latency_ms": 3547.1034049987793
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password string for the 'org:ledgerflow' wifi network?",
          "answer_type": "verbatim",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "LedgerFlow2024!",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Exactly. Oh, while I'm setting up this new laptop, what's the office wifi password again? I always forget the capitalization.",
            "[Session llm-7, Turn 8] user: Thanks. Oh, also, my cousin is dropping by the office to say hi. What's the wifi password again?",
            "[Session llm-4, Turn 10] user: Thanks. Also, I reset my phone and lost the saved networks. What's the office wifi secret again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.535737991333008,
          "generation_latency_ms": 4737.4937534332275,
          "total_latency_ms": 4751.029491424561
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 7,
        "count": 7,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 4,
            "accuracy": 1.0
          },
          "generation": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_020.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7683281898498535,
      "total_evaluation_time_s": 65.18519568443298,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention 'my cofounder' during the backend architecture discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 1] assistant: Hello. I can help with that. What did you and your cofounder discuss?",
            "[Session llm-5, Turn 2] user: No, I know where it is. I'm just stressed about the backend refactor. My cofounder wants to modernize the codebase.",
            "[Session llm-9, Turn 3] assistant: Sounds like a plan. Are you focusing on the legacy components or the newer modules your cofounder has been working on?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99958610534668,
          "generation_latency_ms": 3612.8973960876465,
          "total_latency_ms": 3621.896982192993
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which individual is primarily driving the modernization of the LedgerFlow Core codebase?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: A bit of both. We'll maintain the existing Java code, but we agreed to write all the new services for LedgerFlow Core in Kotlin.",
            "[Session llm-8, Turn 2] user: Just wanted to double-check something my cofounder mentioned earlier about the LedgerFlow Core repo.",
            "[Session llm-5, Turn 5] assistant: That sounds like a reasonable interoperability strategy for LedgerFlow Core given the JVM environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.01739501953125,
          "generation_latency_ms": 2999.4449615478516,
          "total_latency_ms": 3009.462356567383
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the formula provided by the assistant in the final session, what is the wifi password for the organization?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "LedgerFlow2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 8] user: Thanks. Also, I reset my phone and lost the saved networks. What's the wifi pass for the office?",
            "[Session llm-7, Turn 8] user: True. By the way, I reset my phone and lost the saved networks. What's the office wifi pass?",
            "[Session llm-6, Turn 11] assistant: Sure. The password for the office network is pretty standard. It's just our company name, the current year, and an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.518665313720703,
          "generation_latency_ms": 3351.1111736297607,
          "total_latency_ms": 3366.6298389434814
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What two programming languages constitute the hybrid stack of the LedgerFlow Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: A bit of both. We'll maintain the existing Java code, but we agreed to write all the new services for LedgerFlow Core in Kotlin.",
            "[Session llm-6, Turn 3] assistant: Right, for the main LedgerFlow Core project? It looks like a hybrid JVM environment right now.",
            "[Session llm-5, Turn 5] assistant: That sounds like a reasonable interoperability strategy for LedgerFlow Core given the JVM environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.52126693725586,
          "generation_latency_ms": 3728.9493083953857,
          "total_latency_ms": 3741.4705753326416
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user confirmed a dentist appointment for 3 PM, what work-related source of stress did they mention?",
          "answer_type": "short_answer",
          "gold_answer": "Backend refactor",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 1] assistant: Good morning. I've set a reminder for your dentist appointment at 2:00 PM today. Anything else on your mind?",
            "[Session llm-5, Turn 0] user: Hey, did you remind me about that dentist appointment later?",
            "[Session llm-1, Turn 0] user: Hey, good morning. Can you set a reminder for me to call the dentist at 2 PM?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.011341094970703,
          "generation_latency_ms": 4006.829023361206,
          "total_latency_ms": 4014.8403644561768
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user describe managing the core project as a 'headache' in the final session?",
          "answer_type": "short_answer",
          "gold_answer": "Managing two languages",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Sure, what specifically regarding the core project?",
            "[Session llm-0, Turn 2] user: Mostly about the architecture for the main project. We are definitely sticking to the JVM for the core.",
            "[Session llm-2, Turn 2] user: Diving in. I need to get the dev environment set up for the core project before my meeting with Sato."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3434.2689514160156,
          "total_latency_ms": 3444.2689418792725
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Has the user mentioned using Scala for any part of the LedgerFlow Core project?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: A bit of both. We'll maintain the existing Java code, but we agreed to write all the new services for LedgerFlow Core in Kotlin.",
            "[Session llm-5, Turn 5] assistant: That sounds like a reasonable interoperability strategy for LedgerFlow Core given the JVM environment.",
            "[Session llm-6, Turn 3] assistant: Right, for the main LedgerFlow Core project? It looks like a hybrid JVM environment right now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001991271972656,
          "generation_latency_ms": 3284.717082977295,
          "total_latency_ms": 3296.7190742492676
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the specific date of the user's dentist appointment?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 1] assistant: Good morning. I've set a reminder for your dentist appointment at 2:00 PM today. Anything else on your mind?",
            "[Session llm-5, Turn 0] user: Hey, did you remind me about that dentist appointment later?",
            "[Session llm-1, Turn 0] user: Hey, good morning. Can you set a reminder for me to call the dentist at 2 PM?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.989334106445312,
          "generation_latency_ms": 3267.648935317993,
          "total_latency_ms": 3276.6382694244385
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what three components make up the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Organization name, year, exclamation mark",
          "predicted_answer": "company name, the current year, and an exclamation mark",
          "retrieved_context": [
            "[Session llm-7, Turn 8] user: True. By the way, I reset my phone and lost the saved networks. What's the office wifi pass?",
            "[Session llm-4, Turn 8] user: Thanks. Also, I reset my phone and lost the saved networks. What's the wifi pass for the office?",
            "[Session llm-8, Turn 8] user: Not really, just some maintenance schedule. Anyway, I'm trying to connect my phone to the office Wi-Fi."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 2787.5242233276367,
          "total_latency_ms": 2796.5259552001953
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What configuration requirement does the assistant suggest is necessary when balancing verbose legacy syntax with modern JetBrains languages?",
          "answer_type": "short_answer",
          "gold_answer": "Careful configuration",
          "predicted_answer": "careful configuration",
          "retrieved_context": [
            "[Session llm-9, Turn 5] assistant: I can imagine. Balancing the verbose legacy syntax with the modern JetBrains-developed language usually requires careful configuration.",
            "[Session llm-1, Turn 3] assistant: Not exactly. The repository is still split. A good chunk of it remains in the standard legacy JVM language, but all the new services are being written using that concise syntax from JetBrains.",
            "[Session llm-4, Turn 5] assistant: Yes, the codebase contains significant amounts of both, allowing for interoperability."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001493453979492,
          "generation_latency_ms": 2782.674789428711,
          "total_latency_ms": 2791.6762828826904
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user explicitly prefer Java over Kotlin for the new modules?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-2, Turn 5] assistant: Not strictly. While there is still a significant amount of Java in the codebase, a lot of the newer modules in the core project are being written in Kotlin.",
            "[Session llm-5, Turn 4] user: Not completely. We're keeping the old Java parts for now, but all the new modules for the core project are being written in Kotlin.",
            "[Session llm-4, Turn 4] user: So we are supporting both Java and Kotlin files?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.522748947143555,
          "generation_latency_ms": 3748.725652694702,
          "total_latency_ms": 3757.2484016418457
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What is the user's preferred programming language for the LedgerFlow Core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: A bit of both. We'll maintain the existing Java code, but we agreed to write all the new services for LedgerFlow Core in Kotlin.",
            "[Session llm-4, Turn 2] user: I'm writing up some documentation for the new hires. What languages are we officially using for the core backend?",
            "[Session llm-5, Turn 5] assistant: That sounds like a reasonable interoperability strategy for LedgerFlow Core given the JVM environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001016616821289,
          "generation_latency_ms": 4052.302837371826,
          "total_latency_ms": 4061.3038539886475
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 8,
            "accuracy": 0.625
          },
          "boolean": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_021.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7306921482086182,
      "total_evaluation_time_s": 68.80137801170349,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder' throughout the sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: The main one for the organization. I think Liam changed it recently.",
            "[Session llm-0, Turn 4] user: Typical. My cofounder Liam insists on using the latest tech.",
            "[Session llm-8, Turn 3] assistant: Is your cofounder pushing for new features on the platform again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004831314086914,
          "generation_latency_ms": 3380.1510334014893,
          "total_latency_ms": 3389.155864715576
        },
        {
          "probe_id": "world_modeling-task_state_tracking-1",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What is the current migration status of the LedgerFlow Core project according to the user's discussions with Liam?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid environment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Always. We're looking at the main backend service for LedgerFlow right now. It's becoming a bit of a Frankenstein monster.",
            "[Session llm-2, Turn 7] assistant: It seems to be a gradual transition within the LedgerFlow Core repo rather than a hard cutover.",
            "[Session llm-1, Turn 1] assistant: Taking a look at the LedgerFlow Core project, it's currently a hybrid environment. You've got a significant chunk of the legacy code in Java, but the newer services are being written in Kotlin."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.008480072021484,
          "generation_latency_ms": 3713.9477729797363,
          "total_latency_ms": 3721.956253051758
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the assistant's instructions regarding the company name and current year, what is the organization's wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: The password for the org network is the company name, followed by 2024, and ending with an exclamation mark.",
            "[Session llm-4, Turn 6] user: Please do. I have a vendor coming in for a meeting. What is the current wifi password for the office?",
            "[Session llm-6, Turn 8] user: A little bit. Also, we have a consultant coming in later to help with the database. What's the office wireless password again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.0015869140625,
          "generation_latency_ms": 3785.6922149658203,
          "total_latency_ms": 3796.693801879883
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two programming languages are confirmed to be running in the hybrid backend of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Fair enough. I need to review the backend architecture for the core platform. What languages are we actually running on there?",
            "[Session llm-4, Turn 2] user: Thanks. I'm updating the developer onboarding wiki for the core project. Are we officially supporting multiple languages there?",
            "[Session llm-1, Turn 0] user: Hey, I'm trying to update the documentation for the core backend. What languages are we officially supporting in the repo right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.51785659790039,
          "generation_latency_ms": 3342.879295349121,
          "total_latency_ms": 3356.3971519470215
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user discussed the backend architecture and the 'old verbose stuff', what personal appointment did they ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist",
          "predicted_answer": "Dentist appointment",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: I've set a reminder for lunch. Regarding the core project, were you reviewing the source code structure?",
            "[Session llm-3, Turn 2] user: A bit chaotic. My cofounder is stressing about the backend stack for the core project.",
            "[Session llm-1, Turn 11] assistant: Just those two for the main backend. The architecture is strictly JVM-based right now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.002344131469727,
          "generation_latency_ms": 4060.8365535736084,
          "total_latency_ms": 4077.838897705078
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "What time was the dentist appointment set for in the reminder created during the discussion about the core platform's backend architecture?",
          "answer_type": "short_answer",
          "gold_answer": "2 PM",
          "predicted_answer": "2 PM",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: Consider it done. I've logged a reminder for the dentist appointment at 2 PM today.",
            "[Session llm-5, Turn 5] assistant: I've set a reminder for you to call the dentist at 2 PM.",
            "[Session llm-6, Turn 3] assistant: I've set a reminder for lunch. Regarding the core project, were you reviewing the source code structure?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.002706527709961,
          "generation_latency_ms": 4321.702718734741,
          "total_latency_ms": 4333.705425262451
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific JavaScript framework is used for the frontend of the LedgerFlow project?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 1] assistant: Taking a look at the LedgerFlow Core project, it's currently a hybrid environment. You've got a significant chunk of the legacy code in Java, but the newer services are being written in Kotlin.",
            "[Session llm-4, Turn 3] assistant: Yes, for the ledgerflowcore project, the backend is currently a hybrid environment using both Java and Kotlin.",
            "[Session llm-7, Turn 3] assistant: For the main LedgerFlow core, it's a mix. The legacy code is all standard Java, but remember that the team started implementing Kotlin for all the newer services."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.008832931518555,
          "generation_latency_ms": 3875.276803970337,
          "total_latency_ms": 3888.2856369018555
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the sandwich shop the user plans to visit for lunch?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 10] user: Great. I'm starving, do you know if the taco place is open?",
            "[Session llm-7, Turn 10] user: Perfect. I might grab lunch with my cofounder later then.",
            "[Session llm-6, Turn 2] user: That sounds perfect, remind me to go to that sandwich place down the street. Anyway, I was looking at the `ledgerflowcore` repo earlier."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000011444091797,
          "generation_latency_ms": 2695.3301429748535,
          "total_latency_ms": 2708.3301544189453
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What specific capitalization rule must be applied to the company name when constructing the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Capitalize L and F",
          "predicted_answer": "Capitalize the L and F",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: The password for the org network is the company name, followed by 2024, and ending with an exclamation mark.",
            "[Session llm-4, Turn 6] user: Please do. I have a vendor coming in for a meeting. What is the current wifi password for the office?",
            "[Session llm-2, Turn 3] assistant: It should be the standard one we set up. Just capitalize the company name, add the current year, and put an exclamation mark at the very end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.52403450012207,
          "generation_latency_ms": 4116.515636444092,
          "total_latency_ms": 4127.039670944214
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What punctuation mark is required at the end of the generated wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Exclamation point",
          "predicted_answer": "exclamation mark",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: The password for the org network is the company name, followed by 2024, and ending with an exclamation mark.",
            "[Session llm-4, Turn 6] user: Please do. I have a vendor coming in for a meeting. What is the current wifi password for the office?",
            "[Session llm-0, Turn 6] user: True. Hey, I'm at the new office and my phone isn't connecting. What's the wifi pass again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.0015869140625,
          "generation_latency_ms": 3369.6343898773193,
          "total_latency_ms": 3380.635976791382
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What is the user's stated preference regarding the use of 'old verbose' syntax versus 'modern' syntax?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "The user prefers the modern, concise syntax.",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: My cofounder was asking about the backend languages. Are we still strictly on the old verbose stuff, or have we moved to the modern JetBrains syntax?",
            "[Session llm-3, Turn 4] user: Yeah. We have so much legacy code in that verbose, older JVM language. He's hesitant to mix in the newer, concise one I like.",
            "[Session llm-8, Turn 6] user: Yeah, exactly. We're trying to migrate to that modern JetBrains language, but half the repo is still in the old Oracle-owned syntax."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.006095886230469,
          "generation_latency_ms": 4096.037864685059,
          "total_latency_ms": 4104.043960571289
        },
        {
          "probe_id": "preference_learning-preference_scope-1",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user prefer using a specific IDE for their Java development?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: Yes, the legacy backend is still Java-based, though all the new modules are being written in Kotlin.",
            "[Session llm-5, Turn 3] assistant: It is currently a mixed environment. The system still relies heavily on Java, but there is significant adoption of Kotlin for the newer components.",
            "[Session llm-5, Turn 10] user: Sounds good. Back to the code—does the Java interoperability cause any issues with the new Kotlin builds?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.997201919555664,
          "generation_latency_ms": 3548.737049102783,
          "total_latency_ms": 3557.734251022339
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 12,
            "accuracy": 0.5
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_030.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7068824768066406,
      "total_evaluation_time_s": 71.99941873550415,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Morning. Has my cofounder arrived yet?",
            "[Session llm-3, Turn 5] assistant: Is my cofounder still debating the language choice? I thought we settled on a mix for the main repository.",
            "[Session llm-1, Turn 2] user: I need to settle a debate with my cofounder. She thinks we should rewrite everything in Go."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.999897003173828,
          "generation_latency_ms": 3109.903573989868,
          "total_latency_ms": 3117.903470993042
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the organization that the user and Ava are associated with?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: We actually just use the main network for guests now. The password is the company name, followed by the current year, ending with an exclamation mark.",
            "[Session llm-8, Turn 2] user: Thanks. Also, my cofounder Ava is asking about the tech stack for the main project repo. I blanked on the specifics.",
            "[Session llm-1, Turn 3] assistant: Ava is always looking for performance gains, isn't she? What's her main argument against the current stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.001131057739258,
          "generation_latency_ms": 5882.829427719116,
          "total_latency_ms": 5896.8305587768555
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages are currently used in the hybrid environment of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-5, Turn 11] assistant: Yes, the core project relies on the interoperability between those two languages to function correctly during the transition.",
            "[Session llm-8, Turn 13] assistant: Yes, the codebase indicates a significant presence of both languages.",
            "[Session llm-2, Turn 5] assistant: Are you referring to the language mix in the core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.003473281860352,
          "generation_latency_ms": 3865.8134937286377,
          "total_latency_ms": 3873.816967010498
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the exact password for the LedgerFlow office wifi?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "LedgerFlow2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: Alright, I'll catch her then. I'm trying to get my new tablet connected to the network in the meantime. What's the password for the office wifi again?",
            "[Session llm-8, Turn 8] user: Cool. By the way, I'm setting up a new device in the office. What's the pass for the `org:ledgerflow` network again?",
            "[Session llm-2, Turn 3] assistant: For the LedgerFlow office network, the password is just the company name, capitalized normally, followed by 2024 and an exclamation point."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.005380630493164,
          "generation_latency_ms": 2880.3811073303223,
          "total_latency_ms": 2888.3864879608154
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the most recent conversation, why did the user decline the assistant's offer to check Ava's calendar?",
          "answer_type": "short_answer",
          "gold_answer": "To let her rest",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Good morning. I haven't seen Ava log in just yet. Would you like me to check her calendar?",
            "[Session llm-1, Turn 3] assistant: Ava is always looking for performance gains, isn't she? What's her main argument against the current stack?",
            "[Session llm-8, Turn 5] assistant: It happens during transitions. Do you want me to draft a summary for Ava?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.012025833129883,
          "generation_latency_ms": 4257.854700088501,
          "total_latency_ms": 4272.866725921631
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the hybrid Java and Kotlin stack occur before or after the user first asked for the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: That's correct. While a lot of the legacy code in the core project is still Java, the team has been implementing all new features using Kotlin.",
            "[Session llm-0, Turn 5] assistant: That's correct. The codebase is currently a hybrid, utilizing both the legacy Java files and the newer Kotlin components.",
            "[Session llm-4, Turn 8] user: Exactly. We have so many legacy Java files, but all the new modules Ava wrote are in Kotlin. It builds fine, but reading it is a bit of a mental switch."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.000675201416016,
          "generation_latency_ms": 4037.8568172454834,
          "total_latency_ms": 4054.8574924468994
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's behavior in the morning, how do they prefer to handle Ava's schedule when she has been up late?",
          "answer_type": "short_answer",
          "gold_answer": "Do not disturb",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Good morning. I haven't seen Ava log in just yet. Would you like me to check her calendar?",
            "[Session llm-4, Turn 1] assistant: She's currently in a deep work block, but her calendar opens up around 11:30 AM.",
            "[Session llm-1, Turn 3] assistant: Ava is always looking for performance gains, isn't she? What's her main argument against the current stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.007526397705078,
          "generation_latency_ms": 4350.868940353394,
          "total_latency_ms": 4358.876466751099
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to check Ava's calendar immediately upon arriving at the office?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Good morning. I haven't seen Ava log in just yet. Would you like me to check her calendar?",
            "[Session llm-2, Turn 9] assistant: Checking your calendar... yes, you have a sync with design at 2:00 PM.",
            "[Session llm-4, Turn 1] assistant: She's currently in a deep work block, but her calendar opens up around 11:30 AM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.008096694946289,
          "generation_latency_ms": 3923.5501289367676,
          "total_latency_ms": 3933.558225631714
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database system is the LedgerFlow core project connected to?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 9] assistant: For the LedgerFlow Core? You can tell your cofounder that it is running on the JVM, utilizing a hybrid of Java for the older components and Kotlin for the newer modules.",
            "[Session llm-8, Turn 3] assistant: No problem. For the `ledgerflowcore` project, it looks like a hybrid JVM environment. I'm seeing both Java and Kotlin usage in there.",
            "[Session llm-1, Turn 4] user: She thinks the current setup is too bloated. But for the core LedgerFlow engine, I want to stick with what we have."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.003774642944336,
          "generation_latency_ms": 3882.725715637207,
          "total_latency_ms": 3899.7294902801514
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's dentist?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 5] assistant: You're welcome. By the way, don't forget you have a dentist appointment at 3 PM today.",
            "[Session llm-9, Turn 10] user: Yes, let's do that for Thursday. Also, remind me to pick up my dry cleaning on the way to the dentist.",
            "[Session llm-3, Turn 7] assistant: Right, the one from JetBrains. So it'll be a hybrid environment with the classic verbose syntax alongside the modern stuff."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3300.980567932129,
          "total_latency_ms": 3309.9822998046875
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What are the three components of the pattern used to construct the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation mark",
          "predicted_answer": "company name, 2024, exclamation point",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: Alright, I'll catch her then. I'm trying to get my new tablet connected to the network in the meantime. What's the password for the office wifi again?",
            "[Session llm-2, Turn 3] assistant: For the LedgerFlow office network, the password is just the company name, capitalized normally, followed by 2024 and an exclamation point.",
            "[Session llm-0, Turn 9] assistant: We actually just use the main network for guests now. The password is the company name, followed by the current year, ending with an exclamation mark."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999109268188477,
          "generation_latency_ms": 3744.8296546936035,
          "total_latency_ms": 3753.828763961792
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "If the year changes to 2025, what would the new LedgerFlow wifi password likely be based on the established pattern?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2025!",
          "predicted_answer": "LedgerFlow2025!",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: Cool. By the way, I'm setting up a new device in the office. What's the pass for the `org:ledgerflow` network again?",
            "[Session llm-2, Turn 3] assistant: For the LedgerFlow office network, the password is just the company name, capitalized normally, followed by 2024 and an exclamation point.",
            "[Session llm-4, Turn 2] user: Alright, I'll catch her then. I'm trying to get my new tablet connected to the network in the meantime. What's the password for the office wifi again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002613067626953,
          "generation_latency_ms": 3693.0177211761475,
          "total_latency_ms": 3703.0203342437744
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 9,
            "accuracy": 0.6666666666666666
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_034.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.9027459621429443,
      "total_evaluation_time_s": 69.09455132484436,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual the user frequently debates with regarding the backend architecture of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Ming",
          "predicted_answer": "Ming",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Yeah, actually. My cofounder and I were arguing about the backend for the core project earlier.",
            "[Session llm-8, Turn 1] assistant: The core project is essentially a hybrid environment right now; it relies heavily on Java, but there is also a significant amount of Kotlin code involved.",
            "[Session llm-5, Turn 2] user: Not yet, I'm dying for a latte. Speaking of work, Ming was asking about the backend stack for the core project."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00213623046875,
          "generation_latency_ms": 4485.618114471436,
          "total_latency_ms": 4495.620250701904
        },
        {
          "probe_id": "world_modeling-task_state_tracking-1",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What specific task did the user need to complete regarding the LedgerFlow Core project before their cofounder arrived?",
          "answer_type": "short_answer",
          "gold_answer": "Review backend pull requests",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: That sounds important. Documenting the foundation of Project LedgerFlow is a great idea. What specifics did you discuss with him?",
            "[Session llm-9, Turn 3] assistant: Is your cofounder pushing for a specific technology for the LedgerFlow project again?",
            "[Session llm-6, Turn 3] assistant: I have added a task to your list to review the LedgerFlow Core PRs."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001182556152344,
          "generation_latency_ms": 3916.0845279693604,
          "total_latency_ms": 3926.0857105255127
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the descriptions of an 'Oracle-owned language' and a 'JetBrains alternative', which two frameworks comprise the core project's hybrid environment?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Yes, the core codebase is still split. It seems to be a combination of the classic Oracle-owned language and the newer one from JetBrains that Ming is really fond of.",
            "[Session llm-8, Turn 1] assistant: The core project is essentially a hybrid environment right now; it relies heavily on Java, but there is also a significant amount of Kotlin code involved.",
            "[Session llm-0, Turn 5] assistant: Right, the hybrid approach. Are you referring to the blend of Java and Kotlin for the core services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.008573532104492,
          "generation_latency_ms": 3841.472864151001,
          "total_latency_ms": 3851.4814376831055
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password string required to access the LedgerFlow organization's wifi network?",
          "answer_type": "verbatim",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "LedgerFlow2024!",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Yes, for the main org network, the password is just the company name combined with the current year, ending with an exclamation mark. Watch the capitalization on the name.",
            "[Session llm-8, Turn 4] user: Thanks. I'm trying to get this machine on the office network. What's the wireless credentials for the building again?",
            "[Session llm-8, Turn 14] user: Will do. Just to confirm, the wifi password ends with an exclamation point, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 3678.8859367370605,
          "total_latency_ms": 3687.8867149353027
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user asked for wifi credentials for their tablet, what specific beverage did they mention craving?",
          "answer_type": "short_answer",
          "gold_answer": "Latte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Hey, I'm trying to get my tablet on the office network, but I forgot the creds. Is it just the company name?",
            "[Session llm-4, Turn 6] user: Good point. Oh, also, a vendor is coming by the office later and asked for internet access. What's the wireless key again?",
            "[Session llm-6, Turn 8] user: Somewhere quiet. Also, my tablet dropped the connection to the office network. What was the passkey again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00413703918457,
          "generation_latency_ms": 3138.5669708251953,
          "total_latency_ms": 3150.57110786438
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the incident where the user left their wallet on the desk occur before or after they asked for a reminder to pick up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Hey, can you set a reminder for me to pick up my dry cleaning at 5 PM?",
            "[Session llm-7, Turn 0] user: Hey, hope you're having a good morning. Can you set a reminder for me to pick up dry cleaning at 5?",
            "[Session llm-9, Turn 1] assistant: Done. I've set a reminder for your dry cleaning at 5:00 PM. Anything else?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.019447326660156,
          "generation_latency_ms": 3896.026849746704,
          "total_latency_ms": 3904.0462970733643
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Between the two languages used in the core project, which one does the user prefer due to its conciseness?",
          "answer_type": "short_answer",
          "gold_answer": "Kotlin",
          "predicted_answer": "Kotlin",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: Not bad, just grabbing my third coffee. Ming was asking about the backend language for the main core project earlier.",
            "[Session llm-8, Turn 1] assistant: The core project is essentially a hybrid environment right now; it relies heavily on Java, but there is also a significant amount of Kotlin code involved.",
            "[Session llm-4, Turn 3] assistant: Yes, the core codebase is still split. It seems to be a combination of the classic Oracle-owned language and the newer one from JetBrains that Ming is really fond of."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99195671081543,
          "generation_latency_ms": 3354.8552989959717,
          "total_latency_ms": 3363.847255706787
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific coffee drink does the user explicitly state they are 'dying for'?",
          "answer_type": "short_answer",
          "gold_answer": "Latte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 10] user: Perfect. I was worried the legacy stuff would break. Oh, did I leave my coffee mug on the meeting room table?",
            "[Session llm-6, Turn 6] user: No, I think we have it handled, just need to be careful. Oh, switching gears, can you find a good coffee shop nearby for a meeting at 2 PM?",
            "[Session llm-8, Turn 6] user: Got it, that worked. I was worried we changed it recently. Did you see that new coffee shop opened down the street?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.01484489440918,
          "generation_latency_ms": 3667.1712398529053,
          "total_latency_ms": 3676.1860847473145
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific JavaScript framework is used for the frontend of the LedgerFlow Core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Right, regarding the LedgerFlow Core. It is running on the JVM. From what I recall, it's a hybrid codebase.",
            "[Session llm-2, Turn 3] assistant: Taking a look at `ledgerflowcore`, it appears to be a hybrid environment. It started with standard Java, but there's a significant amount of Kotlin being introduced in the newer modules.",
            "[Session llm-2, Turn 2] user: The core backend for the LedgerFlow project. I forgot what the primary language breakdown looks like."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.005857467651367,
          "generation_latency_ms": 4306.694984436035,
          "total_latency_ms": 4314.7008419036865
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the Java environment for the core project running on version 17 or version 21?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 4] user: Yeah, I'm updating the technical brief for the core platform. Are we strictly using Java on the backend now?",
            "[Session llm-8, Turn 1] assistant: The core project is essentially a hybrid environment right now; it relies heavily on Java, but there is also a significant amount of Kotlin code involved.",
            "[Session llm-4, Turn 11] assistant: It looks integrated; the build system compiles both the Kotlin files and the Java sources together in the same tree."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.017778396606445,
          "generation_latency_ms": 3850.984573364258,
          "total_latency_ms": 3859.0023517608643
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant, what three components must be combined to form the organization's wifi credentials?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "organization's name with a capital letter, the current year, and an exclamation point",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Thanks. I'm trying to get this machine on the office network. What's the wireless credentials for the building again?",
            "[Session llm-9, Turn 9] assistant: The credentials for the office network should still be the same. Do you need the passkey?",
            "[Session llm-2, Turn 7] assistant: Yes, for the main org network, the password is just the company name combined with the current year, ending with an exclamation mark. Watch the capitalization on the name."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.019208908081055,
          "generation_latency_ms": 3697.0419883728027,
          "total_latency_ms": 3705.061197280884
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Using the established pattern for the organization's network credentials, what is the password for the current year?",
          "answer_type": "short_answer",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Yes, for the main org network, the password is just the company name combined with the current year, ending with an exclamation mark. Watch the capitalization on the name.",
            "[Session llm-8, Turn 4] user: Thanks. I'm trying to get this machine on the office network. What's the wireless credentials for the building again?",
            "[Session llm-9, Turn 9] assistant: The credentials for the office network should still be the same. Do you need the passkey?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.01992416381836,
          "generation_latency_ms": 3641.3865089416504,
          "total_latency_ms": 3649.4064331054688
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_fintech_047.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7269670963287354,
      "total_evaluation_time_s": 65.8366711139679,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual described as being obsessed with moving the backend to a JetBrains-developed language?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Noah",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Thanks. We're still debating the tech stack for the core project. He's obsessed with moving everything over to that JetBrains language.",
            "[Session llm-2, Turn 4] user: Yeah. We have so much in that classic verbose language, but all the new stuff is in the JetBrains one.",
            "[Session llm-7, Turn 3] assistant: Correct. The core project is a hybrid environment. It utilizes the classic Oracle-owned language for the older services, but the team has been transitioning to that JetBrains-developed language for all the newer components since they interoperate so well."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.989334106445312,
          "generation_latency_ms": 3888.5648250579834,
          "total_latency_ms": 3897.5541591644287
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and the person running late for the sync?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "cofounder",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Can you set a reminder for 2 PM? I need to sync up with my cofounder about the backend architecture.",
            "[Session llm-9, Turn 2] user: No, it's fine. I just need to sync with him on the backend refactor. We're trying to clean up the legacy code in the core repo.",
            "[Session llm-3, Turn 1] assistant: Yes, I have it set for 5:30 PM today. I also noticed you have a gap in your schedule before lunch."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.531280517578125,
          "generation_latency_ms": 3314.18776512146,
          "total_latency_ms": 3326.719045639038
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the descriptions provided, which two programming languages make up the 'Frankenstein monster' codebase of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Java and Kotlin",
          "predicted_answer": "Java and Kotlin",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Correct. The core project is a hybrid environment. It utilizes the classic Oracle-owned language for the older services, but the team has been transitioning to that JetBrains-developed language for all the newer components since they interoperate so well.",
            "[Session llm-8, Turn 3] assistant: Managing a hybrid codebase can be tricky. What mix of languages are you dealing with in the core project?",
            "[Session llm-9, Turn 5] assistant: Managing a hybrid codebase like that can be tricky, especially with interoperability between the two languages."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000944137573242,
          "generation_latency_ms": 3448.119878768921,
          "total_latency_ms": 3458.120822906494
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the LedgerFlow organization network?",
          "answer_type": "verbatim",
          "gold_answer": "LedgerFlow2024!",
          "predicted_answer": "LedgerFlow2024!",
          "retrieved_context": [
            "[Session llm-0, Turn 11] assistant: It happens. The password for the LedgerFlow network is LedgerFlow2024!",
            "[Session llm-2, Turn 9] assistant: For the office network, the password is just the company name combined with the current year and an exclamation point.",
            "[Session llm-1, Turn 7] assistant: I believe so. The record I have shows the network key as the company name, capitalized, followed by the current year and an exclamation point. So, LedgerFlow2024! should work."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001016616821289,
          "generation_latency_ms": 3437.8111362457275,
          "total_latency_ms": 3446.812152862549
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where Noah was late for the sync, what activity did the user decline to do while waiting?",
          "answer_type": "short_answer",
          "gold_answer": "Go over meeting agenda",
          "predicted_answer": "Draft a message for when he arrives",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Yes, set it for 2 PM. Also, Noah is coming by the office later and he always forgets the internet login.",
            "[Session llm-5, Turn 1] assistant: Done. I've set a reminder for your 2 PM sync with Noah.",
            "[Session llm-4, Turn 0] user: Hey, have you seen Noah online yet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000301361083984,
          "generation_latency_ms": 4319.343566894531,
          "total_latency_ms": 4328.343868255615
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user successfully set a dentist reminder in the session immediately following the 'chaotic' morning where they explained the architecture to a new hire?",
          "answer_type": "boolean",
          "gold_answer": "Yes",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Right, the interoperability is key. Switching gears, did I leave a reminder to call the dentist?",
            "[Session llm-7, Turn 5] assistant: I don't see a reminder for the dentist in your active list. Would you like me to set one for later today?",
            "[Session llm-8, Turn 0] user: Hey, can you set a reminder for me to call the dentist at 2 PM? Noah is running a bit late for our sync, so I have a moment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.005950927734375,
          "generation_latency_ms": 3717.4036502838135,
          "total_latency_ms": 3727.409601211548
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user appear hesitant to abandon completely, despite their cofounder's insistence?",
          "answer_type": "short_answer",
          "gold_answer": "Java",
          "predicted_answer": "Java",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: Thanks. Back to the tech stack—can you confirm what languages we are actually compiling in the main repository right now?",
            "[Session llm-5, Turn 2] user: Thanks. We're still debating the tech stack for the core project. He's obsessed with moving everything over to that JetBrains language.",
            "[Session llm-4, Turn 6] user: Ugh, right. I might need to leave early. Hey, regarding the core project, are we strictly using one language for the backend now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 2964.6406173706055,
          "total_latency_ms": 2974.6410846710205
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What is Noah's preferred language for writing new modules in the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Kotlin",
          "predicted_answer": "Kotlin",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Correct. The core project is a hybrid environment. It utilizes the classic Oracle-owned language for the older services, but the team has been transitioning to that JetBrains-developed language for all the newer components since they interoperate so well.",
            "[Session llm-8, Turn 4] user: Well, it's mostly the legacy stuff written in Java, but Noah has been insisting on writing all the new modules in Kotlin.",
            "[Session llm-3, Turn 7] assistant: Is Noah still mixing the classic Oracle-owned language with the newer one from JetBrains?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003971099853516,
          "generation_latency_ms": 2981.2545776367188,
          "total_latency_ms": 2992.2585487365723
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is used in the LedgerFlow Core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Looking at the codebase for project ledgerflowcore, it appears to be a hybrid environment. It's utilizing both Java and Kotlin right now.",
            "[Session llm-9, Turn 3] assistant: Is that for the main LedgerFlow core project?",
            "[Session llm-1, Turn 3] assistant: Taking a look at the LedgerFlow Core project, it seems to be a hybrid situation. It's definitely running on the JVM, so there's plenty of Java, but I'm seeing a lot of Kotlin in the mix now too."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.004903793334961,
          "generation_latency_ms": 3827.155351638794,
          "total_latency_ms": 3835.160255432129
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the model of the new phone the user tried to connect to the office internet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 10] user: Total subject change—my phone isn't connecting to the office internet. What's the password again?",
            "[Session llm-9, Turn 8] user: Thanks. Also, my phone dropped the wifi. Did we change the password for the office network?",
            "[Session llm-4, Turn 2] user: Okay, no worries. I'm trying to connect my new phone to the office internet. Do we still have that same password for the org network?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.014129638671875,
          "generation_latency_ms": 2614.914655685425,
          "total_latency_ms": 2623.9287853240967
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's explanation, what is the rule for constructing the LedgerFlow wifi password?",
          "answer_type": "generation",
          "gold_answer": "Capitalized company name, year, exclamation mark",
          "predicted_answer": "The company name capitalized, followed by the current year and an exclamation point.",
          "retrieved_context": [
            "[Session llm-0, Turn 11] assistant: It happens. The password for the LedgerFlow network is LedgerFlow2024!",
            "[Session llm-6, Turn 0] user: Can you remind me what the wifi pass is for the office?",
            "[Session llm-2, Turn 9] assistant: For the office network, the password is just the company name combined with the current year and an exclamation point."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003732681274414,
          "generation_latency_ms": 4011.600971221924,
          "total_latency_ms": 4022.6047039031982
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user asked if they had left a reminder for the dentist during the 'chaotic' morning, what was the system's finding?",
          "answer_type": "short_answer",
          "gold_answer": "No reminder found",
          "predicted_answer": "I don't see a reminder for the dentist in your active list.",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Right, the interoperability is key. Switching gears, did I leave a reminder to call the dentist?",
            "[Session llm-8, Turn 0] user: Hey, can you set a reminder for me to call the dentist at 2 PM? Noah is running a bit late for our sync, so I have a moment.",
            "[Session llm-1, Turn 0] user: Good morning. Can you set a reminder for me to call the dentist at 2 PM?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000249862670898,
          "generation_latency_ms": 3234.264612197876,
          "total_latency_ms": 3247.264862060547
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 7,
            "accuracy": 0.8571428571428571
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_002.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8203370571136475,
      "total_evaluation_time_s": 67.59403109550476,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What two programming languages comprise the hybrid stack for the medpulsecore project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: Pretty busy. I'm diving back into the MedPulse core repository today. I honestly keep forgetting which specific languages we finalized for the backend stack.",
            "[Session llm-1, Turn 4] user: We are looking at the MedPulse core repo. I completely blanked on the backend languages we settled on.",
            "[Session llm-2, Turn 2] user: Yeah, pretty intense. Anyway, I'm looking at the MedPulse codebase. Are we sticking to just one language for the core services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.995906829833984,
          "generation_latency_ms": 3470.722198486328,
          "total_latency_ms": 3487.718105316162
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "Based on the formula provided to the cofounder, what is the exact string for the MedPulse wifi password?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-1, Turn 8] user: Perfect. One more thing, we have a consultant coming in today. What's the wifi password for the office again?",
            "[Session llm-5, Turn 8] user: Thanks. I'm sitting in the lobby and the network kicked me off. What's the wifi password for the office again?",
            "[Session llm-1, Turn 9] assistant: The password for the MedPulse network is MedPulse2024! -- don't forget the exclamation mark at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.018898010253906,
          "generation_latency_ms": 4220.310688018799,
          "total_latency_ms": 4229.329586029053
        },
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' having trouble with the internet?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Right, it was in the breakroom. Anyway, Olivia is here and she's having trouble logging into the internet.",
            "[Session llm-4, Turn 6] user: Makes sense. Oh, my cofounder just walked in and she's struggling to get her tablet on the office network.",
            "[Session llm-9, Turn 5] assistant: It happens. Remind my cofounder it's just the company name capitalized, followed by the current year, and then an exclamation mark."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999296188354492,
          "generation_latency_ms": 4154.475927352905,
          "total_latency_ms": 4167.47522354126
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific technology stack combination does Olivia explicitly love?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: Sounds good. Is Olivia reviewing the entire architecture or just specific components?",
            "[Session llm-8, Turn 2] user: Yeah, pretty intense. Anyway, Olivia was asking about the backend stack for the core platform. What are we officially running?",
            "[Session llm-5, Turn 11] assistant: Sounds good. Is Olivia bringing anything specific for the meeting?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.068012237548828,
          "generation_latency_ms": 3758.9943408966064,
          "total_latency_ms": 3768.0623531341553
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the conversation about the broken espresso machine occur before or after the user mentioned their dentist appointment?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: Good morning. I'm operating at optimal efficiency, but I can't say the same for the break room espresso machine.",
            "[Session llm-9, Turn 0] user: I just got back to the office, did I leave my coffee mug on the meeting table?",
            "[Session llm-7, Turn 9] assistant: I have set a reminder for 2:00 PM to call the dentist."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.020401000976562,
          "generation_latency_ms": 3281.193971633911,
          "total_latency_ms": 3289.2143726348877
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the discussion about setting up the new intern, what specific appliance did the assistant mention was not operating efficiently?",
          "answer_type": "short_answer",
          "gold_answer": "espresso machine",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: Good morning. I'm operating at optimal efficiency, but I can't say the same for the break room espresso machine.",
            "[Session llm-1, Turn 3] assistant: Sounds good. Is Olivia reviewing the entire architecture or just specific components?",
            "[Session llm-3, Turn 3] assistant: I recall the team decided to split the architecture, utilizing Python for the data processing side and Go for the microservices."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.99705696105957,
          "generation_latency_ms": 3548.5897064208984,
          "total_latency_ms": 3559.586763381958
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What is the primary reason cited for choosing Go for the backend services over other languages?",
          "answer_type": "short_answer",
          "gold_answer": "performance",
          "predicted_answer": "performance",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Right, for the core project, the backend services are being built out in Go for performance, while the analytics components are utilizing Python.",
            "[Session llm-2, Turn 2] user: Yeah, pretty intense. Anyway, I'm looking at the MedPulse codebase. Are we sticking to just one language for the core services?",
            "[Session llm-5, Turn 2] user: I'm trying to update the documentation for the MedPulse Core project. Can you remind me what the primary languages are for the backend and analytics?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 25.00772476196289,
          "generation_latency_ms": 3997.464895248413,
          "total_latency_ms": 4022.472620010376
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "For which specific domain of the MedPulse Core project is Python the preferred tool?",
          "answer_type": "short_answer",
          "gold_answer": "data analysis",
          "predicted_answer": "data analysis components",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, we're definitely sticking with Python for the data analysis components. It just handles the medical datasets better.",
            "[Session llm-9, Turn 12] user: Sticking with Python for that. The libraries are just better for what we need to do with the medical data.",
            "[Session llm-8, Turn 3] assistant: For the core project, we're doing a hybrid approach. We're using Python for the data analysis components and Go for the high-performance services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 23.00286293029785,
          "generation_latency_ms": 4066.8723583221436,
          "total_latency_ms": 4089.8752212524414
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What frontend framework is currently used for the MedPulse Core project?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: Pretty busy. I'm diving back into the MedPulse core repository today. I honestly keep forgetting which specific languages we finalized for the backend stack.",
            "[Session llm-1, Turn 4] user: We are looking at the MedPulse core repo. I completely blanked on the backend languages we settled on.",
            "[Session llm-5, Turn 2] user: I'm trying to update the documentation for the MedPulse Core project. Can you remind me what the primary languages are for the backend and analytics?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001276016235352,
          "generation_latency_ms": 3586.8945121765137,
          "total_latency_ms": 3598.895788192749
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the lead database administrator for the MedPulse organization?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 9] assistant: The password for the MedPulse network is MedPulse2024! -- don't forget the exclamation mark at the end.",
            "[Session llm-0, Turn 7] assistant: It did recently. The new credential for the org network is basically the company name followed by the current year and a bang.",
            "[Session llm-4, Turn 9] assistant: Yes, the password is MedPulse2024! — make sure she includes the exclamation point at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002851486206055,
          "generation_latency_ms": 2862.173557281494,
          "total_latency_ms": 2872.1764087677
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the three-part formula used to construct the MedPulse wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation mark",
          "predicted_answer": "Company name, current year, and an exclamation point",
          "retrieved_context": [
            "[Session llm-1, Turn 8] user: Perfect. One more thing, we have a consultant coming in today. What's the wifi password for the office again?",
            "[Session llm-5, Turn 8] user: Thanks. I'm sitting in the lobby and the network kicked me off. What's the wifi password for the office again?",
            "[Session llm-6, Turn 4] user: Right, Olivia loves that combination. Speaking of setup, what's the guest wifi password again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002374649047852,
          "generation_latency_ms": 3466.0940170288086,
          "total_latency_ms": 3476.0963916778564
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What software installation steps were prescribed for setting up the new intern's environment?",
          "answer_type": "short_answer",
          "gold_answer": "Install Python and Go",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Tragic. Anyway, I'm trying to get the new intern set up on the main repo. Which languages do they need installed?",
            "[Session llm-1, Turn 3] assistant: Sounds good. Is Olivia reviewing the entire architecture or just specific components?",
            "[Session llm-3, Turn 3] assistant: I recall the team decided to split the architecture, utilizing Python for the data processing side and Go for the microservices."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.997772216796875,
          "generation_latency_ms": 3595.062494277954,
          "total_latency_ms": 3606.060266494751
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_003.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7594156265258789,
      "total_evaluation_time_s": 67.81122088432312,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder who insists on using Go?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: Thanks. My cofounder is dropping by the office later and asked for the internet credentials.",
            "[Session llm-3, Turn 6] user: For the API and microservices, she wants to use Go. She thinks it handles the load better.",
            "[Session llm-7, Turn 14] user: Thanks. One last check, the Go implementation is for the high-concurrency stuff, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.514114379882812,
          "generation_latency_ms": 3836.4057540893555,
          "total_latency_ms": 3848.9198684692383
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific component of the MedPulse core project is the Python language associated with?",
          "answer_type": "short_answer",
          "gold_answer": "Data processing",
          "predicted_answer": "data layers",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the current setup for MedPulse Core, it appears to be a mix. The backend relies on Go, but there's also significant Python usage involved.",
            "[Session llm-6, Turn 3] assistant: Yes, for the medpulsecore backend, we remain committed to using Go for the services and Python for the data analysis tools.",
            "[Session llm-0, Turn 11] assistant: Aside from the Python and Go components we discussed, nothing else is explicitly flagged as a major framework right now."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.998891830444336,
          "generation_latency_ms": 4233.160734176636,
          "total_latency_ms": 4245.15962600708
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the sessions, what two programming languages make up the hybrid stack for the MedPulse core repository?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Go and Python",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the current setup for MedPulse Core, it appears to be a mix. The backend relies on Go, but there's also significant Python usage involved.",
            "[Session llm-5, Turn 3] assistant: That makes sense; a hybrid approach is pretty effective. I can help set up a pre-commit hook that handles both languages if you want to maintain the MedPulse Core standards.",
            "[Session llm-4, Turn 5] assistant: Sounds like a busy day. Do you need the documentation for the MedPulse core repository?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.511159896850586,
          "generation_latency_ms": 3253.8650035858154,
          "total_latency_ms": 3264.376163482666
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact character string for the MedPulse organization's wifi password?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: For the MedPulse org network, the password is just the company name capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-0, Turn 7] assistant: For the office network, the password is 'MedPulse2024!'.",
            "[Session llm-4, Turn 12] user: Nice. I might walk home then. Just to double check, that wifi password was MedPulse2024! right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.004852294921875,
          "generation_latency_ms": 4199.116468429565,
          "total_latency_ms": 4211.121320724487
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder for dry cleaning before or after discussing the developer onboarding wiki?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: Noted. I've set a reminder for 5 PM to pick up the dry cleaning.",
            "[Session llm-5, Turn 4] user: Yeah, let's do that later. Also, can you remind me to pick up my dry cleaning at 5?",
            "[Session llm-6, Turn 2] user: Typical. I'm updating the developer onboarding wiki for the core project. We are still sticking to the same two languages, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999534606933594,
          "generation_latency_ms": 3153.5801887512207,
          "total_latency_ms": 3166.5797233581543
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user encounter environment errors when trying to run the backend in the final recorded session?",
          "answer_type": "short_answer",
          "gold_answer": "Missing language runtime",
          "predicted_answer": "Missing a language runtime.",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: I'm trying to get the backend running for the main project, but I keep hitting environment errors.",
            "[Session llm-1, Turn 6] user: We aren't fully switching, just splitting it. The backend services are going to be in Go now.",
            "[Session llm-8, Turn 3] assistant: Are you focusing on the backend services today?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 3586.345672607422,
          "total_latency_ms": 3596.3470935821533
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user's cofounder strongly prefer for high-performance services?",
          "answer_type": "short_answer",
          "gold_answer": "Go",
          "predicted_answer": "Go",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Right, it's a mix. Emma insists on using Go for the high-performance services, but a lot of the data processing code is still Python.",
            "[Session llm-3, Turn 3] assistant: That's not uncommon for complex systems. Which frameworks is your cofounder looking at for the core build?",
            "[Session llm-8, Turn 4] user: Yeah, specifically the interoperability between our two main languages. It's tricky managing the snake-based scripts alongside the compiled services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.008646011352539,
          "generation_latency_ms": 3670.3028678894043,
          "total_latency_ms": 3679.311513900757
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific linter did the user explicitly select for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "None",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Hey, I'm trying to organize the backend repo for the core project. Did we decide on a specific linter?",
            "[Session llm-5, Turn 1] assistant: Hello! For linting, it usually depends on the language files we are targeting. Are we focusing on the Python scripts or the Go modules right now?",
            "[Session llm-3, Turn 3] assistant: That's not uncommon for complex systems. Which frameworks is your cofounder looking at for the core build?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.519691467285156,
          "generation_latency_ms": 3648.697853088379,
          "total_latency_ms": 3663.217544555664
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the brand or model of the new laptop the user was trying to connect to the network?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 8] user: Right, obviously. Speaking of the office, Emma reset the router. What's the new wireless key?",
            "[Session llm-4, Turn 2] user: Yeah, I'm trying to get my new laptop connected to the office network. I completely blanked on the credentials.",
            "[Session llm-6, Turn 4] user: Perfect, just wanted to double-check before I commit the text. Also, my tablet dropped the connection. What's the office wireless key?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003400802612305,
          "generation_latency_ms": 2996.7782497406006,
          "total_latency_ms": 3005.781650543213
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific sports teams were mentioned in the headlines discussed in session 6?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 12] user: No worries, I'll check ESPN later. Back to the code—make sure the documentation reflects that we are dual-stack.",
            "[Session llm-5, Turn 11] assistant: I'm not sure about the sports scores, but I can look that up for you if you'd like.",
            "[Session llm-6, Turn 0] user: Good morning. Any interesting news in the headlines today?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.998466491699219,
          "generation_latency_ms": 3264.5516395568848,
          "total_latency_ms": 3272.550106048584
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what is the logic for constructing the MedPulse wifi password?",
          "answer_type": "generation",
          "gold_answer": "Company name capitalized, year, exclamation mark",
          "predicted_answer": "The company name capitalized, followed by the current year and an exclamation mark.",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: For the MedPulse org network, the password is just the company name capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-4, Turn 12] user: Nice. I might walk home then. Just to double check, that wifi password was MedPulse2024! right?",
            "[Session llm-0, Turn 7] assistant: For the office network, the password is 'MedPulse2024!'."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002374649047852,
          "generation_latency_ms": 3741.558313369751,
          "total_latency_ms": 3751.560688018799
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on the environment errors discussed, what action must the user take to successfully run the MedPulse core backend?",
          "answer_type": "short_answer",
          "gold_answer": "Install both Python and Go",
          "predicted_answer": "Enter MedPulse2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Are you referring to the MedPulse core repository? What kind of errors are you seeing?",
            "[Session llm-4, Turn 5] assistant: Sounds like a busy day. Do you need the documentation for the MedPulse core repository?",
            "[Session llm-0, Turn 3] assistant: Looking at the current setup for MedPulse Core, it appears to be a mix. The backend relies on Go, but there's also significant Python usage involved."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.495735168457031,
          "generation_latency_ms": 4749.448299407959,
          "total_latency_ms": 4758.944034576416
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 10,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 8,
            "accuracy": 0.75
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "generation": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_009.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6508266925811768,
      "total_evaluation_time_s": 67.22073101997375,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'business partner' or 'cofounder' throughout the sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 5] assistant: For the main network, it's just the company name capitalized, followed by the current year and an exclamation point.",
            "[Session llm-8, Turn 6] user: Perfect, that's what I thought. Also, my cofounder is dropping by the office in an hour.",
            "[Session llm-0, Turn 0] user: Hey, have you seen my cofounder this morning? I wanted to grab a quick coffee with him before the daily standup."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000322341918945,
          "generation_latency_ms": 3155.585765838623,
          "total_latency_ms": 3167.586088180542
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Noah?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-8, Turn 7] assistant: Whatever Noah needs, just let me know. Is he staying for the weekly sync?",
            "[Session llm-0, Turn 1] assistant: I haven't seen Noah yet, but the espresso machine in the kitchen is warmed up if you can't wait.",
            "[Session llm-6, Turn 10] user: Cool. Did Noah send over the slide deck?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.997295379638672,
          "generation_latency_ms": 4992.682218551636,
          "total_latency_ms": 5003.679513931274
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages are combined to form the 'hybrid approach' for the MedPulse Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-2, Turn 3] assistant: That is correct. The MedPulse core system utilizes both Python and Go for its architecture.",
            "[Session llm-0, Turn 2] user: I might just do that. By the way, regarding the MedPulse core project, did we finalize which languages we're supporting for the backend services?",
            "[Session llm-8, Turn 5] assistant: Right, the core system is currently a hybrid. We are using Python for the data processing layers and Go for the high-concurrency services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.998601913452148,
          "generation_latency_ms": 3869.0249919891357,
          "total_latency_ms": 3885.023593902588
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact wifi password for the MedPulse organization?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Standard procedure. The password for the office network is MedPulse2024! with an exclamation mark at the end.",
            "[Session llm-0, Turn 6] user: Thanks. One more thing, my tablet keeps dropping the connection. What is the wireless password for the office again?",
            "[Session llm-1, Turn 6] user: Maybe later. I'm currently fighting with the office internet. Did we change the wireless password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004281997680664,
          "generation_latency_ms": 2718.501567840576,
          "total_latency_ms": 2728.505849838257
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about the rain, what happened to their umbrella?",
          "answer_type": "short_answer",
          "gold_answer": "Turned inside out",
          "predicted_answer": "Turned inside out halfway there.",
          "retrieved_context": [
            "[Session llm-3, Turn 6] user: Thanks. Also, did the weather report say it's going to rain today?",
            "[Session llm-3, Turn 7] assistant: I don't have a live weather feed, but it never hurts to keep an umbrella handy just in case.",
            "[Session llm-6, Turn 8] user: Got it, thanks. Is it raining outside?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.020162582397461,
          "generation_latency_ms": 3632.829427719116,
          "total_latency_ms": 3640.8495903015137
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the team decide to use Go for the backend services instead of Python?",
          "answer_type": "short_answer",
          "gold_answer": "High concurrency",
          "predicted_answer": "API performance, high-concurrency services, and scaling",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: It looks like the codebase is a hybrid. There's a significant amount of Golang handling the services, but the data processing side is still heavily relying on Python.",
            "[Session llm-1, Turn 3] assistant: For the MedPulse Core, I believe the backend is split between Go for the services and Python for the analytics components.",
            "[Session llm-3, Turn 9] assistant: The current documentation doesn't specify the web framework, just that Python is integral to the core project alongside the Go components."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.512901306152344,
          "generation_latency_ms": 4448.927402496338,
          "total_latency_ms": 4458.44030380249
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-6",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Which programming language does the user describe as 'non-negotiable' for the ML components?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Right, that makes sense. Python is non-negotiable for the ML stuff.",
            "[Session llm-3, Turn 2] user: Yeah, actually. I was looking through the repo for the core platform. What languages are we actually running in production now?",
            "[Session llm-9, Turn 5] assistant: Ah, the classic trade-off. I assume the data science side is all Python based?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.018255233764648,
          "generation_latency_ms": 2677.042007446289,
          "total_latency_ms": 2685.0602626800537
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine in Session llm-7, what specific type of coffee do they require to start the day?",
          "answer_type": "short_answer",
          "gold_answer": "Double shot espresso",
          "predicted_answer": "double shot of espresso",
          "retrieved_context": [
            "[Session llm-4, Turn 12] user: Perfect. I need to grab some coffee first.",
            "[Session llm-7, Turn 2] user: No, I'm already drinking it. Just waiting for the caffeine to hit. Noah and I were arguing about the backend for the core project again.",
            "[Session llm-3, Turn 0] user: Hey, hope you're having a good morning. Can you remind me to pick up coffee later?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004354476928711,
          "generation_latency_ms": 3286.7937088012695,
          "total_latency_ms": 3295.7980632781982
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology (e.g., PostgreSQL, MongoDB) is the MedPulse core project using?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: Right, the core system is currently a hybrid. We are using Python for the data processing layers and Go for the high-concurrency services.",
            "[Session llm-1, Turn 3] assistant: For the MedPulse Core, I believe the backend is split between Go for the services and Python for the analytics components.",
            "[Session llm-2, Turn 3] assistant: That is correct. The MedPulse core system utilizes both Python and Go for its architecture."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.019779205322266,
          "generation_latency_ms": 3622.0602989196777,
          "total_latency_ms": 3632.080078125
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the user's first name?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: No, just a quick visit. He'll probably ask for the internet login though, he never remembers it.",
            "[Session llm-3, Turn 5] assistant: For the main network, it's just the company name capitalized, followed by the current year and an exclamation point.",
            "[Session llm-7, Turn 7] assistant: I do. It's just the organization's name capitalized, followed by the current year and an exclamation point."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.009983062744141,
          "generation_latency_ms": 2869.6601390838623,
          "total_latency_ms": 2876.6701221466064
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-10",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What action did the assistant perform when the user mentioned 'milk' during the conversation about the jacket?",
          "answer_type": "short_answer",
          "gold_answer": "Added to reminders",
          "predicted_answer": "Added milk to your reminders list.",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: I haven't seen your cofounder's jacket, but I can keep an eye out for it.",
            "[Session llm-6, Turn 5] assistant: Noted, I've added milk to your reminders list.",
            "[Session llm-6, Turn 0] user: Hey, did Noah leave his jacket here?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.005163192749023,
          "generation_latency_ms": 3155.4503440856934,
          "total_latency_ms": 3166.4555072784424
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What 'classic combo' or architectural pattern does the user identify for balancing data analysis and speed?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Using Python for data processing/analysis and Go for high-concurrency services.",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: That sounds like a solid compromise. Using the interpreted language for data and the compiled Google one for speed is a pretty common pattern.",
            "[Session llm-8, Turn 5] assistant: Right, the core system is currently a hybrid. We are using Python for the data processing layers and Go for the high-concurrency services.",
            "[Session llm-6, Turn 3] assistant: For the core system, it looks like the team is splitting the difference. The heavy lifting is done with Go for concurrency, but there's still a lot of Python in there for the analytics side."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.014720916748047,
          "generation_latency_ms": 4267.09246635437,
          "total_latency_ms": 4281.107187271118
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 10,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 9,
            "accuracy": 0.7777777777777778
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_012.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.773733377456665,
      "total_evaluation_time_s": 70.51624178886414,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific individual insisted on the dual-language architecture for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Just the usual. Also, I was looking at the repo for the core project. Are we sticking to the dual-language setup?",
            "[Session llm-7, Turn 13] assistant: Consistency is definitely important with a multi-language codebase. I'll be here if you need those config details later.",
            "[Session llm-7, Turn 2] user: I'm trying to update the readme for the core repo. Elijah mentioned we're using a mix of languages now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.506961822509766,
          "generation_latency_ms": 4502.737045288086,
          "total_latency_ms": 4515.244007110596
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the person identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: Does your cofounder need specific details about the codebase?",
            "[Session llm-2, Turn 2] user: Yeah, actually. My cofounder was asking about the tech stack for the core platform again. My brain is too foggy to recall the specifics.",
            "[Session llm-0, Turn 5] assistant: I haven't seen any notes from your cofounder come through the inbox yet. Do you want me to draft a message asking him for them?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998964309692383,
          "generation_latency_ms": 3785.4244709014893,
          "total_latency_ms": 3796.4234352111816
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "In the MedPulse core project, which programming language is specifically utilized for the microservices to handle load?",
          "answer_type": "short_answer",
          "gold_answer": "Go",
          "predicted_answer": "Go",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the main MedPulse repository, it's actually a hybrid setup. It looks like the architecture relies on Python for the data processing side and Go for the concurrent services.",
            "[Session llm-1, Turn 5] assistant: It's not uncommon to mix them. Using Go for the high-performance services and Python for the data heavy-lifting is a popular hybrid approach for MedPulse Core.",
            "[Session llm-2, Turn 3] assistant: No problem. For the main MedPulse project, the architecture relies on Python for the data processing layers and Go for the concurrent services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.99972152709961,
          "generation_latency_ms": 3962.491273880005,
          "total_latency_ms": 3979.4909954071045
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact character string for the MedPulse organization's wifi password?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Yes, for the office network, the password is just the organization's name with a capital M and P, followed by the current year and an exclamation point.",
            "[Session llm-0, Turn 7] assistant: For the office wifi, it's just the company name with a capital M and P, followed by the current year 2024, and finished with an exclamation mark.",
            "[Session llm-9, Turn 2] user: Good idea. I'm at the MedPulse HQ and forgot the wifi details."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 17.01807975769043,
          "generation_latency_ms": 3873.478651046753,
          "total_latency_ms": 3890.4967308044434
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the conversation about the repository, what is the technical reason for using Go in the backend?",
          "answer_type": "short_answer",
          "gold_answer": "handle load",
          "predicted_answer": "concurrency, high-performance services",
          "retrieved_context": [
            "[Session llm-7, Turn 11] assistant: I recall the team discussing standard linting tools for the Go files, but I can dig up the specific config if you need it.",
            "[Session llm-9, Turn 9] assistant: We have not. The project still heavily utilizes Go for the service layer.",
            "[Session llm-0, Turn 10] user: Back to the code, are the .go files sitting in the root directory or separated?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 3973.1199741363525,
          "total_latency_ms": 3983.121395111084
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific item did the user mention forgetting to bring to the office during the first recorded conversation?",
          "answer_type": "short_answer",
          "gold_answer": "umbrella",
          "predicted_answer": "office network credentials",
          "retrieved_context": [
            "[Session llm-3, Turn 10] user: Please do. Also, I reset my phone and lost the office network credentials. What was the secret phrase again?",
            "[Session llm-6, Turn 1] assistant: Good morning! I've added a reminder for the coffee run. Anything specific on the order?",
            "[Session llm-7, Turn 6] user: No, that's it for the docs. By the way, my cofounder is trying to connect his new tablet and forgot the office wifi string again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.994268417358398,
          "generation_latency_ms": 4214.25199508667,
          "total_latency_ms": 4224.246263504028
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine comments, what beverage do they frequently desire?",
          "answer_type": "short_answer",
          "gold_answer": "coffee",
          "predicted_answer": "Coffee/Caffeine",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Morning! I am really dragging my feet today, need more caffeine.",
            "[Session llm-8, Turn 6] user: I'll risk it. I need the caffeine if I'm going to dig through the core project backend.",
            "[Session llm-7, Turn 8] user: Thanks. He says he needs more coffee before he can function."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.001565933227539,
          "generation_latency_ms": 3973.7722873687744,
          "total_latency_ms": 3981.773853302002
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When asked about a specific order for the team coffee run, what was the user's instruction?",
          "answer_type": "short_answer",
          "gold_answer": "the usual",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: Good morning! I've added a reminder for the coffee run. Anything specific on the order?",
            "[Session llm-8, Turn 5] assistant: I haven't received any alerts about the coffee machine, but it has been acting up lately.",
            "[Session llm-6, Turn 0] user: Hey, good morning. Can you remind me to pick up coffee for the team later?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.022785186767578,
          "generation_latency_ms": 3696.9892978668213,
          "total_latency_ms": 3705.012083053589
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number of Python is required for the MedPulse core project?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: That's right. For the main MedPulse architecture, we are leveraging both Go and Python to handle different workloads.",
            "[Session llm-2, Turn 3] assistant: No problem. For the main MedPulse project, the architecture relies on Python for the data processing layers and Go for the concurrent services.",
            "[Session llm-4, Turn 9] assistant: That makes sense, I recall the team wanting to stick with Python for the data analysis components of the core project."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99958610534668,
          "generation_latency_ms": 3755.7477951049805,
          "total_latency_ms": 3764.747381210327
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What database technology is explicitly listed as part of the MedPulse stack?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the main MedPulse repository, it's actually a hybrid setup. It looks like the architecture relies on Python for the data processing side and Go for the concurrent services.",
            "[Session llm-1, Turn 5] assistant: It's not uncommon to mix them. Using Go for the high-performance services and Python for the data heavy-lifting is a popular hybrid approach for MedPulse Core.",
            "[Session llm-2, Turn 3] assistant: No problem. For the main MedPulse project, the architecture relies on Python for the data processing layers and Go for the concurrent services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.009910583496094,
          "generation_latency_ms": 4128.834247589111,
          "total_latency_ms": 4136.844158172607
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the three-part pattern used to construct the office Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "Name Year Exclamation",
          "predicted_answer": "The company/organization name with a capital M and P, the current year, and an exclamation point.",
          "retrieved_context": [
            "[Session llm-5, Turn 6] user: Will do. My wifi keeps dropping on this machine though. What was the password for the office network again?",
            "[Session llm-2, Turn 7] assistant: Yes, for the office network, the password is just the organization's name with a capital M and P, followed by the current year and an exclamation point.",
            "[Session llm-7, Turn 6] user: No, that's it for the docs. By the way, my cofounder is trying to connect his new tablet and forgot the office wifi string again."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002996444702148,
          "generation_latency_ms": 3097.004175186157,
          "total_latency_ms": 3105.0071716308594
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on the established pattern, if the current year were 2025, what would the MedPulse wifi password be?",
          "answer_type": "generation",
          "gold_answer": "MedPulse2025!",
          "predicted_answer": "MedPulse2025!",
          "retrieved_context": [
            "[Session llm-5, Turn 6] user: Will do. My wifi keeps dropping on this machine though. What was the password for the office network again?",
            "[Session llm-0, Turn 7] assistant: For the office wifi, it's just the company name with a capital M and P, followed by the current year 2024, and finished with an exclamation mark.",
            "[Session llm-2, Turn 6] user: Thanks. Elijah is bringing a consultant by the office later. Do you know the wifi details?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000301361083984,
          "generation_latency_ms": 3546.037197113037,
          "total_latency_ms": 3555.037498474121
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 8,
            "accuracy": 0.625
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "generation": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_024.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6700212955474854,
      "total_evaluation_time_s": 68.82784533500671,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the name of the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: Thanks. My cofounder is bringing a consultant in later. They'll need to get on the internet.",
            "[Session llm-6, Turn 3] assistant: You're welcome. By the way, your cofounder Ava messaged earlier to say she might be running a few minutes late.",
            "[Session llm-4, Turn 7] assistant: If my cofounder vouching for them, I'm sure they'll be fine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002519607543945,
          "generation_latency_ms": 2968.3549404144287,
          "total_latency_ms": 2976.3574600219727
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the animal metaphors used in the conversations, which two programming languages comprise the backend stack for the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Thanks. I'm trying to get the dev environment set up for the core platform. Do you remember what languages Ava picked for the backend?",
            "[Session llm-0, Turn 13] assistant: Currently, the core system relies primarily on that Python and Go combination we discussed; I don't see references to other major backend frameworks right now.",
            "[Session llm-7, Turn 2] user: It was intense, went into overtime. Anyway, I'm trying to update the documentation for the core platform. Do you remember exactly which languages we settled on for the backend services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00125503540039,
          "generation_latency_ms": 3332.5350284576416,
          "total_latency_ms": 3341.536283493042
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user discussed updating documentation, what activity did the assistant say they were doing instead of watching the game?",
          "answer_type": "short_answer",
          "gold_answer": "Reading",
          "predicted_answer": "catching up on some reading",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: Good morning! I missed the game, actually. I was catching up on some reading. How was it?",
            "[Session llm-7, Turn 2] user: It was intense, went into overtime. Anyway, I'm trying to update the documentation for the core platform. Do you remember exactly which languages we settled on for the backend services?",
            "[Session llm-2, Turn 9] assistant: We did. The one with the gopher mascot is handling the concurrent processing now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00943374633789,
          "generation_latency_ms": 2706.2273025512695,
          "total_latency_ms": 2714.2367362976074
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which backend language does the user's cofounder specifically favor for high-performance microservices?",
          "answer_type": "short_answer",
          "gold_answer": "Go",
          "predicted_answer": "Go",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: I believe Ava finalized that stack last week. We're running a hybrid environment with Python handling the data analysis components and Go for the concurrent microservices.",
            "[Session llm-6, Turn 5] assistant: Pulling that up. As a refresher, the core services are primarily built using Go for performance.",
            "[Session llm-0, Turn 13] assistant: Currently, the core system relies primarily on that Python and Go combination we discussed; I don't see references to other major backend frameworks right now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.005878448486328,
          "generation_latency_ms": 4080.4574489593506,
          "total_latency_ms": 4091.463327407837
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific JavaScript framework is used for the frontend of the MedPulse platform?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: We are looking at the core architecture for the MedPulse project. It's mostly built with those .py files right now, but we are hitting some speed bumps.",
            "[Session llm-0, Turn 13] assistant: Currently, the core system relies primarily on that Python and Go combination we discussed; I don't see references to other major backend frameworks right now.",
            "[Session llm-6, Turn 13] assistant: Yes, both the Python scripts and the Go source code are housed in the medpulsecore repository."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.518457412719727,
          "generation_latency_ms": 3754.866600036621,
          "total_latency_ms": 3763.385057449341
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the rule provided by the assistant, what special character must be included at the end of the MedPulse wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Exclamation mark",
          "predicted_answer": "!",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Done. Tacos it is. Oh, before I forget, a vendor is coming by the office later. What's the wifi password again?",
            "[Session llm-6, Turn 1] assistant: No problem. For the main network, the password is the company name with a capital M and P, followed by the current year and an exclamation mark.",
            "[Session llm-9, Turn 8] user: I did, thankfully. Oh, before I forget, a vendor is coming by the office. Can you remind me of the wireless password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000249862670898,
          "generation_latency_ms": 3298.497438430786,
          "total_latency_ms": 3311.497688293457
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact wifi password string for the MedPulse office?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: Done. Tacos it is. Oh, before I forget, a vendor is coming by the office later. What's the wifi password again?",
            "[Session llm-9, Turn 8] user: I did, thankfully. Oh, before I forget, a vendor is coming by the office. Can you remind me of the wireless password?",
            "[Session llm-4, Turn 8] user: Also, they just asked for the office wifi key for their phone. I totally blanked on what we changed it to."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00611686706543,
          "generation_latency_ms": 4502.4120807647705,
          "total_latency_ms": 4513.418197631836
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Ava?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounder",
          "predicted_answer": "Cofounder",
          "retrieved_context": [
            "[Session llm-4, Turn 6] user: Ava thinks they can handle it; she interviewed them personally.",
            "[Session llm-2, Turn 5] assistant: Is Ava worried about the tech stack again?",
            "[Session llm-9, Turn 4] user: Actually, Ava wants to introduce a compiled language. She's really into the one with the gopher mascot for the high-performance microservices."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003971099853516,
          "generation_latency_ms": 3155.9550762176514,
          "total_latency_ms": 3166.959047317505
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a dentist reminder before or after they forgot the wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: I did, thankfully. Oh, before I forget, a vendor is coming by the office. Can you remind me of the wireless password?",
            "[Session llm-7, Turn 6] user: Done. Tacos it is. Oh, before I forget, a vendor is coming by the office later. What's the wifi password again?",
            "[Session llm-4, Turn 8] user: Also, they just asked for the office wifi key for their phone. I totally blanked on what we changed it to."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.023189544677734,
          "generation_latency_ms": 4030.8265686035156,
          "total_latency_ms": 4039.8497581481934
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What food did the assistant agree sounded 'amazing' for lunch during the documentation update session?",
          "answer_type": "short_answer",
          "gold_answer": "Tacos",
          "predicted_answer": "Tacos",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: Good morning! I missed the game, actually. I was catching up on some reading. How was it?",
            "[Session llm-7, Turn 4] user: Right, the hybrid approach. That makes sense given the performance needs. Also, I need to order lunch, are you feeling tacos or pizza?",
            "[Session llm-7, Turn 2] user: It was intense, went into overtime. Anyway, I'm trying to update the documentation for the core platform. Do you remember exactly which languages we settled on for the backend services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00356674194336,
          "generation_latency_ms": 3189.8393630981445,
          "total_latency_ms": 3199.842929840088
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the exact date the MedPulse core project was first initialized?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: We are looking at the core architecture for the MedPulse project. It's mostly built with those .py files right now, but we are hitting some speed bumps.",
            "[Session llm-8, Turn 3] assistant: I can help with that. Are we looking at the MedPulse core repository?",
            "[Session llm-6, Turn 13] assistant: Yes, both the Python scripts and the Go source code are housed in the medpulsecore repository."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.003826141357422,
          "generation_latency_ms": 3811.290502548218,
          "total_latency_ms": 3824.294328689575
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Using the password generation logic described by the assistant (Company Name + M + P + Year + !), what would be the password for 'TechCorp' in 2025?",
          "answer_type": "generation",
          "gold_answer": "TechCorpMP2025!",
          "predicted_answer": "TechCorpMP2025!",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: No problem. For the main network, the password is the company name with a capital M and P, followed by the current year and an exclamation mark.",
            "[Session llm-7, Turn 7] assistant: For the main network, it's the company name with a capital M and P, followed by 2024 and an exclamation point.",
            "[Session llm-7, Turn 6] user: Done. Tacos it is. Oh, before I forget, a vendor is coming by the office later. What's the wifi password again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 28.002262115478516,
          "generation_latency_ms": 5761.597394943237,
          "total_latency_ms": 5789.599657058716
        }
      ],
      "summary": {
        "overall_accuracy": 0.9166666666666666,
        "total_score": 11,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 8,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "generation": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_029.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8501222133636475,
      "total_evaluation_time_s": 69.9215259552002,
      "probe_results": [
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which programming language does the MedPulse Core project rely on specifically for its high-concurrency API services?",
          "answer_type": "short_answer",
          "gold_answer": "Go",
          "predicted_answer": "Go",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Yes, the architecture for the MedPulse core project is still hybrid. We are leveraging Python for the data science components and Go for the high-performance services.",
            "[Session llm-1, Turn 3] assistant: For the MedPulse Core project, the team is utilizing a mix of Python for data tasks and Go for the API services.",
            "[Session llm-6, Turn 3] assistant: Correct, the data processing modules in MedPulse Core are built with that. Are we still using the language developed by Google for the microservices?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999296188354492,
          "generation_latency_ms": 3490.232467651367,
          "total_latency_ms": 3503.2317638397217
        },
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual specifically concerned about the latency issues in the interpreted parts of the codebase?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Sounds like a productive morning with Emma. Is she still worried about the latency issues?",
            "[Session llm-8, Turn 5] assistant: Moving away from Python might help with speed, but it's a lot of work. Is she planning to port everything over to the other language you guys use?",
            "[Session llm-7, Turn 4] user: Right, standard choice. And for the high-concurrency stuff?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 2787.205696105957,
          "total_latency_ms": 2797.206163406372
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the construction rule described by the user, what is the exact string for the MedPulse office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: The password for the office network is the company name capitalized, followed by the current year, and ending with an exclamation mark.",
            "[Session llm-3, Turn 8] user: I'll check. Also, I reset my phone and lost the network settings. What's the password for our office internet?",
            "[Session llm-8, Turn 8] user: Exactly. By the way, I got a new tablet for testing. What's the logic for the office network password again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003877639770508,
          "generation_latency_ms": 4506.890535354614,
          "total_latency_ms": 4515.894412994385
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-0",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which specific technology component is the cofounder considering rewriting to address latency concerns?",
          "answer_type": "short_answer",
          "gold_answer": "Python scripts",
          "predicted_answer": "interpreted parts of the medpulsecore repo",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Sounds like a productive morning with Emma. Is she still worried about the latency issues?",
            "[Session llm-6, Turn 2] user: Yeah, my cofounder is asking for a summary of the tech stack for the core platform. We're using that snake language for the data stuff, right?",
            "[Session llm-2, Turn 2] user: Yeah, my cofounder Emma is asking about the tech stack documentation for the core project. We're still doing the hybrid approach, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002923965454102,
          "generation_latency_ms": 3277.9645919799805,
          "total_latency_ms": 3286.9675159454346
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific reminder did the user set during the conversation where they referred to the 'snake language'?",
          "answer_type": "short_answer",
          "gold_answer": "Call the caterers",
          "predicted_answer": "I knew there was a second language involved.",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Understood. I will list both the snake-based language and the gopher-based one as the primary frameworks.",
            "[Session llm-2, Turn 3] assistant: That's my understanding. You are combining the language named after a snake with the one developed by Google, correct?",
            "[Session llm-3, Turn 2] user: I'll make some here. Hey, my cofounder was asking about the backend languages for the main project. We are using the snake one for the data science bits, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000394821166992,
          "generation_latency_ms": 4216.368198394775,
          "total_latency_ms": 4227.368593215942
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user mention the 'jarring' context switching between languages before or after the meeting at the bakery?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-2, Turn 3] assistant: That's my understanding. You are combining the language named after a snake with the one developed by Google, correct?",
            "[Session llm-5, Turn 4] user: Okay, good to remember. I knew there was a second language involved.",
            "[Session llm-8, Turn 1] assistant: Sure thing, I've added a reminder to buy bagels. Do you need directions to that bakery you liked last week?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.527849197387695,
          "generation_latency_ms": 3149.3051052093506,
          "total_latency_ms": 3159.8329544067383
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user require navigation assistance to reach the bakery mentioned in their morning meeting plans?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: Sure thing, I've added a reminder to buy bagels. Do you need directions to that bakery you liked last week?",
            "[Session llm-2, Turn 10] user: No, I'm going to grab lunch first. Do you know if that sandwich place is open today?",
            "[Session llm-6, Turn 1] assistant: Done. I've set a reminder for 12:00 PM to call the caterers. Anything else?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.016317367553711,
          "generation_latency_ms": 3591.9082164764404,
          "total_latency_ms": 3606.924533843994
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user describe the experience of switching between the data services and API services languages?",
          "answer_type": "short_answer",
          "gold_answer": "Jarring",
          "predicted_answer": "a bit jarring",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: For the MedPulse Core project, the team is utilizing a mix of Python for data tasks and Go for the API services.",
            "[Session llm-6, Turn 3] assistant: Correct, the data processing modules in MedPulse Core are built with that. Are we still using the language developed by Google for the microservices?",
            "[Session llm-4, Turn 2] user: No, that's it for now. I'm actually looking at the repo for the core platform. It's a bit jarring switching between the data services in Python and the API services in Go."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.998870849609375,
          "generation_latency_ms": 3570.653200149536,
          "total_latency_ms": 3579.6520709991455
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database management system is used in the MedPulse Core stack?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Yes, the architecture for the MedPulse core project is still hybrid. We are leveraging Python for the data science components and Go for the high-performance services.",
            "[Session llm-1, Turn 3] assistant: For the MedPulse Core project, the team is utilizing a mix of Python for data tasks and Go for the API services.",
            "[Session llm-6, Turn 3] assistant: Correct, the data processing modules in MedPulse Core are built with that. Are we still using the language developed by Google for the microservices?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003162384033203,
          "generation_latency_ms": 3717.144250869751,
          "total_latency_ms": 3726.147413253784
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the last name of the user's cofounder?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 7] assistant: If your cofounder asks, just tell her it's the company name, the current year 2024, and a bang at the end.",
            "[Session llm-7, Turn 0] user: Hey, has my cofounder checked in yet today?",
            "[Session llm-5, Turn 6] user: Speaking of needs, my cofounder is dropping by the office today and she never remembers the guest network credentials."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 4165.717840194702,
          "total_latency_ms": 4174.717903137207
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What three components make up the formula for the office wifi password according to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, bang",
          "predicted_answer": "company name, the current year, and a bang at the end",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: Exactly. By the way, I got a new tablet for testing. What's the logic for the office network password again?",
            "[Session llm-9, Turn 7] assistant: The password for the office network is the company name capitalized, followed by the current year, and ending with an exclamation mark.",
            "[Session llm-3, Turn 8] user: I'll check. Also, I reset my phone and lost the network settings. What's the password for our office internet?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3762.9175186157227,
          "total_latency_ms": 3772.9179859161377
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on the user's password generation rule, what would the wifi password be if the current year was 2025?",
          "answer_type": "short_answer",
          "gold_answer": "MedPulse2025!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Thanks. Emma is coming in later and she'll probably ask for the wifi password again.",
            "[Session llm-8, Turn 8] user: Exactly. By the way, I got a new tablet for testing. What's the logic for the office network password again?",
            "[Session llm-7, Turn 8] user: Thanks. Also, I'm at the office and my phone disconnected. What's the wifi pass again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.995553970336914,
          "generation_latency_ms": 4406.643629074097,
          "total_latency_ms": 4418.639183044434
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 9,
            "accuracy": 0.5555555555555556
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_032.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6600043773651123,
      "total_evaluation_time_s": 67.11000108718872,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' who is worried about maintenance overhead?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-0, Turn 2] user: I'm a bit scattered. My cofounder Elijah just messaged me asking about the tech stack for our main project again.",
            "[Session llm-8, Turn 2] user: I have a meeting with Elijah about the medpulsecore architecture later. He's worried about the maintenance overhead.",
            "[Session llm-7, Turn 1] assistant: Sure, I'll set a reminder to message your cofounder in an hour."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001991271972656,
          "generation_latency_ms": 2553.6348819732666,
          "total_latency_ms": 2565.6368732452393
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which project is the user and Elijah actively discussing the architecture and language stack for?",
          "answer_type": "short_answer",
          "gold_answer": "medpulse_core",
          "predicted_answer": "MedPulse Core",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: A bit chaotic. Elijah is pushing for a rewrite of the core backend services, but we can't decide on the language.",
            "[Session llm-5, Turn 8] user: Thanks. I'm presenting the architecture for the core project today. I need to explain why we're using two different languages.",
            "[Session llm-4, Turn 3] assistant: Refactoring can be tough. Is Elijah changing the core languages you're using?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001276016235352,
          "generation_latency_ms": 3687.079906463623,
          "total_latency_ms": 3699.0811824798584
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What two programming languages make up the hybrid backend stack for the MedPulse core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Thanks. I was talking to Elijah earlier about the MedPulse core repo. We are still using a hybrid approach for the backend languages, right?",
            "[Session llm-0, Turn 3] assistant: No worries. Are you referring to the backend languages for the core MedPulse build?",
            "[Session llm-9, Turn 3] assistant: Right, for the MedPulse core, Elijah set it up as a mix. The data processing services are all Python, but the high-concurrency stuff is written in Go."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.527994155883789,
          "generation_latency_ms": 2534.9020957946777,
          "total_latency_ms": 2543.4300899505615
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password string used to connect to the MedPulse organization's Wi-Fi?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: The password for the MedPulse network is the company name capitalized, the year 2024, and an exclamation mark at the end.",
            "[Session llm-0, Turn 10] user: Nice. Back to the office stuff—I'm heading into the MedPulse HQ and reset my phone. What's the wifi pass?",
            "[Session llm-1, Turn 10] user: Thanks. Also, Elijah is asking for the office wifi password again. It's MedPulse2024!, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003732681274414,
          "generation_latency_ms": 4279.387950897217,
          "total_latency_ms": 4290.391683578491
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the user's laptop unable to auto-connect to the network in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Security protocols updated",
          "predicted_answer": "Elijah updated security protocols.",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: I just got into the office and my laptop isn't auto-connecting to the network. Did Elijah change the router settings again?",
            "[Session llm-9, Turn 6] user: Yeah, actually, my laptop disconnected from the network. What's the office wifi pass?",
            "[Session llm-7, Turn 8] user: No, I'll grab a sandwich later. My laptop forgot the network again. What's the password?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000871658325195,
          "generation_latency_ms": 2891.9906616210938,
          "total_latency_ms": 2902.991533279419
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user discuss the 'maintenance overhead' of the backend with Elijah before or after obtaining the new Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 10] user: Thanks. Also, Elijah is asking for the office wifi password again. It's MedPulse2024!, right?",
            "[Session llm-5, Turn 1] assistant: He mentioned updating security protocols yesterday. Do you need the new credentials for the main Wi-Fi?",
            "[Session llm-8, Turn 8] user: Thanks. Also, the new intern is asking about the office wifi. I totally blanked on the password."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001970291137695,
          "generation_latency_ms": 4259.59849357605,
          "total_latency_ms": 4268.6004638671875
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user agree with Elijah that the context switching costs of the hybrid stack are too high?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Yeah, exactly. He thinks the context switching is too high, but I think we need Go for the speed and Python for the ML bits.",
            "[Session llm-6, Turn 2] user: Thanks. I was talking to Elijah earlier about the MedPulse core repo. We are still using a hybrid approach for the backend languages, right?",
            "[Session llm-4, Turn 3] assistant: Refactoring can be tough. Is Elijah changing the core languages you're using?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3745.021343231201,
          "total_latency_ms": 3755.021810531616
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which programming language does the user prefer for the machine learning components of the project?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-5, Turn 11] assistant: You can mention that the data science ecosystem is strongest in Python, while the API needs the performance of a compiled language.",
            "[Session llm-8, Turn 5] assistant: It is a classic trade-off, but using the specific strengths of both languages usually pays off despite the complexity.",
            "[Session llm-5, Turn 8] user: Thanks. I'm presenting the architecture for the core project today. I need to explain why we're using two different languages."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.995294570922852,
          "generation_latency_ms": 3752.8035640716553,
          "total_latency_ms": 3761.798858642578
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is Elijah's last name?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: That's great progress. What did you and Elijah decide on?",
            "[Session llm-8, Turn 13] assistant: Sounds like a plan. Good luck with the tickets and the meeting with Elijah.",
            "[Session llm-4, Turn 3] assistant: Refactoring can be tough. Is Elijah changing the core languages you're using?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.998394012451172,
          "generation_latency_ms": 3252.5086402893066,
          "total_latency_ms": 3261.507034301758
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific brand of router is used at the MedPulse office?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: The password for the MedPulse network is the company name capitalized, the year 2024, and an exclamation mark at the end.",
            "[Session llm-0, Turn 10] user: Nice. Back to the office stuff—I'm heading into the MedPulse HQ and reset my phone. What's the wifi pass?",
            "[Session llm-9, Turn 6] user: Yeah, actually, my laptop disconnected from the network. What's the office wifi pass?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001514434814453,
          "generation_latency_ms": 3752.6602745056152,
          "total_latency_ms": 3764.6617889404297
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's hint, what components make up the new Wi-Fi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, M, P, year, exclamation mark",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: He mentioned updating security protocols yesterday. Do you need the new credentials for the main Wi-Fi?",
            "[Session llm-0, Turn 10] user: Nice. Back to the office stuff—I'm heading into the MedPulse HQ and reset my phone. What's the wifi pass?",
            "[Session llm-8, Turn 8] user: Thanks. Also, the new intern is asking about the office wifi. I totally blanked on the password."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000944137573242,
          "generation_latency_ms": 3477.2815704345703,
          "total_latency_ms": 3487.2825145721436
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What primary technical advantage does the user cite for retaining Go in the backend stack?",
          "answer_type": "short_answer",
          "gold_answer": "Speed",
          "predicted_answer": "high-concurrency stuff",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: That would mean keeping both Python and Go for the backend services, correct?",
            "[Session llm-5, Turn 9] assistant: Are you discussing the split between the data processing layers and the backend services?",
            "[Session llm-6, Turn 2] user: Thanks. I was talking to Elijah earlier about the MedPulse core repo. We are still using a hybrid approach for the backend languages, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.589910507202148,
          "generation_latency_ms": 3474.290609359741,
          "total_latency_ms": 3483.8805198669434
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 8,
            "accuracy": 0.625
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_040.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8502991199493408,
      "total_evaluation_time_s": 67.60533666610718,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder during the schedule sync?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: I can help with that. Does your cofounder need the guest login or the main internal one?",
            "[Session llm-5, Turn 1] assistant: You have a sync with Olivia at 2 PM regarding the roadmap, and then a block reserved for deep work until 5 PM.",
            "[Session llm-4, Turn 10] user: Perfect. My cofounder is asking if I'm free for lunch."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.009620666503906,
          "generation_latency_ms": 3882.786273956299,
          "total_latency_ms": 3894.7958946228027
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Olivia?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Cofounder",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: Always good to do a review. Is Olivia suggesting any major changes to the stack?",
            "[Session llm-0, Turn 3] assistant: You mean for the MedPulse core platform? What were you discussing with Olivia?",
            "[Session llm-6, Turn 6] user: Thanks. Also, Olivia is stopping by the new office today and she always forgets the network credentials."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00523567199707,
          "generation_latency_ms": 2680.4139614105225,
          "total_latency_ms": 2690.4191970825195
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages comprise the hybrid tech stack for the medpulse_core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Yeah, regarding the MedPulse core repo. We were debating the backend structure. Which two languages are we currently running in that environment?",
            "[Session llm-1, Turn 9] assistant: I see, so MedPulse Core is running a hybrid of Python and Go. I'll make a note of that stack.",
            "[Session llm-6, Turn 3] assistant: Looking at the codebase, it's a hybrid stack. The data science components are built in Python, while the backend services are utilizing Go."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.000965118408203,
          "generation_latency_ms": 3256.7336559295654,
          "total_latency_ms": 3269.7346210479736
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Based on the pattern described by the assistant, what is the current wifi password for the organization?",
          "answer_type": "short_answer",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 11] assistant: Yes, that matches the stored credential for the wifi.",
            "[Session llm-4, Turn 6] user: Thanks. I have a vendor coming by the office later. Can you remind me of the wifi password for the org network?",
            "[Session llm-1, Turn 10] user: Please do. Also, I'm setting up a new device in the office and forgot the credentials. Is the wifi just the company name plus the year?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.524106979370117,
          "generation_latency_ms": 3836.6153240203857,
          "total_latency_ms": 3846.139430999756
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about the loud air conditioning, what personal task did they ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Buy mom a gift",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Good morning. Is it just me or is the air conditioning really loud today?",
            "[Session llm-4, Turn 1] assistant: Good morning. I can't hear the AC, but I can help you with your tasks. What's on your mind?",
            "[Session llm-6, Turn 1] assistant: Good morning! Systems are green. How can I help you kick off the day?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.002872467041016,
          "generation_latency_ms": 4459.033489227295,
          "total_latency_ms": 4472.036361694336
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for the office wifi password before or after scheduling the dentist appointment?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-1, Turn 10] user: Please do. Also, I'm setting up a new device in the office and forgot the credentials. Is the wifi just the company name plus the year?",
            "[Session llm-4, Turn 6] user: Thanks. I have a vendor coming by the office later. Can you remind me of the wifi password for the org network?",
            "[Session llm-7, Turn 4] user: Thanks. I have a contractor coming by later to look at the servers. What's the wifi key again for the office?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00356674194336,
          "generation_latency_ms": 3160.1345539093018,
          "total_latency_ms": 3170.138120651245
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the repeated requests in recent sessions, what time does the user prefer to call their mother?",
          "answer_type": "short_answer",
          "gold_answer": "6 PM",
          "predicted_answer": "6 PM",
          "retrieved_context": [
            "[Session llm-3, Turn 9] assistant: Consider it done. I've set a reminder for you to call your mom at 5 PM.",
            "[Session llm-6, Turn 5] assistant: Done. I've set a reminder to call your mom at 6:00 PM.",
            "[Session llm-9, Turn 0] user: Good morning. Can you set a reminder for me to call my mom at 6 PM?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002208709716797,
          "generation_latency_ms": 3360.9049320220947,
          "total_latency_ms": 3369.9071407318115
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user have a stated preference for using Java over Go for the backend microservices?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: I don't think so. Looking at the recent technical notes, the team seems to be splitting the codebase between that popular data science language and the one with the gopher mascot for the microservices.",
            "[Session llm-9, Turn 5] assistant: I believe the microservices for the core project are actually implemented in Go, not C++.",
            "[Session llm-0, Turn 5] assistant: Right, the architecture is hybrid. The heavy lifting for data analysis is all Python, but the microservices are built with Go for speed."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.007547378540039,
          "generation_latency_ms": 3828.507423400879,
          "total_latency_ms": 3839.514970779419
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the lead designer for the MedPulse project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 7] assistant: A hybrid approach with Go for the performance-critical paths sounds like a solid plan for MedPulse Core.",
            "[Session llm-1, Turn 9] assistant: I see, so MedPulse Core is running a hybrid of Python and Go. I'll make a note of that stack.",
            "[Session llm-0, Turn 3] assistant: You mean for the MedPulse core platform? What were you discussing with Olivia?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.999773025512695,
          "generation_latency_ms": 3990.1785850524902,
          "total_latency_ms": 4003.178358078003
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the medpulse_core backend currently implemented in C++?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-8, Turn 4] user: Yeah, regarding the MedPulse core repo. We were debating the backend structure. Which two languages are we currently running in that environment?",
            "[Session llm-1, Turn 9] assistant: I see, so MedPulse Core is running a hybrid of Python and Go. I'll make a note of that stack.",
            "[Session llm-3, Turn 7] assistant: A hybrid approach with Go for the performance-critical paths sounds like a solid plan for MedPulse Core."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.006500244140625,
          "generation_latency_ms": 4117.808818817139,
          "total_latency_ms": 4126.815319061279
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the specific rule mentioned for constructing the MedPulse office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name, year, exclamation mark",
          "predicted_answer": "the company name capitalized, followed by the current year and an exclamation mark",
          "retrieved_context": [
            "[Session llm-5, Turn 6] user: I'll check on my way out. Speaking of the office, my phone dropped the connection again. What's the password for the MedPulse network?",
            "[Session llm-8, Turn 9] assistant: I do. For the MedPulse office network, the password is just the company name capitalized, followed by the current year and an exclamation mark.",
            "[Session llm-1, Turn 10] user: Please do. Also, I'm setting up a new device in the office and forgot the credentials. Is the wifi just the company name plus the year?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.017799377441406,
          "generation_latency_ms": 3082.664966583252,
          "total_latency_ms": 3093.6827659606934
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Given the established tech stack, which language should be used for data science components in the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-7, Turn 8] user: Also, the contractor asked about the backend environment for the core project. Is it strictly one language?",
            "[Session llm-8, Turn 4] user: Yeah, regarding the MedPulse core repo. We were debating the backend structure. Which two languages are we currently running in that environment?",
            "[Session llm-6, Turn 3] assistant: Looking at the codebase, it's a hybrid stack. The data science components are built in Python, while the backend services are utilizing Go."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.00136947631836,
          "generation_latency_ms": 3609.179973602295,
          "total_latency_ms": 3623.1813430786133
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 10,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 9,
            "accuracy": 0.7777777777777778
          },
          "boolean": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_healthtech_045.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7035775184631348,
      "total_evaluation_time_s": 66.64939403533936,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to by the user as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Good morning. I can help with that. What's on the agenda with your cofounder today?",
            "[Session llm-6, Turn 8] user: Got it. Oh, did my cofounder mention if he's coming in before lunch?",
            "[Session llm-7, Turn 12] user: I'll keep an eye on it. My cofounder Elijah is supposed to be joining me here in an hour."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 2757.6816082000732,
          "total_latency_ms": 2767.6849365234375
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "To which organization does the wifi password 'MedPulse2024!' belong?",
          "answer_type": "short_answer",
          "gold_answer": "MedPulse",
          "predicted_answer": "MedPulse",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: If I remember correctly, the password for the MedPulse network is just the company name, followed by the current year, and finished with an exclamation mark.",
            "[Session llm-6, Turn 7] assistant: For the MedPulse office network, the password is the company name capitalized, followed by 2024, and ending with an exclamation point.",
            "[Session llm-9, Turn 6] user: Exactly. Hey, random question, my phone dropped the wifi. Is the password still the company name, the current year, and an exclamation mark?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001493453979492,
          "generation_latency_ms": 4135.457992553711,
          "total_latency_ms": 4144.45948600769
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages are confirmed to constitute the hybrid stack for the 'medpulse_core' project?",
          "answer_type": "short_answer",
          "gold_answer": "Python and Go",
          "predicted_answer": "Python and Go",
          "retrieved_context": [
            "[Session llm-8, Turn 7] assistant: That is correct. The architecture for the MedPulse core utilizes a mix of Python for the data processing and Go for the concurrent services.",
            "[Session llm-6, Turn 4] user: Thanks. I was talking to Elijah earlier about the MedPulse core repo. What languages did we decide on for the backend services?",
            "[Session llm-0, Turn 3] assistant: For the MedPulse core platform, the decision was to use a mix. The backend services are written in Go for speed, while the data processing components utilize Python."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.102054595947266,
          "generation_latency_ms": 3430.2546977996826,
          "total_latency_ms": 3443.35675239563
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact character string used for the office network security credential?",
          "answer_type": "verbatim",
          "gold_answer": "MedPulse2024!",
          "predicted_answer": "MedPulse2024!",
          "retrieved_context": [
            "[Session llm-6, Turn 7] assistant: For the MedPulse office network, the password is the company name capitalized, followed by 2024, and ending with an exclamation point.",
            "[Session llm-8, Turn 11] assistant: For the office network, it follows the standard pattern: the company name, capitalized as usual, followed by 2024 and an exclamation point.",
            "[Session llm-8, Turn 10] user: No, that's fine. Actually, I reset my phone and lost the office network credentials. What's the passkey?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000394821166992,
          "generation_latency_ms": 3726.2344360351562,
          "total_latency_ms": 3737.2348308563232
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user requested a reminder for dry cleaning, what technical topic was subsequently discussed?",
          "answer_type": "short_answer",
          "gold_answer": "MedPulse core repo",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: Got it. I've set a reminder for your dry cleaning at 5 PM today.",
            "[Session llm-6, Turn 2] user: A bit chaotic to be honest. Can you remind me to pick up my dry cleaning at 5 PM?",
            "[Session llm-3, Turn 12] user: Will do. Also, remind me to pick up my dry cleaning at 5 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 3014.9502754211426,
          "total_latency_ms": 3024.953603744507
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the 'tech stack decision' with the cofounder occur before or after the user successfully connected to the wifi?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 2] user: Barely. Anyway, I'm setting up a new laptop for the core project work. I know my cofounder Elijah had strong opinions on the tech stack. What did we settle on?",
            "[Session llm-9, Turn 6] user: Exactly. Hey, random question, my phone dropped the wifi. Is the password still the company name, the current year, and an exclamation mark?",
            "[Session llm-6, Turn 6] user: Right, the hybrid approach. Also, I'm setting up a new tablet in the office. What's the Wi-Fi pass again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 3749.305486679077,
          "total_latency_ms": 3759.3069076538086
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does Elijah specifically prefer for high-concurrency services?",
          "answer_type": "short_answer",
          "gold_answer": "Go",
          "predicted_answer": "Go",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Looking at the project notes, it seems the core architecture relies on a mix of Python for the data analysis components and Go for the high-concurrency services.",
            "[Session llm-8, Turn 5] assistant: That sounds like Elijah. Is he reconsidering the languages currently in use?",
            "[Session llm-8, Turn 7] assistant: That is correct. The architecture for the MedPulse core utilizes a mix of Python for the data processing and Go for the concurrent services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.00136947631836,
          "generation_latency_ms": 4016.042709350586,
          "total_latency_ms": 4030.0440788269043
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What language does the data science team prefer for their scripts?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: Yeah, exactly. The data science team loves their Python scripts, but the API is all compiled Go.",
            "[Session llm-3, Turn 4] user: Yeah, specifically what languages we locked in. I know the data stuff is all Python.",
            "[Session llm-6, Turn 4] user: Thanks. I was talking to Elijah earlier about the MedPulse core repo. What languages did we decide on for the backend services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999441146850586,
          "generation_latency_ms": 2779.639720916748,
          "total_latency_ms": 2790.6391620635986
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is mentioned for the MedPulse Core backend?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: For the MedPulse core platform, the decision was to use a mix. The backend services are written in Go for speed, while the data processing components utilize Python.",
            "[Session llm-8, Turn 7] assistant: That is correct. The architecture for the MedPulse core utilizes a mix of Python for the data processing and Go for the concurrent services.",
            "[Session llm-3, Turn 2] user: I'm writing up a summary for the investors about the MedPulse Core architecture."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.006500244140625,
          "generation_latency_ms": 3676.1388778686523,
          "total_latency_ms": 3685.145378112793
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the lead frontend engineer mentioned in the logs?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 9] assistant: It should be our organization's name, followed by 2024, and finished with an exclamation point.",
            "[Session llm-4, Turn 9] assistant: For the performance-critical parts of the core, the team settled on using Go.",
            "[Session llm-1, Turn 5] assistant: Tell him it's just our startup's name, capitalized, plus the current year and a bang at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00180435180664,
          "generation_latency_ms": 3774.577856063843,
          "total_latency_ms": 3782.5796604156494
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "When setting up the environment for the production build, what must be configured after installing the Go binaries?",
          "answer_type": "short_answer",
          "gold_answer": "path variables",
          "predicted_answer": "Python configurations",
          "retrieved_context": [
            "[Session llm-4, Turn 13] assistant: I can help draft that. Do you want to include the details about installing both the Go and Python toolchains?",
            "[Session llm-7, Turn 4] user: Right, the hybrid stack. I always forget to install the Go binaries.",
            "[Session llm-0, Turn 8] user: Got it, connected now. Did Elijah mention if we are using a specific version for the Go code?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.005308151245117,
          "generation_latency_ms": 3439.4147396087646,
          "total_latency_ms": 3448.4200477600098
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "According to the architectural discussions, which language is assigned the role of handling data analysis components?",
          "answer_type": "short_answer",
          "gold_answer": "Python",
          "predicted_answer": "Python",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Looking at the project notes, it seems the core architecture relies on a mix of Python for the data analysis components and Go for the high-concurrency services.",
            "[Session llm-5, Turn 3] assistant: Is this regarding the project core that splits the logic between two different languages?",
            "[Session llm-4, Turn 7] assistant: I don't think so. The last architecture review for the core platform mentioned a hybrid approach, using Python for the data science modules."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.016109466552734,
          "generation_latency_ms": 3194.416046142578,
          "total_latency_ms": 3202.432155609131
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 11,
            "accuracy": 0.7272727272727273
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_001.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6316580772399902,
      "total_evaluation_time_s": 66.00387573242188,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder' throughout the sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Good morning. Has my cofounder logged on yet?",
            "[Session llm-0, Turn 12] user: Perfect. My cofounder usually writes it on the whiteboard, but someone erased it.",
            "[Session llm-2, Turn 8] user: Right, the usual pattern. My cofounder really needs to use a password manager."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001182556152344,
          "generation_latency_ms": 3309.8580837249756,
          "total_latency_ms": 3319.859266281128
        },
        {
          "probe_id": "world_modeling-task_state_tracking-0",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What is the current implementation status of the ShipFast Core codebase regarding its programming languages?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid mix (Ruby and Elixir)",
          "predicted_answer": "Elixir and Ruby",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Yeah, I was looking at the ShipFast core repo. Are we fully committed to one language there yet?",
            "[Session llm-7, Turn 3] assistant: Is Sato still pushing for a change in the tech stack for ShipFast Core?",
            "[Session llm-0, Turn 5] assistant: Yes, the current setup for ShipFast Core utilizes both Elixir and Ruby."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.004924774169922,
          "generation_latency_ms": 3822.540283203125,
          "total_latency_ms": 3833.545207977295
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the construction rule provided by the assistant (Organization + Year + !), what is the exact wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: By the way, a guest is coming by later. What's the wifi string for the office network?",
            "[Session llm-1, Turn 6] user: No, that high-level info is fine. Oh, we have a guest coming to the office later. What's the internet password?",
            "[Session llm-8, Turn 8] user: Thanks. Speaking of Sato, he just got a new tablet and is asking for the office wifi login."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002208709716797,
          "generation_latency_ms": 3409.5003604888916,
          "total_latency_ms": 3418.5025691986084
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which legacy programming language is still being maintained in the ShipFast Core project alongside Elixir?",
          "answer_type": "short_answer",
          "gold_answer": "Ruby",
          "predicted_answer": "Ruby",
          "retrieved_context": [
            "[Session llm-0, Turn 5] assistant: Yes, the current setup for ShipFast Core utilizes both Elixir and Ruby.",
            "[Session llm-1, Turn 3] assistant: Regarding the ShipFast core project, the backend is currently a hybrid setup. We are using Ruby for the primary logic alongside Elixir.",
            "[Session llm-8, Turn 4] user: Yeah, my cofounder loves the concurrency of Elixir, but we still have so much legacy code."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.016368865966797,
          "generation_latency_ms": 3788.3706092834473,
          "total_latency_ms": 3799.386978149414
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why does the user describe reviewing the ShipFast Core architecture as a 'headache'?",
          "answer_type": "short_answer",
          "gold_answer": "Context switching",
          "predicted_answer": "They are debating with Sato about the tech stack, and Sato is pushing for a change.",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Thanks. I'm currently reviewing the architecture for the core ShipFast project, and it's a bit of a headache.",
            "[Session llm-7, Turn 3] assistant: Is Sato still pushing for a change in the tech stack for ShipFast Core?",
            "[Session llm-0, Turn 2] user: I need to double-check something regarding the tech stack for the main ShipFast project. I was debating with Sato about it earlier."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.005380630493164,
          "generation_latency_ms": 3860.767126083374,
          "total_latency_ms": 3868.772506713867
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Among the reminders set for the user (calling mom, calling the dentist, picking up dry cleaning), which is scheduled for the latest time of day?",
          "answer_type": "short_answer",
          "gold_answer": "Picking up dry cleaning",
          "predicted_answer": "picking up dry cleaning",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Hey, hope you're having a good morning. Can you set a reminder for me to call the dentist at 2 PM?",
            "[Session llm-5, Turn 1] assistant: I've added a reminder for your dry cleaning at 6 PM.",
            "[Session llm-2, Turn 1] assistant: Good morning! I've added a reminder for the dry cleaning to your schedule for 5 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99195671081543,
          "generation_latency_ms": 3932.551622390747,
          "total_latency_ms": 3941.5435791015625
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does Sato strongly advocate for moving the entire platform to?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: It sounds like Sato is pushing for performance, even if it means maintaining two languages for a while.",
            "[Session llm-7, Turn 3] assistant: Is Sato still pushing for a change in the tech stack for ShipFast Core?",
            "[Session llm-4, Turn 7] assistant: How did that go? Did Sato have strong opinions on the tech stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000539779663086,
          "generation_latency_ms": 4188.650369644165,
          "total_latency_ms": 4197.650909423828
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does Sato's preference for Elixir apply only to new services or to the entire legacy codebase?",
          "answer_type": "short_answer",
          "gold_answer": "Entire legacy codebase",
          "predicted_answer": "The entire legacy codebase",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: That's a big decision. Since the core project relies heavily on it alongside the new Elixir stuff, you might want to discuss it with your cofounder first.",
            "[Session llm-1, Turn 4] user: Right, Sato always forgets about the Elixir implementation. Can you also remind me to call the caterers at noon?",
            "[Session llm-2, Turn 13] assistant: It's a big decision. Let me know if you want me to summarize the performance metrics between the Ruby and Elixir services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.001825332641602,
          "generation_latency_ms": 3934.5548152923584,
          "total_latency_ms": 3945.556640625
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the ShipFast Core project currently using?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: Regarding the ShipFast core project, the backend is currently a hybrid setup. We are using Ruby for the primary logic alongside Elixir.",
            "[Session llm-3, Turn 3] assistant: I believe the documentation for shipfastcore references a mix of Ruby and Elixir for the backend services.",
            "[Session llm-0, Turn 5] assistant: Yes, the current setup for ShipFast Core utilizes both Elixir and Ruby."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00943374633789,
          "generation_latency_ms": 3488.743305206299,
          "total_latency_ms": 3496.7527389526367
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Has the user mentioned using Python for any part of the ShipFast project?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: I believe the documentation for shipfastcore references a mix of Ruby and Elixir for the backend services.",
            "[Session llm-0, Turn 5] assistant: Yes, the current setup for ShipFast Core utilizes both Elixir and Ruby.",
            "[Session llm-1, Turn 3] assistant: Regarding the ShipFast core project, the backend is currently a hybrid setup. We are using Ruby for the primary logic alongside Elixir."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.019208908081055,
          "generation_latency_ms": 2861.0360622406006,
          "total_latency_ms": 2869.0552711486816
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What three components are required to construct the office wifi password according to the assistant?",
          "answer_type": "short_answer",
          "gold_answer": "Organization name, 2024, exclamation point",
          "predicted_answer": "A word, a number, and a special character.",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: By the way, a guest is coming by later. What's the wifi string for the office network?",
            "[Session llm-8, Turn 8] user: Thanks. Speaking of Sato, he just got a new tablet and is asking for the office wifi login.",
            "[Session llm-1, Turn 6] user: No, that high-level info is fine. Oh, we have a guest coming to the office later. What's the internet password?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.006500244140625,
          "generation_latency_ms": 4090.49129486084,
          "total_latency_ms": 4099.4977951049805
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What cognitive impact does the assistant identify as a consequence of managing a polyglot environment?",
          "answer_type": "short_answer",
          "gold_answer": "Taxing on mental resources",
          "predicted_answer": "Taxing on mental resources.",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: Managing a polyglot environment with both Ruby and Elixir can definitely be taxing on mental resources.",
            "[Session llm-5, Turn 3] assistant: What seems to be the issue with the project architecture?",
            "[Session llm-6, Turn 11] assistant: That's a big decision. Since the core project relies heavily on it alongside the new Elixir stuff, you might want to discuss it with your cofounder first."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.001535415649414,
          "generation_latency_ms": 2516.448497772217,
          "total_latency_ms": 2531.450033187866
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 10,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_006.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7247927188873291,
      "total_evaluation_time_s": 69.18487882614136,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the first name of the person referred to as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-3, Turn 1] assistant: Good morning! I can help with that. What is on the agenda with your cofounder today?",
            "[Session llm-9, Turn 12] user: Okay, I'll relay that to my cofounder. She was worried about legacy debt.",
            "[Session llm-6, Turn 4] user: Totally. By the way, my cofounder is dropping by the office today and she never remembers the wifi credentials."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00523567199707,
          "generation_latency_ms": 3422.2919940948486,
          "total_latency_ms": 3432.2972297668457
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific project is the user's cofounder debating the architecture for?",
          "answer_type": "short_answer",
          "gold_answer": "shipfast_core",
          "predicted_answer": "main platform",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Thanks. I've been debating with Olivia about the architecture for the main platform. Do you remember what languages we're currently running on the backend?",
            "[Session llm-8, Turn 2] user: A bit chaotic. I'm trying to review the architecture for the core platform, but I keep forgetting the exact backend split.",
            "[Session llm-1, Turn 5] assistant: It does align with your cofounder's typical preferences for backend architecture."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001991271972656,
          "generation_latency_ms": 4115.578413009644,
          "total_latency_ms": 4127.580404281616
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the pattern described by the assistant, what is the exact wifi password for the office?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 6] user: No worries, I'll check later. I have a vendor coming in soon. What's the wifi password for the office?",
            "[Session llm-9, Turn 6] user: Good to know. We have a freelancer coming into the office later to help with the migration. What's the wifi password again?",
            "[Session llm-1, Turn 6] user: Yeah. Hey, the intern is asking for the office wifi credentials. I totally forgot the password."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000871658325195,
          "generation_latency_ms": 3150.7327556610107,
          "total_latency_ms": 3161.733627319336
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "If a developer is working on the legacy modules of the core project, which programming language must they use?",
          "answer_type": "short_answer",
          "gold_answer": "Ruby",
          "predicted_answer": "Ruby",
          "retrieved_context": [
            "[Session llm-1, Turn 2] user: Thanks. I'm trying to onboard the new intern to the core repo. What languages do they need to install?",
            "[Session llm-9, Turn 2] user: Thanks. I've been debating with Olivia about the architecture for the main platform. Do you remember what languages we're currently running on the backend?",
            "[Session llm-4, Turn 2] user: Yeah, trying to. My cofounder was asking about the dependencies for the main project. Are we still split on languages?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 2469.2718982696533,
          "total_latency_ms": 2478.271961212158
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific task was the user attempting to do when they noticed it looked gloomy outside?",
          "answer_type": "short_answer",
          "gold_answer": "Update the readme",
          "predicted_answer": "Mentioning both languages to new hires regarding a dual setup.",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Right, the dual setup. I'll make sure to mention both languages so new hires don't get confused. Is it raining outside? It looks gloomy.",
            "[Session llm-0, Turn 10] user: Got it, I'm in. By the way, did it rain earlier? The sidewalk is soaked.",
            "[Session llm-7, Turn 0] user: Hey, do you know if it's supposed to rain later this afternoon?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001493453979492,
          "generation_latency_ms": 2739.246129989624,
          "total_latency_ms": 2748.2476234436035
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In what order did the user request reminders for calling their mom, the dentist, and picking up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Mom, dry cleaning, dentist",
          "predicted_answer": "Dentist, dry cleaning",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Done. I've scheduled a reminder for the dentist at 2 PM today.",
            "[Session llm-9, Turn 0] user: Hey, can you set a reminder for me to call the dentist at 2 PM?",
            "[Session llm-0, Turn 5] assistant: I've set a reminder for you to call your mom at 5 PM today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.51673698425293,
          "generation_latency_ms": 4283.132076263428,
          "total_latency_ms": 4295.648813247681
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user enjoy the context switching required by the hybrid backend stack?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: Managing a hybrid environment can be tricky. Are you referring to the mix of frameworks in ShipFast Core?",
            "[Session llm-9, Turn 4] user: Right, that's what I thought. The context switching is killing me. Did it rain yesterday?",
            "[Session llm-1, Turn 5] assistant: It does align with your cofounder's typical preferences for backend architecture."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.997793197631836,
          "generation_latency_ms": 3570.488452911377,
          "total_latency_ms": 3584.486246109009
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Which individual in the conversations has a preference for experimenting with new technology stacks?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-3, Turn 2] user: We need to review the backend for the core project. I feel like the tech stack is getting a bit complicated.",
            "[Session llm-9, Turn 2] user: Thanks. I've been debating with Olivia about the architecture for the main platform. Do you remember what languages we're currently running on the backend?",
            "[Session llm-5, Turn 6] user: I'm diving into the main repository for the core product. Why is the environment setup so complicated?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3689.459800720215,
          "total_latency_ms": 3698.4615325927734
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific database technology is the shipfast_core project using?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: It appears so. The shipfastcore codebase still relies heavily on both Ruby and Elixir frameworks.",
            "[Session llm-3, Turn 3] assistant: Managing a hybrid environment can be tricky. Are you referring to the mix of frameworks in ShipFast Core?",
            "[Session llm-7, Turn 2] user: Awesome. I was digging through the shipfastcore repo earlier and got a bit confused by the languages used."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000301361083984,
          "generation_latency_ms": 3889.8262977600098,
          "total_latency_ms": 3898.8265991210938
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Did the Giants win the game that the user asked about in the first session?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Okay, that explains the complexity. By the way, did the Giants win last night?",
            "[Session llm-0, Turn 10] user: Got it, I'm in. By the way, did it rain earlier? The sidewalk is soaked.",
            "[Session llm-0, Turn 12] user: Makes sense. Okay, back to the tech stack. Is the Elixir part handling the real-time stuff?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.027978897094727,
          "generation_latency_ms": 3179.938554763794,
          "total_latency_ms": 3191.9665336608887
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "According to the established architecture rules, which language should be used for building new high-concurrency services?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Thanks. I've been debating with Olivia about the architecture for the main platform. Do you remember what languages we're currently running on the backend?",
            "[Session llm-8, Turn 3] assistant: I can help clarify. It looks like the codebase is currently a hybrid; the older modules are written in Ruby, while the newer high-concurrency features have been built with Elixir.",
            "[Session llm-6, Turn 1] assistant: Morning! It's going great, thanks. For the core platform, remember we are in that transition phase, so it's technically a hybrid of Ruby on Rails for the legacy parts and Elixir for the newer high-concurrency services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.068437576293945,
          "generation_latency_ms": 3303.0295372009277,
          "total_latency_ms": 3316.0979747772217
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-12",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the procedure for constructing the office wifi password?",
          "answer_type": "generation",
          "gold_answer": "Capitalize company name, append year and exclamation mark",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 6] user: No worries, I'll check later. I have a vendor coming in soon. What's the wifi password for the office?",
            "[Session llm-9, Turn 6] user: Good to know. We have a freelancer coming into the office later to help with the migration. What's the wifi password again?",
            "[Session llm-1, Turn 6] user: Yeah. Hey, the intern is asking for the office wifi credentials. I totally forgot the password."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003162384033203,
          "generation_latency_ms": 3203.202724456787,
          "total_latency_ms": 3212.2058868408203
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 8,
            "accuracy": 0.625
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_016.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.8063573837280273,
      "total_evaluation_time_s": 68.31982517242432,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' in the context of the backend migration discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "A female cofounder.",
          "retrieved_context": [
            "[Session llm-7, Turn 1] assistant: No problem, I've set a reminder to email your cofounder once you arrive.",
            "[Session llm-5, Turn 9] assistant: I think my cofounder might find some in the supply closet near the break room.",
            "[Session llm-2, Turn 1] assistant: Good morning! I know the feeling. Did you manage to catch up with your cofounder yet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.652469635009766,
          "generation_latency_ms": 4267.362356185913,
          "total_latency_ms": 4279.014825820923
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the relationship between the project 'ShipFast Core' and the programming language 'Ruby'?",
          "answer_type": "short_answer",
          "gold_answer": "Uses framework",
          "predicted_answer": "ShipFast Core is running a dual stack with Ruby and Elixir, and a large portion of its",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: It looks like the shipfastcore project is still running a dual stack with both Ruby and Elixir active.",
            "[Session llm-8, Turn 11] assistant: Since the new modules in ShipFast core are likely Elixir, make sure you aren't accidentally using Ruby syntax there.",
            "[Session llm-9, Turn 3] assistant: That would be a significant change. Is the core platform currently built on Ruby?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.999866485595703,
          "generation_latency_ms": 4182.255983352661,
          "total_latency_ms": 4197.255849838257
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite earlier discussions about a hybrid stack, what does the user explicitly state is the current composition of the core platform in the final session?",
          "answer_type": "short_answer",
          "gold_answer": "All Ruby",
          "predicted_answer": "the old backend is still hanging around",
          "retrieved_context": [
            "[Session llm-0, Turn 2] user: Just reviewing the architecture for the core platform with my cofounder.",
            "[Session llm-1, Turn 4] user: Got it, thanks. Also, Olivia was asking about the backend structure for the core project. What are we running on again?",
            "[Session llm-6, Turn 11] assistant: Managing a dual-stack environment during a migration is definitely a challenge."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.010335922241211,
          "generation_latency_ms": 4203.786611557007,
          "total_latency_ms": 4215.796947479248
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-4",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the assistant's instructions, what is the complete password for the office network?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: I can update that for the organization. What is the new wifi password?",
            "[Session llm-7, Turn 6] user: No, that's fine. I have a guest coming by the office later. What's the wireless password again?",
            "[Session llm-1, Turn 2] user: Yeah, I'm trying to get on the office network. Do you remember the credentials for the org?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00356674194336,
          "generation_latency_ms": 2943.873643875122,
          "total_latency_ms": 2953.8772106170654
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Chronologically, which reminder was requested for an earlier time of day: the call to the dentist or the call to the user's mother?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist",
          "predicted_answer": "call to the dentist",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: Done, I've set a reminder for 5 PM to call your mom. Anything else on your mind?",
            "[Session llm-1, Turn 0] user: Hey, hope you're having a good morning. Can you set a reminder for my dentist appointment at 3 PM?",
            "[Session llm-3, Turn 3] assistant: I've set a timer for sixty minutes to remind you about the dentist appointment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.999130249023438,
          "generation_latency_ms": 3663.850784301758,
          "total_latency_ms": 3675.8499145507812
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user first forgot the wifi credentials, how did the assistant describe the weather?",
          "answer_type": "short_answer",
          "gold_answer": "Wind is howling",
          "predicted_answer": "the wind is howling",
          "retrieved_context": [
            "[Session llm-2, Turn 8] user: Actually, a vendor is coming by the office in an hour. Do you recall the wifi password? I always forget it.",
            "[Session llm-9, Turn 7] assistant: I can update that for the organization. What is the new wifi password?",
            "[Session llm-4, Turn 0] user: Hey, crazy weather out there today, right? I just got in and my phone won't auto-connect to the office internet."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 2896.219253540039,
          "total_latency_ms": 2905.2200317382812
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user's cofounder believe is better for scalability?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-8, Turn 5] assistant: Olivia usually has a good eye for scalability, though maintaining a hybrid environment with those two languages can definitely be tricky.",
            "[Session llm-4, Turn 7] assistant: Ruby is a solid choice for rapid development. Is my cofounder still advocating for the other option?",
            "[Session llm-4, Turn 6] user: Yeah, we are still arguing about the backend for the core platform. I'm leaning towards the language Matz created."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.539199829101562,
          "generation_latency_ms": 3520.834445953369,
          "total_latency_ms": 3529.3736457824707
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user express a personal preference for migrating to Elixir?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-6, Turn 10] user: Yeah, she loves Elixir. It's just hard to maintain both.",
            "[Session llm-7, Turn 13] assistant: I'll make a note to prioritize the Elixir documentation for the core project.",
            "[Session llm-7, Turn 12] user: Never mind, I'll check later. Just make sure those backend docs for the Elixir parts are ready."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00466537475586,
          "generation_latency_ms": 4014.7292613983154,
          "total_latency_ms": 4022.7339267730713
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific name of the sandwich shop where the user plans to have lunch with Olivia?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Hey, can you remind me to grab lunch with Olivia later today?",
            "[Session llm-9, Turn 2] user: Probably that sandwich place downstairs. We need to talk about the backend for the main project. She's really pushing for Elixir.",
            "[Session llm-3, Turn 11] assistant: I can't check the kitchen inventory, but you might want to check the break room."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.002229690551758,
          "generation_latency_ms": 2720.4811573028564,
          "total_latency_ms": 2732.483386993408
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the user's official job title at ShipFast?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: The credentials for the ShipFast network are the company name followed by 2024 and an exclamation point.",
            "[Session llm-9, Turn 8] user: It's ShipFast2024! — make sure to include the exclamation mark.",
            "[Session llm-6, Turn 6] user: I've been reviewing the shipfastcore repository this morning. It feels a bit disjointed."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00833511352539,
          "generation_latency_ms": 3277.3492336273193,
          "total_latency_ms": 3287.3575687408447
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific punctuation mark must be included at the end of the wifi password according to the assistant's instructions?",
          "answer_type": "short_answer",
          "gold_answer": "Exclamation point",
          "predicted_answer": "Exclamation mark",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: I can update that for the organization. What is the new wifi password?",
            "[Session llm-1, Turn 10] user: Back to work though. Is the wifi password just the name capitalized?",
            "[Session llm-9, Turn 9] assistant: Understood. I've updated the wifi password for ShipFast."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004831314086914,
          "generation_latency_ms": 2933.6228370666504,
          "total_latency_ms": 2942.6276683807373
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical challenge does the user identify regarding the maintenance of the current project environment?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid setup is tricky",
          "predicted_answer": "Two different tech stacks fighting each other.",
          "retrieved_context": [
            "[Session llm-1, Turn 4] user: Got it, thanks. Also, Olivia was asking about the backend structure for the core project. What are we running on again?",
            "[Session llm-3, Turn 8] user: That sounds like a headache to maintain. By the way, how about the weather?",
            "[Session llm-6, Turn 11] assistant: Managing a dual-stack environment during a migration is definitely a challenge."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00015640258789,
          "generation_latency_ms": 4289.220571517944,
          "total_latency_ms": 4300.220727920532
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_017.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7949886322021484,
      "total_evaluation_time_s": 71.89188027381897,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual frequently referred to as the user's 'cofounder' during discussions about the backend?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: A bit chaotic. My cofounder is already messaging me about the backend repo.",
            "[Session llm-1, Turn 2] user: Barely. Hey, I'm writing up a job description for a new dev to help my cofounder. What tech stack are we running on the core platform again?",
            "[Session llm-3, Turn 0] user: Hey, has my cofounder decided on the tech stack for the main project yet?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.143922805786133,
          "generation_latency_ms": 3985.8198165893555,
          "total_latency_ms": 3996.9637393951416
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between Liam and the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: Oh? What is Liam stuck on this time?",
            "[Session llm-7, Turn 3] assistant: Is Liam still trying to decide on the framework for the core project?",
            "[Session llm-8, Turn 8] user: Got it, that worked. Did I have any meetings with Liam today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.001680374145508,
          "generation_latency_ms": 4622.478246688843,
          "total_latency_ms": 4635.479927062988
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the standard pattern described for the office network credentials, what is the specific password for the user's organization?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Thanks. Also, my phone disconnected from the network. What's the password for the office wifi?",
            "[Session llm-3, Turn 2] user: Of course he is. Anyway, I need to connect my phone to the office network. What's the pass?",
            "[Session llm-5, Turn 8] user: Actually, unrelated, but I reset my tablet. What's the wifi password for the office?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.998342514038086,
          "generation_latency_ms": 3130.4197311401367,
          "total_latency_ms": 3143.418073654175
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two programming frameworks are currently utilized in the shipfast_core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir and Ruby",
          "predicted_answer": "Elixir and Ruby",
          "retrieved_context": [
            "[Session llm-6, Turn 8] user: Probably a mix. We'll likely end up running both side-by-side in the main repo for a few months to keep the shipfast core running.",
            "[Session llm-5, Turn 4] user: Yeah, specifically what frameworks we are juggling right now.",
            "[Session llm-7, Turn 3] assistant: Is Liam still trying to decide on the framework for the core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.007049560546875,
          "generation_latency_ms": 3628.1261444091797,
          "total_latency_ms": 3636.1331939697266
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific task triggered the user's request for the office network credentials during the morning they mentioned strong coffee?",
          "answer_type": "short_answer",
          "gold_answer": "Connecting new iPad",
          "predicted_answer": "connecting a new iPad to the office network",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Thanks. Also, my phone disconnected from the network. What's the password for the office wifi?",
            "[Session llm-4, Turn 6] user: Nah, I'll figure it out. Oh, speaking of setup, my phone disconnected from the network. What's the office wifi key again?",
            "[Session llm-6, Turn 2] user: Strong, thankfully. I'm trying to get my new iPad connected to the office network. Did we change the credentials recently?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.518074035644531,
          "generation_latency_ms": 2955.4429054260254,
          "total_latency_ms": 2965.96097946167
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user admitted forgetting about the Phoenix component, what specific functionality was attributed to the Elixir service?",
          "answer_type": "short_answer",
          "gold_answer": "Sockets",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Right, I completely forgot about the Phoenix component. That explains the dependencies.",
            "[Session llm-3, Turn 6] user: Do you think we're going to fully switch to Elixir eventually?",
            "[Session llm-5, Turn 5] assistant: You can tell him the core project is currently utilizing both Elixir and Ruby."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.024879455566406,
          "generation_latency_ms": 4003.1116008758545,
          "total_latency_ms": 4015.136480331421
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user prefer to interrupt Liam during meetings to get answers about lunch orders?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Hey, did Liam get back to you about the lunch order yet?",
            "[Session llm-8, Turn 9] assistant: Checking your calendar... nope, you're clear of meetings with him for the rest of the day.",
            "[Session llm-4, Turn 1] assistant: Not yet, I think he is stuck in a meeting. Do you want me to ping him?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.135339736938477,
          "generation_latency_ms": 3838.886022567749,
          "total_latency_ms": 3850.0213623046875
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user describe their preference for the strength of their coffee in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Strong",
          "predicted_answer": "Strong",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: Good morning. It looks a bit overcast, but no rain yet. How is the coffee treating you?",
            "[Session llm-0, Turn 8] user: Perfect. I'm feeling really tired today, maybe I need more coffee.",
            "[Session llm-0, Turn 9] assistant: Caffeine usually helps! Do you want me to look up coffee shops nearby?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001016616821289,
          "generation_latency_ms": 4147.646188735962,
          "total_latency_ms": 4156.647205352783
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Has the team discussed migrating the core project to Python or Django?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Is Liam still trying to decide on the framework for the core project?",
            "[Session llm-4, Turn 2] user: No, it's fine, I'll just grab a sandwich later. By the way, I'm setting up the new dev environment for the core repo. Are we still fully on Rails?",
            "[Session llm-8, Turn 2] user: I'm trying to remember what Liam said about the backend for the main project. Are we sticking with the old framework?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.432865142822266,
          "generation_latency_ms": 4262.965202331543,
          "total_latency_ms": 4271.398067474365
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What specific sandwich did the user decide to grab after declining to ping Liam about the lunch order?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Hey, did Liam get back to you about the lunch order yet?",
            "[Session llm-6, Turn 5] assistant: Oh? What is Liam stuck on this time?",
            "[Session llm-4, Turn 1] assistant: Not yet, I think he is stuck in a meeting. Do you want me to ping him?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004281997680664,
          "generation_latency_ms": 4156.787395477295,
          "total_latency_ms": 4166.791677474976
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the established pattern for constructing the office wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name capitalized, year, exclamation point",
          "predicted_answer": "capitalized",
          "retrieved_context": [
            "[Session llm-5, Turn 8] user: Actually, unrelated, but I reset my tablet. What's the wifi password for the office?",
            "[Session llm-1, Turn 6] user: Thanks. Also, my phone disconnected from the network. What's the password for the office wifi?",
            "[Session llm-4, Turn 6] user: Nah, I'll figure it out. Oh, speaking of setup, my phone disconnected from the network. What's the office wifi key again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.012937545776367,
          "generation_latency_ms": 3741.055727005005,
          "total_latency_ms": 3750.0686645507812
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "According to the assistant's advice, what is the best way to explain the core project's stack to the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Utilizing both Elixir and Ruby",
          "predicted_answer": "Tell him the core project is currently utilizing both Elixir and Ruby.",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Is Liam still trying to decide on the framework for the core project?",
            "[Session llm-5, Turn 3] assistant: Dealing with cofounders can be tricky. Does he need a refresher on the languages used in the core repository?",
            "[Session llm-6, Turn 9] assistant: Managing a polyglot stack can be tricky. Do you need me to find some resources on integrating those two environments?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 4190.614700317383,
          "total_latency_ms": 4199.614763259888
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 9,
            "accuracy": 0.5555555555555556
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_019.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.9714193344116211,
      "total_evaluation_time_s": 72.75794839859009,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' frequently mentioned by the user during discussions about backend performance?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: It's a bit of a hybrid situation. My cofounder is really into that functional language from Jose Valim, so we're seeing a lot of that.",
            "[Session llm-1, Turn 5] assistant: Sato does seem to enjoy exploring high-concurrency frameworks.",
            "[Session llm-6, Turn 2] user: Speaking of PRs, what's the deal with the backend for the core module right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002779006958008,
          "generation_latency_ms": 4809.226274490356,
          "total_latency_ms": 4820.229053497314
        },
        {
          "probe_id": "world_modeling-task_state_tracking-0",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "Is the migration of the shipfast_core project to Elixir considered fully complete?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: No, for the new high-concurrency stuff, the team switched over to Elixir.",
            "[Session llm-8, Turn 11] assistant: You mean Elixir? That would be a massive undertaking given how much is already built.",
            "[Session llm-5, Turn 10] user: No worries. Back to work stuff—is there a specific linter we use for the Elixir files?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.004779815673828,
          "generation_latency_ms": 4555.683612823486,
          "total_latency_ms": 4568.68839263916
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What password does the user need to enter to connect their tablet to the 'org:shipfast' network?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: No, I'll just put on a jacket. I'm trying to get my new tablet set up on the network, but I blanked on the credentials.",
            "[Session llm-2, Turn 8] user: Speaking of coding, I need to connect my new phone to the office network. What's the pass again?",
            "[Session llm-9, Turn 6] user: Haha, fair enough. I have a vendor coming in later; do you know what the wifi password is these days?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 17.004728317260742,
          "generation_latency_ms": 4262.580394744873,
          "total_latency_ms": 4279.585123062134
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which specific programming language is the team using for the 'high-concurrency' services in the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, specifically the concurrency parts. That's why we're leaning so heavily into that functional language based on Erlang for the core services now.",
            "[Session llm-1, Turn 5] assistant: Sato does seem to enjoy exploring high-concurrency frameworks.",
            "[Session llm-5, Turn 3] assistant: Looking at the file structure for the core project, it appears to be a hybrid environment. I see significant usage of the language associated with Rails, alongside a functional language that runs on the BEAM virtual machine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00125503540039,
          "generation_latency_ms": 4183.560848236084,
          "total_latency_ms": 4192.562103271484
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What technical issue primarily drove the decision to adopt Elixir for the new shipfast_core services?",
          "answer_type": "short_answer",
          "gold_answer": "Concurrency",
          "predicted_answer": "high-concurrency",
          "retrieved_context": [
            "[Session llm-4, Turn 5] assistant: No, for the new high-concurrency stuff, the team switched over to Elixir.",
            "[Session llm-5, Turn 10] user: No worries. Back to work stuff—is there a specific linter we use for the Elixir files?",
            "[Session llm-9, Turn 3] assistant: I recall from the meeting notes that there's a heavy reliance on Ruby for the existing codebase, but there was also a significant move toward Elixir for the newer services."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.017229080200195,
          "generation_latency_ms": 4042.2356128692627,
          "total_latency_ms": 4051.252841949463
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What device was the user attempting to configure when they requested the office network credentials?",
          "answer_type": "short_answer",
          "gold_answer": "Tablet",
          "predicted_answer": "new phone",
          "retrieved_context": [
            "[Session llm-2, Turn 8] user: Speaking of coding, I need to connect my new phone to the office network. What's the pass again?",
            "[Session llm-4, Turn 9] assistant: Yes, for the office network, the password is the company name combined with the current year and an exclamation point at the end. Note the capitalization on the name.",
            "[Session llm-7, Turn 9] assistant: Do you need the credentials for the office internet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 23.529529571533203,
          "generation_latency_ms": 4366.066455841064,
          "total_latency_ms": 4389.595985412598
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the user's cofounder explicitly favor for the new backend modules?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Speaking of PRs, what's the deal with the backend for the core module right now?",
            "[Session llm-2, Turn 2] user: Fair enough. I'm trying to remember what we built the core backend in. Was it Python?",
            "[Session llm-9, Turn 8] user: Thanks. Back to the ShipFast core repo, are we fully committed to those two languages, or was there a third one?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.517473220825195,
          "generation_latency_ms": 3436.8157386779785,
          "total_latency_ms": 3452.3332118988037
        },
        {
          "probe_id": "preference_learning-preference_scope-0",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "When the office thermostat was broken, did the user prefer to report it to the manager or handle the cold personally?",
          "answer_type": "short_answer",
          "gold_answer": "Handle personally",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 0] user: It is freezing in the office today, I think the thermostat is broken again.",
            "[Session llm-8, Turn 1] assistant: That sounds uncomfortable. Do you want me to look up the building manager's number so you can report it?",
            "[Session llm-0, Turn 10] user: Got it. Did I leave my jacket in the meeting room?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002447128295898,
          "generation_latency_ms": 3121.6704845428467,
          "total_latency_ms": 3130.6729316711426
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific version number of the Ruby environment currently running in production?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: I believe Sato said the migration isn't 100% complete. The core system still has a significant amount of Ruby code running in production.",
            "[Session llm-6, Turn 5] assistant: Yeah, the original codebase is all standard .rb files. We haven't fully deprecated the Rails-style logic yet.",
            "[Session llm-1, Turn 2] user: I'm trying to get my dev environment set up for the core platform. Are we strictly a Rails shop these days?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003089904785156,
          "generation_latency_ms": 3803.8318157196045,
          "total_latency_ms": 3813.8349056243896
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What frontend framework is used in the shipfast_core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 5] assistant: It certainly keeps the ShipFast architecture unique.",
            "[Session llm-5, Turn 3] assistant: Looking at the file structure for the core project, it appears to be a hybrid environment. I see significant usage of the language associated with Rails, alongside a functional language that runs on the BEAM virtual machine.",
            "[Session llm-2, Turn 2] user: Fair enough. I'm trying to remember what we built the core backend in. Was it Python?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.555961608886719,
          "generation_latency_ms": 3817.5456523895264,
          "total_latency_ms": 3825.101613998413
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's explanation, what is the formula used to construct the ShipFast wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name + Year + !",
          "predicted_answer": "company name combined with the current year and an exclamation point at the end, with capitalization on the name",
          "retrieved_context": [
            "[Session llm-9, Turn 6] user: Haha, fair enough. I have a vendor coming in later; do you know what the wifi password is these days?",
            "[Session llm-4, Turn 9] assistant: Yes, for the office network, the password is the company name combined with the current year and an exclamation point at the end. Note the capitalization on the name.",
            "[Session llm-2, Turn 8] user: Speaking of coding, I need to connect my new phone to the office network. What's the pass again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00811767578125,
          "generation_latency_ms": 3552.60968208313,
          "total_latency_ms": 3565.617799758911
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific time-management action does the user frequently request the assistant to perform across multiple sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Set reminders",
          "predicted_answer": "Setting reminders",
          "retrieved_context": [
            "[Session llm-0, Turn 13] assistant: Added \"Call Mom\" to your schedule for 5 PM.",
            "[Session llm-7, Turn 3] assistant: Is the current setup struggling to handle the load?",
            "[Session llm-0, Turn 5] assistant: Exactly. It seems the team is split between sticking with the classic web framework and moving everything to that newer, concurrent one."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 3588.7291431427,
          "total_latency_ms": 3597.729206085205
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 9,
            "accuracy": 0.6666666666666666
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_023.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.6624181270599365,
      "total_evaluation_time_s": 68.71261501312256,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' during the discussions about the tech stack?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Sato",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Yeah, actually. Sato is asking for a summary of our tech stack choices for the main project. I forgot exactly what we settled on.",
            "[Session llm-1, Turn 3] assistant: Are you referring to the ongoing discussions with Sato about the ShipFast core project?",
            "[Session llm-3, Turn 3] assistant: Ah, is your cofounder pushing for a change in the backend languages?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002540588378906,
          "generation_latency_ms": 4024.407148361206,
          "total_latency_ms": 4035.409688949585
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Sato?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Cofounder",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Yeah, actually. Sato is asking for a summary of our tech stack choices for the main project. I forgot exactly what we settled on.",
            "[Session llm-4, Turn 4] user: Right, Sato really pushed for that switch last year. He's obsessed with functional programming.",
            "[Session llm-3, Turn 2] user: It's okay, a bit stressful. Sato and I are debating the tech stack for the main project again."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.01031494140625,
          "generation_latency_ms": 4214.908123016357,
          "total_latency_ms": 4223.918437957764
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages make up the hybrid backend stack for the ShipFast core project?",
          "answer_type": "short_answer",
          "gold_answer": "Ruby and Elixir",
          "predicted_answer": "Ruby and Elixir",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Looking at the repository details, it seems the core system is a hybrid. It utilizes that dynamic language created by Matz, alongside the functional one that runs on the Erlang VM.",
            "[Session llm-4, Turn 2] user: Perfect, thanks. I'm trying to get the new intern set up on the ShipFast core repo. Are we still running that hybrid stack?",
            "[Session llm-8, Turn 3] assistant: For the core ShipFast project, the documentation points to a dual setup involving Elixir and Ruby."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.029481887817383,
          "generation_latency_ms": 3187.173366546631,
          "total_latency_ms": 3198.2028484344482
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "If a developer is tasked with working on the real-time components of the core project, which language framework must they use?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Looking at the repository details, it seems the core system is a hybrid. It utilizes that dynamic language created by Matz, alongside the functional one that runs on the Erlang VM.",
            "[Session llm-6, Turn 2] user: Alright. I'm trying to get the dev environment set up for the core platform. What languages do I need installed?",
            "[Session llm-7, Turn 2] user: It went into overtime, pretty intense. Anyway, I'm reviewing the architecture docs for the core project. Are we still maintaining a dual-language backend?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00191879272461,
          "generation_latency_ms": 3373.619794845581,
          "total_latency_ms": 3386.6217136383057
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user mentioned heading out for coffee, what specific task did they ask to be reminded about regarding Sato?",
          "answer_type": "short_answer",
          "gold_answer": "Email him about deployment schedule",
          "predicted_answer": "Email him about the deployment schedule",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Yeah, actually. Sato is asking for a summary of our tech stack choices for the main project. I forgot exactly what we settled on.",
            "[Session llm-1, Turn 3] assistant: Are you referring to the ongoing discussions with Sato about the ShipFast core project?",
            "[Session llm-4, Turn 4] user: Right, Sato really pushed for that switch last year. He's obsessed with functional programming."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.015798568725586,
          "generation_latency_ms": 3435.1885318756104,
          "total_latency_ms": 3444.204330444336
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the 'sports feed' occur before or after the user asked for a reminder to call the dentist?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Morning. Any big news on the sports feed today?",
            "[Session llm-4, Turn 11] assistant: I've added a reminder for your dentist appointment at 4 PM today.",
            "[Session llm-8, Turn 5] assistant: I've added a reminder to call the dentist at 2 PM. Is there anything else your cofounder needs regarding the architecture?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00246810913086,
          "generation_latency_ms": 4217.813491821289,
          "total_latency_ms": 4229.81595993042
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What programming paradigm is Sato explicitly described as being 'obsessed with'?",
          "answer_type": "short_answer",
          "gold_answer": "Functional programming",
          "predicted_answer": "functional programming",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Right, Sato really pushed for that switch last year. He's obsessed with functional programming.",
            "[Session llm-8, Turn 2] user: Yeah, actually. Sato is asking for a summary of our tech stack choices for the main project. I forgot exactly what we settled on.",
            "[Session llm-1, Turn 3] assistant: Are you referring to the ongoing discussions with Sato about the ShipFast core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000633239746094,
          "generation_latency_ms": 2974.3902683258057,
          "total_latency_ms": 2985.3909015655518
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Despite the complexity, what technical benefit does the user state they like about the current dual-language setup?",
          "answer_type": "short_answer",
          "gold_answer": "Concurrency handling",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: It went into overtime, pretty intense. Anyway, I'm reviewing the architecture docs for the core project. Are we still maintaining a dual-language backend?",
            "[Session llm-2, Turn 8] user: I noticed. My cofounder has been pushing for that new BEAM-based language, right?",
            "[Session llm-9, Turn 3] assistant: Looking at the repository details, it seems the core system is a hybrid. It utilizes that dynamic language created by Matz, alongside the functional one that runs on the Erlang VM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.994672775268555,
          "generation_latency_ms": 4289.8008823394775,
          "total_latency_ms": 4300.795555114746
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Does the ShipFast core project currently utilize Python for its backend services?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Yes, looking at the current setup for Project ShipFast, the backend is still split between Ruby and Elixir.",
            "[Session llm-6, Turn 3] assistant: For `shipfastcore`, you're going to need a dual setup. It looks like the legacy parts are all Ruby, but the newer real-time services are running on Elixir.",
            "[Session llm-8, Turn 3] assistant: For the core ShipFast project, the documentation points to a dual setup involving Elixir and Ruby."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.003473281860352,
          "generation_latency_ms": 3949.2576122283936,
          "total_latency_ms": 3957.261085510254
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the wifi password for the ShipFast organization?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-1, Turn 11] assistant: The password for the wifi is ShipFast2024!.",
            "[Session llm-0, Turn 7] assistant: I do. The password for the wifi is ShipFast2024! — don't forget the exclamation point.",
            "[Session llm-3, Turn 7] assistant: Do you need the password for the office WiFi?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 3050.616979598999,
          "total_latency_ms": 3060.6203079223633
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the setup discussions, what language must be installed to run the legacy parts of the ShipFast core platform?",
          "answer_type": "short_answer",
          "gold_answer": "Ruby",
          "predicted_answer": "Ruby",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: Alright. I'm trying to get the dev environment set up for the core platform. What languages do I need installed?",
            "[Session llm-9, Turn 3] assistant: Looking at the repository details, it seems the core system is a hybrid. It utilizes that dynamic language created by Matz, alongside the functional one that runs on the Erlang VM.",
            "[Session llm-4, Turn 2] user: Perfect, thanks. I'm trying to get the new intern set up on the ShipFast core repo. Are we still running that hybrid stack?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00070571899414,
          "generation_latency_ms": 2724.780797958374,
          "total_latency_ms": 2734.781503677368
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Based on the architecture reviews, how is the backend logic distributed between the two languages?",
          "answer_type": "generation",
          "gold_answer": "Ruby for legacy, Elixir for real-time",
          "predicted_answer": "Ruby for legacy logic and Elixir for real-time components.",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: It went into overtime, pretty intense. Anyway, I'm reviewing the architecture docs for the core project. Are we still maintaining a dual-language backend?",
            "[Session llm-9, Turn 3] assistant: Looking at the repository details, it seems the core system is a hybrid. It utilizes that dynamic language created by Matz, alongside the functional one that runs on the Erlang VM.",
            "[Session llm-4, Turn 3] assistant: I believe so. The backend is currently a mix, primarily utilizing Ruby for the legacy logic but integrating Elixir for the real-time components."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000778198242188,
          "generation_latency_ms": 4245.718717575073,
          "total_latency_ms": 4254.719495773315
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 10,
            "accuracy": 0.8
          },
          "abstain": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "generation": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_025.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.722114086151123,
      "total_evaluation_time_s": 73.2287278175354,
      "probe_results": [
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between Elijah and the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Cofounder",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: Yeah, my cofounder loves Elixir, so that's probably why. Speaking of him, is Elijah free later?",
            "[Session llm-9, Turn 3] assistant: That sounds intense for the morning. What specifically is Elijah asking about?",
            "[Session llm-1, Turn 4] user: Right, I remember my cofounder pushing for that transition. Speaking of Elijah, did he send over those design files?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.000654220581055,
          "generation_latency_ms": 4296.837329864502,
          "total_latency_ms": 4310.837984085083
        },
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific project is the cofounder rewriting the message bus for?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast core",
          "predicted_answer": "core platform",
          "retrieved_context": [
            "[Session llm-7, Turn 4] user: Yeah, for the core platform. He's rewriting the message bus in Elixir, which is cool, but the rest of the app is still Ruby.",
            "[Session llm-1, Turn 2] user: Yeah, actually. I'm writing up some documentation for the new interns. What's the primary tech stack for the main ShipFast project again?",
            "[Session llm-6, Turn 5] assistant: That makes sense. Your cofounder usually has a good eye for architectural consolidation."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000799179077148,
          "generation_latency_ms": 3421.1483001708984,
          "total_latency_ms": 3433.1490993499756
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What two programming languages are currently used in the ShipFast core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir and Ruby",
          "predicted_answer": "Elixir and Ruby",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: He's asking about the languages backing the core ShipFast project. He forgot what we settled on.",
            "[Session llm-8, Turn 3] assistant: Regarding the ShipFast core project, my records indicate a mix of frameworks; it seems to be using both Elixir and Ruby right now.",
            "[Session llm-4, Turn 9] assistant: Understood. I've noted that Project ShipFast is utilizing both frameworks for the time being."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.001535415649414,
          "generation_latency_ms": 4088.3796215057373,
          "total_latency_ms": 4103.381156921387
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the ShipFast office wifi?",
          "answer_type": "verbatim",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-8, Turn 7] assistant: I believe the wifi password for the org is simply the company name followed by the current year and an exclamation mark, so ShipFast2024!.",
            "[Session llm-4, Turn 13] assistant: I can help with that. What is the new password for the ShipFast office network?",
            "[Session llm-3, Turn 4] user: He definitely needs one. By the way, my phone disconnected from the office network. What's the password for the org wifi again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.004426956176758,
          "generation_latency_ms": 2905.182361602783,
          "total_latency_ms": 2913.18678855896
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user asked for the guest wifi string, what was the weather forecast?",
          "answer_type": "short_answer",
          "gold_answer": "Heavy rain",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 4] user: Good call. Speaking of the office, my phone keeps dropping the connection. Can you remind me of the guest wifi string?",
            "[Session llm-8, Turn 8] user: Got it, I'll text it to him. Did the weather report mention rain for the weekend?",
            "[Session llm-0, Turn 12] user: It worked. By the way, did you see the weather forecast for tomorrow?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.524770736694336,
          "generation_latency_ms": 4095.0422286987305,
          "total_latency_ms": 4108.566999435425
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask about the wifi password before or after the discussion about the message bus rewrite?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 4] user: He definitely needs one. By the way, my phone disconnected from the office network. What's the password for the org wifi again?",
            "[Session llm-7, Turn 8] user: Also, we have a consultant coming in to help with the Elixir transition. What's our wifi pass again?",
            "[Session llm-8, Turn 7] assistant: I believe the wifi password for the org is simply the company name followed by the current year and an exclamation mark, so ShipFast2024!."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001970291137695,
          "generation_latency_ms": 4669.456243515015,
          "total_latency_ms": 4678.458213806152
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What frontend framework does the ShipFast project use?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 9] assistant: Understood. I've noted that Project ShipFast is utilizing both frameworks for the time being.",
            "[Session llm-8, Turn 3] assistant: Regarding the ShipFast core project, my records indicate a mix of frameworks; it seems to be using both Elixir and Ruby right now.",
            "[Session llm-6, Turn 3] assistant: Yes, it appears the ShipFast core project is currently built using both Elixir and Ruby."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00180435180664,
          "generation_latency_ms": 3647.09210395813,
          "total_latency_ms": 3655.0939083099365
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the user's favorite brand of coffee beans?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: I need to buy more coffee beans on my way home around 6 PM.",
            "[Session llm-1, Turn 0] user: Hey, hope you're having a good morning. Can you remind me to buy coffee beans later?",
            "[Session llm-1, Turn 1] assistant: Good morning! I've added coffee beans to your shopping list. Is there anything else you need help with right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.010025024414062,
          "generation_latency_ms": 4100.172281265259,
          "total_latency_ms": 4113.182306289673
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the cofounder prefer a split stack or a consolidated architecture in the long run?",
          "answer_type": "short_answer",
          "gold_answer": "Consolidated architecture",
          "predicted_answer": "Consolidated architecture",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: That makes sense. Your cofounder usually has a good eye for architectural consolidation.",
            "[Session llm-4, Turn 5] assistant: Always a big decision. What direction is your cofounder leaning towards for the core project?",
            "[Session llm-8, Turn 2] user: No, keep it free for now. I might grab coffee with my cofounder. Speaking of Elijah, we were debating the backend stack for the core platform earlier. What are we actually using?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.006832122802734,
          "generation_latency_ms": 4253.551006317139,
          "total_latency_ms": 4264.557838439941
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Did the user want to schedule a meeting at 2 PM in the session where they discussed the backend stack with Elijah?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: That sounds intense for the morning. What specifically is Elijah asking about?",
            "[Session llm-3, Turn 8] user: Makes sense. Can you set a reminder for me to ping Elijah about the migration meeting at 2 PM?",
            "[Session llm-5, Turn 11] assistant: I can check Elijah's calendar if you would like to propose a time."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 18.000125885009766,
          "generation_latency_ms": 3823.0090141296387,
          "total_latency_ms": 3841.0091400146484
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, how should the guest wifi password be constructed?",
          "answer_type": "generation",
          "gold_answer": "Company name, year, exclamation point",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 4] user: He definitely needs one. By the way, my phone disconnected from the office network. What's the password for the org wifi again?",
            "[Session llm-1, Turn 6] user: No, that's fine, I'll just ask him when he gets here. By the way, a guest just walked in and needs the internet access. What's the password?",
            "[Session llm-8, Turn 7] assistant: I believe the wifi password for the org is simply the company name followed by the current year and an exclamation mark, so ShipFast2024!."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.007381439208984,
          "generation_latency_ms": 3650.5112648010254,
          "total_latency_ms": 3660.5186462402344
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific update did the user decide to make to the dev setup guide regarding the languages?",
          "answer_type": "short_answer",
          "gold_answer": "Support both Elixir and Ruby",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: He's asking about the languages backing the core ShipFast project. He forgot what we settled on.",
            "[Session llm-6, Turn 12] user: Good to know. Back to the tech stack, do we know which language is handling the primary load right now?",
            "[Session llm-6, Turn 2] user: Thanks. I was looking through the codebase for the core platform earlier. Are we still running a dual setup for the backend languages?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00411605834961,
          "generation_latency_ms": 4194.962501525879,
          "total_latency_ms": 4203.9666175842285
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 7,
            "accuracy": 0.42857142857142855
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_027.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7658882141113281,
      "total_evaluation_time_s": 73.92225980758667,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual described as being 'dead set' on using Elixir for the main services?",
          "answer_type": "short_answer",
          "gold_answer": "Sato",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 2] user: We were arguing about the tech stack for the core project again. He's dead set on using Elixir for the main services.",
            "[Session llm-5, Turn 9] assistant: The transition to Elixir is definitely underway and active, but it hasn't completely replaced the original codebase yet.",
            "[Session llm-8, Turn 10] user: So we are transitioning to Elixir?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.115644454956055,
          "generation_latency_ms": 4517.268896102905,
          "total_latency_ms": 4530.384540557861
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is Sato's professional relationship to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounder",
          "predicted_answer": "Colleague",
          "retrieved_context": [
            "[Session llm-1, Turn 5] assistant: It sounds like Sato made a technical decision to handle the real-time requirements.",
            "[Session llm-4, Turn 0] user: Hey, I just got into the office. Is Sato around? Also, remind me to call my mom at 5 PM.",
            "[Session llm-0, Turn 5] assistant: Sato does enjoy experimenting with different frameworks. It keeps the engineering interesting, at least."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002208709716797,
          "generation_latency_ms": 4147.851943969727,
          "total_latency_ms": 4156.854152679443
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming languages are confirmed to make up the hybrid architecture of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir and Ruby",
          "predicted_answer": "Ruby and Elixir",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: Ah, for the ShipFast core? I remember my cofounder, Sato, mentioning a specific combination of languages.",
            "[Session llm-9, Turn 3] assistant: Yes, the architecture seems to be a hybrid. It relies on Ruby for the standard logic and Elixir for the concurrent processes.",
            "[Session llm-8, Turn 11] assistant: That's the plan for the high-traffic components, so expect to see both languages in the repo for a while."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.999866485595703,
          "generation_latency_ms": 3618.1130409240723,
          "total_latency_ms": 3633.112907409668
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-1",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact password for the ShipFast office network?",
          "answer_type": "verbatim",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: No problem. For the ShipFast network, the password is just the company name, capitalized S and F, followed by the current year and an exclamation mark.",
            "[Session llm-2, Turn 7] assistant: I do. The password for the organization's wifi is ShipFast2024!.",
            "[Session llm-8, Turn 4] user: Thanks. I'm trying to get my iPad on the office network. I forgot the credentials for the ShipFast wifi."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003805160522461,
          "generation_latency_ms": 3343.8773155212402,
          "total_latency_ms": 3353.8811206817627
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user complain about the 'messy' repo before or after Sato brought in the new bag of coffee beans?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: I think Sato brought in a new bag of beans yesterday, check the kitchen counter.",
            "[Session llm-3, Turn 5] assistant: How did the discussion with Sato go regarding the ShipFast core?",
            "[Session llm-4, Turn 8] user: Okay. Back to the code—Sato is pushing hard for the new stuff. Does the Elixir implementation cover the main billing module yet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.50912857055664,
          "generation_latency_ms": 4541.380882263184,
          "total_latency_ms": 4556.89001083374
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user asked about the weather forecast, what specific information did they request regarding the office?",
          "answer_type": "short_answer",
          "gold_answer": "Wifi password",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Good morning! The forecast does look a bit gray with a chance of showers in the afternoon. Do you need an umbrella?",
            "[Session llm-0, Turn 12] user: Just checking the weather, looks like rain later.",
            "[Session llm-3, Turn 10] user: Not yet, maybe a stray. Anyway, back to business. We have a consultant coming to the office today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 19.005775451660156,
          "generation_latency_ms": 4247.721195220947,
          "total_latency_ms": 4266.726970672607
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Which programming language does the cofounder strongly prefer for real-time components?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "Elixir",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Yeah, your cofounder is really pushing for that functional language running on the Erlang VM for the real-time features.",
            "[Session llm-1, Turn 5] assistant: It sounds like Sato made a technical decision to handle the real-time requirements.",
            "[Session llm-7, Turn 5] assistant: I think he decided on a mix. He wanted Ruby for the rapid prototyping feel, but Elixir for the real-time components."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000322341918945,
          "generation_latency_ms": 4365.263938903809,
          "total_latency_ms": 4377.2642612457275
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to completely eliminate Ruby from the core project?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-5, Turn 7] assistant: It looks like the core platform is currently a mix. There is still a significant amount of Ruby being used.",
            "[Session llm-4, Turn 14] user: Just say we are freezing feature development on the Ruby side of the core project to focus on the migration.",
            "[Session llm-6, Turn 4] user: Yeah, mostly. But we have to keep Ruby for the older scripts and some side modules. So it's a mix."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 15.006780624389648,
          "generation_latency_ms": 4311.264276504517,
          "total_latency_ms": 4326.271057128906
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What database technology is used for the ShipFast Core project?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: I believe the ShipFast core is using a hybrid approach. It looks like a combination of Ruby and Elixir based on the latest repository stats.",
            "[Session llm-2, Turn 3] assistant: Yes, looking at the current specs for the ShipFast core project, it utilizes both Elixir and Ruby.",
            "[Session llm-1, Turn 3] assistant: That's correct. The ShipFast core project is currently built using both Ruby and Elixir."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.000062942504883,
          "generation_latency_ms": 3752.379894256592,
          "total_latency_ms": 3761.3799571990967
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Did the Knicks win the game mentioned in the session where the user watered the plants?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Okay, just wanted to double-check before I update the wiki. By the way, did the Knicks win last night?",
            "[Session llm-2, Turn 8] user: Awesome, thanks. Also, did the Giants win last night?",
            "[Session llm-7, Turn 6] user: That makes sense. By the way, did you see the game last night?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.001110076904297,
          "generation_latency_ms": 4171.137571334839,
          "total_latency_ms": 4182.138681411743
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What punctuation mark is required at the end of the ShipFast wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Exclamation mark",
          "predicted_answer": "Exclamation mark",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: No problem. For the ShipFast network, the password is just the company name, capitalized S and F, followed by the current year and an exclamation mark.",
            "[Session llm-9, Turn 6] user: No worries. I have a guest here at the ShipFast office. What's the wifi string again?",
            "[Session llm-2, Turn 7] assistant: I do. The password for the organization's wifi is ShipFast2024!."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001897811889648,
          "generation_latency_ms": 3410.475492477417,
          "total_latency_ms": 3420.4773902893066
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "In the hybrid architecture, what specific role is assigned to the Ruby language?",
          "answer_type": "short_answer",
          "gold_answer": "Standard logic",
          "predicted_answer": "standard logic, older scripts, side modules, rapid prototyping feel",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Yes, the architecture seems to be a hybrid. It relies on Ruby for the standard logic and Elixir for the concurrent processes.",
            "[Session llm-4, Turn 5] assistant: I noticed the architecture is a bit split. It looks like there's still a lot of Ruby code, but a significant portion is being written in Elixir now.",
            "[Session llm-5, Turn 7] assistant: It looks like the core platform is currently a mix. There is still a significant amount of Ruby being used."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.000415802001953,
          "generation_latency_ms": 4023.4322547912598,
          "total_latency_ms": 4037.4326705932617
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "verbatim": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_037.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.715430498123169,
      "total_evaluation_time_s": 68.71420693397522,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' during the technical debt discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Mostly technical debt discussions with my cofounder. We're at a crossroads with the main repo.",
            "[Session llm-2, Turn 9] assistant: Correct, it's the company name capitalized, then 2024, ending with an exclamation point.",
            "[Session llm-6, Turn 7] assistant: It should be the company name capitalized, followed by the year 2024 and an exclamation point."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.510942459106445,
          "generation_latency_ms": 3323.5702514648438,
          "total_latency_ms": 3337.08119392395
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the professional relationship between the user and Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Cofounders",
          "predicted_answer": "Colleagues",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Is this regarding Charlotte and the architecture of the core project?",
            "[Session llm-3, Turn 3] assistant: I can imagine. Is Charlotte helping you with the codebase today?",
            "[Session llm-7, Turn 4] user: Right, okay. Charlotte was asking about that earlier. She's worried about the complexity."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3915.170431137085,
          "total_latency_ms": 3924.1721630096436
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two programming frameworks are currently used in the hybrid backend of the ShipFast Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Ruby and Elixir",
          "predicted_answer": "Ruby and Elixir",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Yeah, I was looking at the ShipFast Core repo. Are we still splitting the backend between two languages?",
            "[Session llm-6, Turn 13] assistant: That's correct, the primary frameworks identified for the core project are still Ruby and Elixir.",
            "[Session llm-6, Turn 3] assistant: Yes, the shipfastcore project is currently utilizing both Ruby and Elixir. You might want to mention both in the spec."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.002229690551758,
          "generation_latency_ms": 3888.1759643554688,
          "total_latency_ms": 3900.1781940460205
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What is the exact wifi password for the ShipFast organization?",
          "answer_type": "verbatim",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast[current year]!",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Do they need the password for the main ShipFast network?",
            "[Session llm-3, Turn 6] user: No, we've got a handle on it, it's just tedious. Oh, unrelated, but a vendor is stopping by the office. What's our wifi password again?",
            "[Session llm-9, Turn 6] user: Yeah, probably. He might need to send a few emails while he waits for me. What's the office wifi password these days?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999109268188477,
          "generation_latency_ms": 3192.776679992676,
          "total_latency_ms": 3201.7757892608643
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "For what specific type of features was Elixir introduced into the ShipFast Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Real-time / high-concurrency",
          "predicted_answer": "high-concurrency stuff",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: Yes, the shipfastcore project is currently utilizing both Ruby and Elixir. You might want to mention both in the spec.",
            "[Session llm-8, Turn 13] assistant: Reminder set to check the Elixir build pipeline for the project.",
            "[Session llm-4, Turn 5] assistant: Yes, that's the foundation, but remember we started porting high-concurrency stuff over to Elixir recently."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001659393310547,
          "generation_latency_ms": 3662.6181602478027,
          "total_latency_ms": 3672.6198196411133
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version of Ruby is the ShipFast Core project currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: Yes, the shipfastcore project is currently utilizing both Ruby and Elixir. You might want to mention both in the spec.",
            "[Session llm-6, Turn 13] assistant: That's correct, the primary frameworks identified for the core project are still Ruby and Elixir.",
            "[Session llm-5, Turn 5] assistant: Managing a polyglot setup like that in ShipFast Core can be tricky. Is the friction about maintaining the Ruby parts?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001731872558594,
          "generation_latency_ms": 3487.574577331543,
          "total_latency_ms": 3496.5763092041016
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the lead engineer responsible for the Elixir migration?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 13] assistant: Reminder set to check the Elixir build pipeline for the project.",
            "[Session llm-5, Turn 14] user: Good idea. Anyway, back to the code. If we stick with Elixir for new features, do we need to hire another specialist?",
            "[Session llm-6, Turn 3] assistant: Yes, the shipfastcore project is currently utilizing both Ruby and Elixir. You might want to mention both in the spec."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.325214385986328,
          "generation_latency_ms": 3696.821928024292,
          "total_latency_ms": 3710.1471424102783
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user asked to add 'espresso beans' to their list, what document was the user drafting?",
          "answer_type": "short_answer",
          "gold_answer": "Tech spec",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Good morning. Can you quickly add \"buy espresso beans\" to my personal shopping list?",
            "[Session llm-8, Turn 6] user: Speaking of the office, can you remind me to pick up coffee beans on my way home?",
            "[Session llm-7, Turn 1] assistant: Good morning! I've added coffee beans to your shopping list. Anything specific you need help with right now?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.520292282104492,
          "generation_latency_ms": 3487.9543781280518,
          "total_latency_ms": 3497.4746704101562
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to add espresso beans to their list before or after discussing the broken coffee machine?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Good morning. Can you quickly add \"buy espresso beans\" to my personal shopping list?",
            "[Session llm-8, Turn 6] user: Speaking of the office, can you remind me to pick up coffee beans on my way home?",
            "[Session llm-9, Turn 0] user: Hey, hope you're having a good morning. Is the coffee machine broken again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.001110076904297,
          "generation_latency_ms": 2724.813222885132,
          "total_latency_ms": 2735.814332962036
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "How does the team handle the architectural complexity of the ShipFast Core backend according to the user's discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid / Polyglot setup",
          "predicted_answer": "Managing a polyglot setup can be tricky, with friction potentially arising from maintaining the Ruby",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: I was reviewing the ShipFast Core repo earlier. Are we still maintaining that split backend stack?",
            "[Session llm-5, Turn 3] assistant: Is this regarding Charlotte and the architecture of the core project?",
            "[Session llm-7, Turn 2] user: Yeah, I was looking at the ShipFast Core repo. Are we still splitting the backend between two languages?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.537240982055664,
          "generation_latency_ms": 4088.192939758301,
          "total_latency_ms": 4100.730180740356
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's repeated shopping list requests, what type of beverage ingredient do they frequently consume?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans / Espresso beans",
          "predicted_answer": "Coffee beans",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Good morning. Can you quickly add \"buy espresso beans\" to my personal shopping list?",
            "[Session llm-7, Turn 1] assistant: Good morning! I've added coffee beans to your shopping list. Anything specific you need help with right now?",
            "[Session llm-5, Turn 12] user: No, I'll write it on the whiteboard. Oh, and did I tell you I'm thinking of switching coffee brands? The current one is too acidic."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002540588378906,
          "generation_latency_ms": 3946.244955062866,
          "total_latency_ms": 3957.247495651245
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-0",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "What is Charlotte's primary concern regarding the current architectural state of the ShipFast Core project?",
          "answer_type": "short_answer",
          "gold_answer": "Complexity",
          "predicted_answer": "Complexity",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Is this regarding Charlotte and the architecture of the core project?",
            "[Session llm-3, Turn 3] assistant: I can imagine. Is Charlotte helping you with the codebase today?",
            "[Session llm-1, Turn 2] user: Yeah, Charlotte is pushing hard for it. She wants the concurrency speed for the new features while keeping the old logic intact."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003877639770508,
          "generation_latency_ms": 4055.6886196136475,
          "total_latency_ms": 4064.692497253418
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 9,
            "accuracy": 0.5555555555555556
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 3,
            "accuracy": 0.3333333333333333
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_039.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 6,
      "ingestion_time_s": 0.7145164012908936,
      "total_evaluation_time_s": 33.781694173812866,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the name of the individual the user intends to contact regarding the hiring plan?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: I've set a reminder for 12:00 PM to message your cofounder about the hiring plan.",
            "[Session llm-5, Turn 4] user: Okay, that makes sense. I was confused by the different file extensions. Remind me to ping Olivia about the hiring plan at noon.",
            "[Session llm-7, Turn 9] assistant: It should be the standard one: the organization name, the current year, and an exclamation mark at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00070571899414,
          "generation_latency_ms": 2800.5313873291016,
          "total_latency_ms": 2810.5320930480957
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the construction rule provided by the assistant, what is the wifi password for the organization for the year 2024?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "SHIPFAST2024!",
          "retrieved_context": [
            "[Session llm-2, Turn 2] user: Yeah, actually. My phone forgot the office network credentials. What's the pass for the main org wifi again?",
            "[Session llm-1, Turn 6] user: Not yet. I just need to get online on my phone. What's the wifi password here again?",
            "[Session llm-7, Turn 8] user: Speaking of the office, do we have a specific password for the internet? A freelancer is coming in later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.011507034301758,
          "generation_latency_ms": 4158.872365951538,
          "total_latency_ms": 4167.88387298584
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request the wifi password before or after setting a reminder about the hiring plan?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "After",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Not yet. I just need to get online on my phone. What's the wifi password here again?",
            "[Session llm-2, Turn 2] user: Yeah, actually. My phone forgot the office network credentials. What's the pass for the main org wifi again?",
            "[Session llm-7, Turn 8] user: Speaking of the office, do we have a specific password for the internet? A freelancer is coming in later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.517524719238281,
          "generation_latency_ms": 3862.9746437072754,
          "total_latency_ms": 3874.4921684265137
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "In the context of the shipfast_core project, which specific type of functionality is assigned to the Elixir services?",
          "answer_type": "short_answer",
          "gold_answer": "Real-time features",
          "predicted_answer": "real-time features",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Yes, the core project is still split between the Ruby backend and the Elixir services.",
            "[Session llm-4, Turn 3] assistant: Good luck. Just a heads up, the shipfastcore repo is a mix of Ruby and Elixir now, so you might see some different syntax.",
            "[Session llm-6, Turn 7] assistant: Yes, the project seems to be in a transition phase, moving from the older Ruby codebase to the new Elixir implementation."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.002655029296875,
          "generation_latency_ms": 3455.259084701538,
          "total_latency_ms": 3471.261739730835
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which specific sports team did the user mention watching in overtime?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 0] user: Hey, did you happen to catch the game last night?",
            "[Session llm-7, Turn 9] assistant: It should be the standard one: the organization name, the current year, and an exclamation mark at the end.",
            "[Session llm-0, Turn 7] assistant: Your cofounder updated them recently. It's the organization name, then the current year 2024, and an exclamation mark at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999202728271484,
          "generation_latency_ms": 3348.2141494750977,
          "total_latency_ms": 3359.213352203369
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What is the user's sentiment regarding the context switching required by the dual-language backend?",
          "answer_type": "short_answer",
          "gold_answer": "Draining",
          "predicted_answer": "Pain",
          "retrieved_context": [
            "[Session llm-3, Turn 9] assistant: Context switching between languages can be taxing. Since the core project relies on both frameworks, maybe take a break before switching gears.",
            "[Session llm-6, Turn 8] user: Right. It makes context switching a pain. Can you play some focus music?",
            "[Session llm-7, Turn 5] assistant: I bet. Running those two languages side-by-side in the same project is definitely a complex setup."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 15.00082015991211,
          "generation_latency_ms": 3495.4833984375,
          "total_latency_ms": 3510.484218597412
        }
      ],
      "summary": {
        "overall_accuracy": 0.8333333333333334,
        "total_score": 5,
        "count": 6,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 5,
            "accuracy": 0.8
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\simple_test_set\\benchmark_nebula_logistics_042.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 10,
      "num_probes": 12,
      "ingestion_time_s": 0.7243218421936035,
      "total_evaluation_time_s": 71.16534447669983,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: Done. I've set a reminder for 12:00 PM to have lunch with your cofounder. Anything specific on the agenda?",
            "[Session llm-3, Turn 9] assistant: I haven't seen an update from your cofounder regarding the deployment times yet.",
            "[Session llm-1, Turn 2] user: Awesome. By the way, my cofounder Emma was asking about the dependency analysis for the main repo."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.512590408325195,
          "generation_latency_ms": 2510.709047317505,
          "total_latency_ms": 2521.22163772583
        },
        {
          "probe_id": "world_modeling-task_state_tracking-0",
          "pillar": "world_modeling",
          "subpillar": "task_state_tracking",
          "question": "What is the current architectural state of the ShipFast core project regarding its programming languages?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid Ruby and Elixir",
          "predicted_answer": "Hybrid environment utilizing both Ruby and Elixir.",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the project details for ShipFast core, it seems a bit split; there are strong indicators for both Elixir and Ruby being used in the framework.",
            "[Session llm-5, Turn 3] assistant: There does seem to be some overlap in the ShipFast core project. I see evidence of both Elixir and Ruby being used currently.",
            "[Session llm-7, Turn 2] user: Perfect. I was just talking to my cofounder about our tech debt. Remind me what languages the core platform is built on?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.008169174194336,
          "generation_latency_ms": 3959.35320854187,
          "total_latency_ms": 3968.3613777160645
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the pattern described by the assistant, what is the specific wifi password for the user's organization?",
          "answer_type": "short_answer",
          "gold_answer": "ShipFast2024!",
          "predicted_answer": "ShipFast2024!",
          "retrieved_context": [
            "[Session llm-1, Turn 9] assistant: Do you mean the standard Wi-Fi password for the ShipFast office?",
            "[Session llm-7, Turn 7] assistant: Yes, the password for the office wifi is now the company name combined with 2024 and an exclamation point at the end.",
            "[Session llm-0, Turn 7] assistant: I believe the wireless network for the organization uses a combination of the company name, the current year, and an exclamation mark at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.520292282104492,
          "generation_latency_ms": 3704.9450874328613,
          "total_latency_ms": 3714.465379714966
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two frameworks are currently being used in the ShipFast core project?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir and Ruby",
          "predicted_answer": "Elixir and Ruby",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the project details for ShipFast core, it seems a bit split; there are strong indicators for both Elixir and Ruby being used in the framework.",
            "[Session llm-5, Turn 3] assistant: There does seem to be some overlap in the ShipFast core project. I see evidence of both Elixir and Ruby being used currently.",
            "[Session llm-1, Turn 3] assistant: Are you referring to the ShipFast Core project?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.00358772277832,
          "generation_latency_ms": 3864.56561088562,
          "total_latency_ms": 3877.5691986083984
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why does the user describe the current development environment for the main app as 'chaotic'?",
          "answer_type": "short_answer",
          "gold_answer": "Managing two different frameworks",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Thanks. I'm trying to get the dev environment set up for the main app. My cofounder mentioned we might be switching languages?",
            "[Session llm-9, Turn 2] user: Mostly the backend architecture for the core project. It's getting complicated running two different languages side-by-side.",
            "[Session llm-1, Turn 2] user: Awesome. By the way, my cofounder Emma was asking about the dependency analysis for the main repo."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.997222900390625,
          "generation_latency_ms": 4539.968252182007,
          "total_latency_ms": 4551.9654750823975
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the session where the user first asked for the guest network key, what task did they mention they were about to start?",
          "answer_type": "short_answer",
          "gold_answer": "Digging into core project",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 8] user: Thanks. Also, totally unrelated, but we have a guest coming to the office and I blanked on the guest network credentials.",
            "[Session llm-4, Turn 2] user: A bit rainy, but I made it to the ShipFast office. Can you remind me of the guest network key?",
            "[Session llm-8, Turn 2] user: Yeah, my cousin is dropping by the office later. What's the guest wifi pass for ShipFast again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 19.002914428710938,
          "generation_latency_ms": 3870.758056640625,
          "total_latency_ms": 3889.760971069336
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which technology stack is Emma explicitly advocating for regarding the core platform?",
          "answer_type": "short_answer",
          "gold_answer": "Elixir",
          "predicted_answer": "functional stuff",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Perfect. I was just talking to my cofounder about our tech debt. Remind me what languages the core platform is built on?",
            "[Session llm-4, Turn 9] assistant: Emma usually has a good reason for those architectural shifts.",
            "[Session llm-6, Turn 2] user: Yeah, went into double overtime. Anyway, I'm looking at the backend for the core platform. Are we still running a dual stack?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.002758026123047,
          "generation_latency_ms": 3193.002700805664,
          "total_latency_ms": 3201.005458831787
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What is the user's implied stance on the current dual-language architecture of the core project?",
          "answer_type": "short_answer",
          "gold_answer": "Dislikes it",
          "predicted_answer": "Negative",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Perfect. I was just talking to my cofounder about our tech debt. Remind me what languages the core platform is built on?",
            "[Session llm-9, Turn 2] user: Mostly the backend architecture for the core project. It's getting complicated running two different languages side-by-side.",
            "[Session llm-6, Turn 2] user: Yeah, went into double overtime. Anyway, I'm looking at the backend for the core platform. Are we still running a dual stack?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.897470474243164,
          "generation_latency_ms": 4805.475473403931,
          "total_latency_ms": 4815.372943878174
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Is the ShipFast core project currently using Python?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: Looking at the project details for ShipFast core, it seems a bit split; there are strong indicators for both Elixir and Ruby being used in the framework.",
            "[Session llm-5, Turn 3] assistant: There does seem to be some overlap in the ShipFast core project. I see evidence of both Elixir and Ruby being used currently.",
            "[Session llm-7, Turn 3] assistant: Looking at the ShipFast core repository, it appears to be a hybrid environment utilizing both Ruby and Elixir."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.000373840332031,
          "generation_latency_ms": 3026.4639854431152,
          "total_latency_ms": 3034.4643592834473
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific street address of the ShipFast office?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 7] assistant: It should be the standard one for the organization. Try \"ShipFast\" followed by \"2024\" and an exclamation point.",
            "[Session llm-5, Turn 9] assistant: I have the current password for the ShipFast network listed as ShipFast2024! if that helps.",
            "[Session llm-0, Turn 9] assistant: Yes, that matches the standard formatting for the ShipFast credentials I have on file."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.002208709716797,
          "generation_latency_ms": 2851.2368202209473,
          "total_latency_ms": 2860.239028930664
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-0",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the assistant's instructions, what is the formula for the guest wifi password?",
          "answer_type": "short_answer",
          "gold_answer": "Company name capitalized, year, exclamation",
          "predicted_answer": "company name, the current year, and an exclamation mark at the end",
          "retrieved_context": [
            "[Session llm-8, Turn 2] user: Yeah, my cousin is dropping by the office later. What's the guest wifi pass for ShipFast again?",
            "[Session llm-1, Turn 9] assistant: Do you mean the standard Wi-Fi password for the ShipFast office?",
            "[Session llm-7, Turn 7] assistant: Yes, the password for the office wifi is now the company name combined with 2024 and an exclamation point at the end."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.004592895507812,
          "generation_latency_ms": 4492.619276046753,
          "total_latency_ms": 4501.623868942261
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What lesson has been observed regarding the management of the ShipFast core project's current stack?",
          "answer_type": "short_answer",
          "gold_answer": "Polyglot stacks are complicated",
          "predicted_answer": "Managing a polyglot stack can be tough.",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: Are you referring to the ShipFast Core project?",
            "[Session llm-0, Turn 3] assistant: Looking at the project details for ShipFast core, it seems a bit split; there are strong indicators for both Elixir and Ruby being used in the framework.",
            "[Session llm-5, Turn 3] assistant: There does seem to be some overlap in the ShipFast core project. I see evidence of both Elixir and Ruby being used currently."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.517929077148438,
          "generation_latency_ms": 4452.461957931519,
          "total_latency_ms": 4464.979887008667
        }
      ],
      "summary": {
        "overall_accuracy": 0.75,
        "total_score": 9,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 10,
            "accuracy": 0.7
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    }
  ]
}