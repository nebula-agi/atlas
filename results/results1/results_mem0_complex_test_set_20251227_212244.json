{
  "generated_at": "2025-12-27T22:35:51.359404",
  "adapter_type": "Mem0Adapter",
  "num_benchmarks": 50,
  "total_probes": 503,
  "overall_summary": {
    "overall_accuracy": 0.4254473161033797,
    "total_score": 214,
    "count": 503,
    "by_answer_type": {
      "short_answer": {
        "total": 131,
        "count": 408,
        "accuracy": 0.32107843137254904
      },
      "boolean": {
        "total": 13,
        "count": 18,
        "accuracy": 0.7222222222222222
      },
      "abstain": {
        "total": 70,
        "count": 73,
        "accuracy": 0.958904109589041
      },
      "verbatim": {
        "total": 0,
        "count": 2,
        "accuracy": 0.0
      },
      "generation": {
        "total": 0,
        "count": 2,
        "accuracy": 0.0
      }
    },
    "by_pillar": {
      "world_modeling": {
        "total": 39,
        "count": 85,
        "accuracy": 0.4588235294117647
      },
      "declarative_reasoning": {
        "total": 37,
        "count": 95,
        "accuracy": 0.3894736842105263
      },
      "temporal_episodic": {
        "total": 11,
        "count": 83,
        "accuracy": 0.13253012048192772
      },
      "preference_learning": {
        "total": 27,
        "count": 75,
        "accuracy": 0.36
      },
      "knowledge_boundaries": {
        "total": 84,
        "count": 90,
        "accuracy": 0.9333333333333333
      },
      "procedural_knowledge": {
        "total": 16,
        "count": 75,
        "accuracy": 0.21333333333333335
      }
    }
  },
  "stored_memories": [],
  "benchmark_summaries": [
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_001.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 75.25742769241333
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_002.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 74.98111200332642
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_003.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 73.03117108345032
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_004.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 72.434317111969
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_005.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 70.42613768577576
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_006.json",
      "accuracy": 0.36363636363636365,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "eval_time_s": 77.17189717292786
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_007.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 84.36707592010498
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_008.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 89.25054979324341
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_009.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 92.35550689697266
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_010.json",
      "accuracy": 0.16666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 92.13600373268127
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_011.json",
      "accuracy": 0.14285714285714285,
      "num_probes": 7,
      "ingestion_time_s": 0.0,
      "eval_time_s": 62.20882606506348
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_012.json",
      "accuracy": 1.0,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 15.369293689727783
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_013.json",
      "accuracy": 1.0,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 14.52296233177185
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_014.json",
      "accuracy": 1.0,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 16.497376918792725
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_015.json",
      "accuracy": 0.45454545454545453,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "eval_time_s": 94.63795375823975
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_016.json",
      "accuracy": 0.46153846153846156,
      "num_probes": 13,
      "ingestion_time_s": 0.0,
      "eval_time_s": 116.24353814125061
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_017.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 106.60520887374878
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_018.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 108.47590041160583
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_019.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 110.89736127853394
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_020.json",
      "accuracy": 0.16666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 116.23591661453247
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_021.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 122.38992667198181
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_022.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 139.95926094055176
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_023.json",
      "accuracy": 0.5,
      "num_probes": 8,
      "ingestion_time_s": 0.0,
      "eval_time_s": 92.0879282951355
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_024.json",
      "accuracy": 0.4,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "eval_time_s": 118.97123885154724
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_025.json",
      "accuracy": 0.6,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "eval_time_s": 119.94667434692383
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_026.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 171.83021521568298
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_027.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 25.18427562713623
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_028.json",
      "accuracy": 0.16666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 181.18860960006714
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_029.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 555.3938343524933
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_030.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 14.662556886672974
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_031.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 13.766170501708984
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_032.json",
      "accuracy": 0.5,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "eval_time_s": 66.347971200943
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_033.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 81.76580262184143
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_034.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 74.97472882270813
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_035.json",
      "accuracy": 0.3,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "eval_time_s": 62.300779581069946
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_036.json",
      "accuracy": 0.5454545454545454,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "eval_time_s": 64.98711848258972
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_037.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 72.76232862472534
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_038.json",
      "accuracy": 0.08333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 76.8669753074646
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_039.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "eval_time_s": 12.77497148513794
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_040.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 73.06232929229736
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_041.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 73.54803442955017
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_042.json",
      "accuracy": 0.4444444444444444,
      "num_probes": 9,
      "ingestion_time_s": 0.0,
      "eval_time_s": 54.74913930892944
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_043.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 71.15589761734009
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_044.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 73.82456564903259
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_045.json",
      "accuracy": 0.6,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "eval_time_s": 58.631990909576416
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_046.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 70.81209754943848
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_047.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 74.79774308204651
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_048.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 74.37974381446838
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_049.json",
      "accuracy": 0.16666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "eval_time_s": 73.52783918380737
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_050.json",
      "accuracy": 0.4444444444444444,
      "num_probes": 9,
      "ingestion_time_s": 0.0,
      "eval_time_s": 53.007657289505005
    }
  ],
  "detailed_results": [
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_001.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 75.25742769241333,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who deployed the analytics engine situated on Google Cloud Platform?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas deployed AnalyticsEngine2 elsewhere for ML tools, which is implied to be the Google Cloud Platform instance.",
          "retrieved_context": [
            "Alex wants to know where the analytics engines are hosted",
            "Alex asks if all analytics engines are running on Amazon infrastructure",
            "User is uncertain if there is a rogue engine on Google Cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 548.4557151794434,
          "generation_latency_ms": 3518.9998149871826,
          "total_latency_ms": 4067.455530166626
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Data Scientist",
          "predicted_answer": "The user's job involves getting datasets ready, suggesting a role like Data Scientist or Data Analyst.",
          "retrieved_context": [
            "User's cat's name is Luna.",
            "User refers to cat Luna.",
            "Will's cat Luna was sick last week"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 642.8108215332031,
          "generation_latency_ms": 5015.1190757751465,
          "total_latency_ms": 5657.92989730835
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service that is currently reporting conflicting version numbers?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Microsoft Cloud (Azure)",
          "retrieved_context": [
            "Alex is checking status of main database on Microsoft cloud and sees weird version numbers in logs.",
            "User is uncertain if there is a rogue engine on Google Cloud.",
            "Alex asks if the main database still shows a version conflict"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 758.3534717559814,
          "generation_latency_ms": 3400.683879852295,
          "total_latency_ms": 4159.037351608276
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What specific versions are currently being flagged by the primary database according to the latest logs and config?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "Alex is checking status of main database on Microsoft cloud and sees weird version numbers in logs.",
            "User sees conflicting logs for PrimaryDB7.",
            "User sees version conflicts on main database, with logs for beta release v2.0.0-beta"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 616.8358325958252,
          "generation_latency_ms": 3762.143850326538,
          "total_latency_ms": 4378.979682922363
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation about consolidating analytics services, what personal appointment was the user reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist appointment",
          "predicted_answer": "Dentist appointment",
          "retrieved_context": [
            "User thanks reminder for dentist appointment",
            "User wants help preparing for Nebula Corp all-hands meeting later.",
            "User thinks analytics pipeline is acting up"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 714.7319316864014,
          "generation_latency_ms": 3983.96635055542,
          "total_latency_ms": 4698.698282241821
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Was the reminder to call mom set for 5 PM before or after the user discussed the CEO's memo?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex wants a reminder to call mom around 6 PM tonight.",
            "User wants to call mom at 5 PM today.",
            "Alex wants a reminder to call mom at lunch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 574.9475955963135,
          "generation_latency_ms": 2893.8193321228027,
          "total_latency_ms": 3468.766927719116
        },
        {
          "probe_id": "preference_learning-preference_drift-0",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "What is the most recently requested time for the user's reminder to call their mom?",
          "answer_type": "short_answer",
          "gold_answer": "6 PM",
          "predicted_answer": "5 PM today",
          "retrieved_context": [
            "User wants to call mom at 5 PM today.",
            "User thanks reminder for dentist appointment",
            "Alex wants a reminder to call mom around 6 PM tonight."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 543.2999134063721,
          "generation_latency_ms": 3112.431287765503,
          "total_latency_ms": 3655.731201171875
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Did the user specify a particular topic to discuss during the call with their mom?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User asks if he has meeting with cofounder this afternoon",
            "User wants to call mom at 5 PM today.",
            "Charlotte was asking about our cloud spread earlier."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 864.412784576416,
          "generation_latency_ms": 2551.410913467407,
          "total_latency_ms": 3415.8236980438232
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version is the AnalyticsEngine2 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex wants to know where the analytics engines are hosted",
            "User wants list of analytics engines currently in Amazon environment",
            "User recalls Lucas deployed AnalyticsEngine2 elsewhere for ML tools"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 429.1353225708008,
          "generation_latency_ms": 2520.4286575317383,
          "total_latency_ms": 2949.563980102539
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Product Designer's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cat's name is Luna.",
            "User's cat is huge now and is trying to get datasets ready before Noah needs them for the new product interface designs.",
            "User's cofounder wants an update on product designs Noah is drafting."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 397.1872329711914,
          "generation_latency_ms": 3248.220443725586,
          "total_latency_ms": 3645.4076766967773
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant determine the version conflict on the Azure-hosted database?",
          "answer_type": "short_answer",
          "gold_answer": "Comparing logs and config",
          "predicted_answer": "The assistant (Alex/User) determined the version conflict by checking the status of the main database on the Microsoft cloud and seeing weird version numbers in the logs, specifically logs for beta release v2.0.0-beta.",
          "retrieved_context": [
            "Alex notes Lucas mentioned a version mismatch on Azure database.",
            "Alex asks if the main database still shows a version conflict",
            "Primary database on Azure is stable on version 1.2.0 but 2.0.0 beta is acting shaky."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 739.6035194396973,
          "generation_latency_ms": 3511.988639831543,
          "total_latency_ms": 4251.59215927124
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What technical task is the DevOps lead currently struggling with?",
          "answer_type": "short_answer",
          "gold_answer": "Azure deployment",
          "predicted_answer": "Untangling a database version conflict on the Microsoft cloud.",
          "retrieved_context": [
            "Lucas is a DevOps engineer responsible for fixing pipelines to prevent prod database disruptions and handles deployment pipelines",
            "User will have Lucas investigate why the beta is running alongside the stable release and look into consolidating the analytics engines, and Lucas is the best DevOps guy the user has.",
            "User told DevOps guy not to push the beta branch yet"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 521.1310386657715,
          "generation_latency_ms": 4204.127073287964,
          "total_latency_ms": 4725.258111953735
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_002.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 74.98111200332642,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the person referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User thinks Noah was responsible for beta code.",
            "User's cofounder loves redundancy",
            "Alex is the user."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 794.152021408081,
          "generation_latency_ms": 3565.3350353240967,
          "total_latency_ms": 4359.487056732178
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which DevOps team member is associated with a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Noah",
          "retrieved_context": [
            "User wants to know which DevOps engineer has a cat",
            "Noah has a cat named Luna",
            "User says Noah's cat Luna walked on his keyboard this morning."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 612.4968528747559,
          "generation_latency_ms": 2802.9041290283203,
          "total_latency_ms": 3415.400981903076
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service currently reporting a conflict between v1.2.0 and v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google",
          "retrieved_context": [
            "User acknowledges at least one cloud is behaving.",
            "User is looking into a deployment issue.",
            "User hears reports of a version mismatch on Google service highlighted."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 399.3105888366699,
          "generation_latency_ms": 3584.8793983459473,
          "total_latency_ms": 3984.189987182617
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "What two specific versions are simultaneously reported as active on AuthService7?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and unstable 2.0 beta",
          "retrieved_context": [
            "User wants deployment status of AuthService7",
            "User deployed a beta version of AuthService7",
            "User asks whether the seventh auth service is running stable version or testing new version"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 651.0426998138428,
          "generation_latency_ms": 3236.9258403778076,
          "total_latency_ms": 3887.9685401916504
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event involving a pet did the user speculate might have caused the canary deployment issues?",
          "answer_type": "short_answer",
          "gold_answer": "Luna walking on keyboard",
          "predicted_answer": "Noah's cat walking across the keyboard last week causing a typo.",
          "retrieved_context": [
            "User believes the deployment is botched and thinks the situation is a messy canary release.",
            "Alex started a canary deployment yesterday and believes it did not clean up properly.",
            "User thinks Noah is messing with a canary deployment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 570.1708793640137,
          "generation_latency_ms": 4790.70520401001,
          "total_latency_ms": 5360.876083374023
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "Immediately after mentioning the strategy session with Charlotte, what personal task did the user request a reminder for?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User had a long strategy session with Charlotte",
            "User will have Charlotte follow up with Emma during the executive sync",
            "Alex wants a reminder in an hour to reply to his cofounder's email about the quarterly review"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 637.2134685516357,
          "generation_latency_ms": 4580.537557601929,
          "total_latency_ms": 5217.751026153564
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Emma's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Emma is the Frontend Lead",
            "User intends to check with Emma before the meeting",
            "User asks who managed config for load balancer; asks if it was Emma."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 566.9450759887695,
          "generation_latency_ms": 3270.9269523620605,
          "total_latency_ms": 3837.87202835083
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "On which cloud platform is the 'PaymentGateway' service hosted?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's main authentication service is on Google's cloud.",
            "User asks if AuthService7 is on Google Cloud.",
            "User is looking at the Google Cloud dashboard"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 547.7068424224854,
          "generation_latency_ms": 2339.597225189209,
          "total_latency_ms": 2887.3040676116943
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "At what time does the user repeatedly request to be reminded to pick up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "5 PM",
          "predicted_answer": "5 PM",
          "retrieved_context": [
            "User wants a reminder to pick up dry cleaning on the way home",
            "User wants a reminder to pick up dry cleaning at 5 PM.",
            "User needs to pick up dry cleaning at 5 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 451.24244689941406,
          "generation_latency_ms": 3071.2594985961914,
          "total_latency_ms": 3522.5019454956055
        },
        {
          "probe_id": "preference_learning-preference_scope-1",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Has the user expressed a preference for which cloud provider to use for new services?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User says they are using every provider under the sun.",
            "User is asking about the Azure environment.",
            "User acknowledges at least one cloud is behaving."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 370.4044818878174,
          "generation_latency_ms": 4381.327867507935,
          "total_latency_ms": 4751.732349395752
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What administrative action did the assistant take to ensure the version discrepancy was addressed during the user's sync with Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Added a note",
          "predicted_answer": "Unknown. The context does not mention any administrative action taken by an assistant to address a version discrepancy during the user's sync with Charlotte.",
          "retrieved_context": [
            "User will have Charlotte follow up with Emma during the executive sync",
            "User will update Charlotte on the situation",
            "User thinks Noah should have cleaned up the version conflict."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 760.5900764465332,
          "generation_latency_ms": 3896.5799808502197,
          "total_latency_ms": 4657.170057296753
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Which two team members did the assistant suggest contacting to handle an infrastructure rollback?",
          "answer_type": "short_answer",
          "gold_answer": "Ava or Noah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User plans to check in with infrastructure team.",
            "User needs to get the DevOps team on this immediately",
            "User wants a quick sanity check on infrastructure before board meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 638.2951736450195,
          "generation_latency_ms": 3904.900550842285,
          "total_latency_ms": 4543.195724487305
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 9,
            "accuracy": 0.2222222222222222
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_003.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 73.03117108345032,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "User is CTO",
            "User's cofounder asked about the cloud footprint.",
            "User's cofounder asked about cloud spend"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 695.9028244018555,
          "generation_latency_ms": 2847.2886085510254,
          "total_latency_ms": 3543.191432952881
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which employee is associated with the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "User has a cat named Luna.",
            "Luna was worried about the vet visit.",
            "User says Luna is great."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 346.9998836517334,
          "generation_latency_ms": 3670.462131500244,
          "total_latency_ms": 4017.4620151519775
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Is the service currently running the v2.0.0-beta version hosted on the same cloud provider as LoadBalancer2?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if service is in same environment as load balancer.",
            "User is asking about traffic distribution and if LoadBalancer2 is handling split okay.",
            "User states that most services are on Microsoft's cloud, except one load balancer."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 699.5806694030762,
          "generation_latency_ms": 3983.473300933838,
          "total_latency_ms": 4683.053970336914
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What exact phrase did the user use to describe their low energy level in Session 23?",
          "answer_type": "verbatim",
          "gold_answer": "I am dragging today",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User feels they are running on fumes today.",
            "User is almost out of coffee beans.",
            "User finds split latency annoying."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 929.7738075256348,
          "generation_latency_ms": 2837.965488433838,
          "total_latency_ms": 3767.7392959594727
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the request to text Ava occur before or after the discussion about the Azure bill?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Before.",
          "retrieved_context": [
            "User wants a reminder to text Ava later.",
            "User reports that the Azure bill seems high this month",
            "User wants to text Ava at 5 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 807.1799278259277,
          "generation_latency_ms": 3109.830379486084,
          "total_latency_ms": 3917.0103073120117
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific time slot was secured for the meeting with the CEO regarding the data team?",
          "answer_type": "short_answer",
          "gold_answer": "2 PM",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "The CEO is worried about the data science team's resources.",
            "User will tell the data team himself during the standup",
            "User asks who else besides Sophia is on the data team."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 619.1365718841553,
          "generation_latency_ms": 3882.608413696289,
          "total_latency_ms": 4501.744985580444
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which specific service did the user explicitly instruct to exclude from the Azure cost report?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "The load balancer.",
          "retrieved_context": [
            "User wants to exclude the load balancer from the Azure report",
            "User wants assistant to generate a report for Liam regarding cross-cloud costs",
            "User wants to know if all analytics services are on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 875.0617504119873,
          "generation_latency_ms": 2613.6608123779297,
          "total_latency_ms": 3488.722562789917
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When asking for system status for the investor call, what level of detail did the user prefer?",
          "answer_type": "short_answer",
          "gold_answer": "High level",
          "predicted_answer": "The user preferred a detailed, service-level status, specifically asking to confirm the status of AuthService4 and AnalyticsEngine9.",
          "retrieved_context": [
            "User wants current system status for a stability report before an investor call.",
            "User wants to confirm the status of AuthService4 and AnalyticsEngine9.",
            "User's cofounder is asking for a stability report before an investor call."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 449.10240173339844,
          "generation_latency_ms": 3597.0981121063232,
          "total_latency_ms": 4046.2005138397217
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific version number currently running on AuthService4?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know where AuthService4 is located",
            "User wants to confirm the status of AuthService4 and AnalyticsEngine9.",
            "AuthService4 has been acting up"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 660.1686477661133,
          "generation_latency_ms": 3513.3111476898193,
          "total_latency_ms": 4173.479795455933
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current Product Manager at Nebula?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is looking at the infrastructure for Nebula.",
            "User is requesting hosting details for main services at Nebula Corp.",
            "User confirms Ava is Product Designer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 623.0032444000244,
          "generation_latency_ms": 3087.3465538024902,
          "total_latency_ms": 3710.3497982025146
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What habit of Will's is believed to be the cause of the version conflicts on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Testing in prod",
          "predicted_answer": "Deploying beta without killing the old instance.",
          "retrieved_context": [
            "Sophia and Will are reporting weird outputs from the second analytics engine",
            "User says Sophia and Will are running heavy models on the analytics stack and they may be bottlenecking",
            "Sophia and Will are debating which build is live on AnalyticsEngine2"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 523.1955051422119,
          "generation_latency_ms": 3694.586753845215,
          "total_latency_ms": 4217.782258987427
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What logic was applied to exclude LoadBalancer2 from the Azure spend report?",
          "answer_type": "short_answer",
          "gold_answer": "Hosted on GCP",
          "predicted_answer": "Unknown. The context states the user wants to exclude \"the load balancer\" (which is on Google Cloud) from the Azure report, but it does not specify any logic applied to exclude \"LoadBalancer2\" or confirm its exclusion.",
          "retrieved_context": [
            "User wants to exclude the load balancer from the Azure report",
            "User is worried about the bill because of moving heavy lifting to Azure",
            "User reports that the Azure bill seems high this month"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 806.5531253814697,
          "generation_latency_ms": 3677.3040294647217,
          "total_latency_ms": 4483.857154846191
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 8,
            "accuracy": 0.375
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_004.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 72.434317111969,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user planning to discuss quarterly goals with?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Mia wants to chat about quarterly goals.",
            "User asks if cofounder mentioned sales meeting time.",
            "User wants to know if Lucas is closing new client deal soon"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 568.7084197998047,
          "generation_latency_ms": 3124.113082885742,
          "total_latency_ms": 3692.821502685547
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which member of the sales team is the owner of Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas",
          "retrieved_context": [
            "Luna interrupts Lucas",
            "User expresses sympathy for Luna",
            "Luna is getting massive."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 887.6714706420898,
          "generation_latency_ms": 3216.542959213257,
          "total_latency_ms": 4104.214429855347
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What cloud provider hosts the load balancer that the DevOps lead was worried about?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google's cloud (GCP)",
          "retrieved_context": [
            "User talked to DevOps guy Liam about changes to load balancer on Google's cloud",
            "Liam is checking the load balancer hosted on Google's infrastructure.",
            "Alex wants to know if PrimaryDB2 is on the same cloud provider as the load balancer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 732.0718765258789,
          "generation_latency_ms": 3537.205457687378,
          "total_latency_ms": 4269.277334213257
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-0",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the conflicting system logs, which two cloud providers are reportedly hosting CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "GCP and Azure",
          "predicted_answer": "Google Cloud and Azure.",
          "retrieved_context": [
            "User is uncertain whether the third cache layer has been migrated to Google's cloud.",
            "CacheLayer3 is set up on Google's cloud by Liam, resolving conflicting hosting data.",
            "There is confusion regarding CacheLayer3."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 554.0306568145752,
          "generation_latency_ms": 3417.264461517334,
          "total_latency_ms": 3971.295118331909
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What ongoing activity explains why CacheLayer3 is reporting two different software versions simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Update migration",
          "predicted_answer": "Unknown. The provided context states that CacheLayer3 is reporting two different versions (config shows v1.2.0 and logs show v2.0.0-beta), but does not explain why this is happening simultaneously.",
          "retrieved_context": [
            "User wants to know what version is being reported on CacheLayer3, noting config shows v1.2.0 and logs show v2.0.0-beta.",
            "Alex wants to know what version CacheLayer3 is reporting.",
            "User asks whether the issue is related to weird behavior on CacheLayer3."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 614.8622035980225,
          "generation_latency_ms": 3752.547025680542,
          "total_latency_ms": 4367.409229278564
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the strategy meeting, what specific event interrupted the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User had a long strategy sync with cofounder and sales leads.",
            "User will ask Sophia about sales meeting.",
            "User asks if cofounder mentioned sales meeting time."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1033.4436893463135,
          "generation_latency_ms": 3174.5917797088623,
          "total_latency_ms": 4208.035469055176
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user frequently consume to cope with work exhaustion?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "User is on third cup of coffee",
            "User plans to grab coffee before the board meeting.",
            "User's coffee hasn't kicked in yet."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 537.1017456054688,
          "generation_latency_ms": 3424.351453781128,
          "total_latency_ms": 3961.4531993865967
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What item did the user explicitly ask to be added to their shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to add 'buy espresso beans' to personal to-do list.",
            "User wants to know what Liam is up to.",
            "User wants a reminder to grab a double espresso in ten minutes."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 657.9358577728271,
          "generation_latency_ms": 3732.30242729187,
          "total_latency_ms": 4390.238285064697
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Backend Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Charlotte is the backend lead.",
            "User says Charlotte knows backend better than anyone.",
            "The user does not want the Backend Lead yelling at them today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 735.2163791656494,
          "generation_latency_ms": 3281.5260887145996,
          "total_latency_ms": 4016.742467880249
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the 'auth_service'?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know cloud provider of node",
            "User thought everything was moved to one provider",
            "Alex wants to know if PrimaryDB2 is on the same cloud provider as the load balancer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 536.6382598876953,
          "generation_latency_ms": 2479.4232845306396,
          "total_latency_ms": 3016.061544418335
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What two specific data sources does the assistant cite when analyzing the version conflict on CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "Response headers and heartbeats",
          "predicted_answer": "Config and logs.",
          "retrieved_context": [
            "Alex wants to know what version CacheLayer3 is reporting.",
            "Charlotte asked about versioning on CacheLayer3.",
            "User asks if CacheLayer6 has version issue."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 404.46925163269043,
          "generation_latency_ms": 2738.959312438965,
          "total_latency_ms": 3143.4285640716553
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on previous checks, what is the expected stability status of CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "Unstable",
          "predicted_answer": "Unstable, experiencing issues, and showing inconsistent metrics and versioning.",
          "retrieved_context": [
            "User wants to check deployment status of CacheLayer3 after Liam tweaked configs this morning to ensure nothing broke.",
            "Alex wants to know what version CacheLayer3 is reporting.",
            "There is confusion regarding CacheLayer3."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 357.6524257659912,
          "generation_latency_ms": 4742.708444595337,
          "total_latency_ms": 5100.360870361328
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 10,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_005.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 70.42613768577576,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which executive is repeatedly referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "User's cofounder is breathing down their neck for the infra summary",
            "User identifies James as Sales Lead, says James joked about cloud spaghetti, and that James doesn't have to fix the mess.",
            "User's cofounder loves using the buzzword 'multi-cloud' in investor updates."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 678.2050132751465,
          "generation_latency_ms": 3193.1278705596924,
          "total_latency_ms": 3871.332883834839
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific service is co-located on the Google Cloud Platform alongside the main database?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "The fourth cache layer (CacheLayer4).",
          "retrieved_context": [
            "Alex confirms that the main database is on Google Cloud.",
            "Primary database is on Google's cloud",
            "User wants to check the primary database and the other cache on Google platform."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 681.9620132446289,
          "generation_latency_ms": 3256.082534790039,
          "total_latency_ms": 3938.044548034668
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "The Backend Lead set up a specific caching layer. Which cloud platform is that layer hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "Backend Lead wants to consolidate CacheLayer2 off AWS.",
            "Will deployed a caching layer on Amazon.",
            "Backend lead hasn't figured out why AWS instance is lagging, and the cache layer is stranded on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 684.5967769622803,
          "generation_latency_ms": 3997.39408493042,
          "total_latency_ms": 4681.9908618927
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two software versions is AuthService7 currently reporting in the logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is observing something weird with AuthService7 and asks if it is running the beta.",
            "User wants to check the version on auth service again",
            "Auth service update rollout is not finished"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 760.8680725097656,
          "generation_latency_ms": 3177.5693893432617,
          "total_latency_ms": 3938.4374618530273
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the most recent interaction, what reason was given for the Sales Lead being inactive or late?",
          "answer_type": "short_answer",
          "gold_answer": "Luna was sick",
          "predicted_answer": "Unknown. The context states the Sales Lead forgets to update CRM and has been useless with vendor calls, but does not provide a reason for this inactivity or lateness.",
          "retrieved_context": [
            "Sales lead forgets to update CRM",
            "User asks if James sent over sales figures",
            "James was supposed to help with vendor calls but has been useless"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 660.7506275177002,
          "generation_latency_ms": 4130.879163742065,
          "total_latency_ms": 4791.629791259766
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder for concert tickets before or after the reminder for the team lunch pizza?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After.",
          "retrieved_context": [
            "User wants a reminder to order pizza for team lunch at 12:00 PM.",
            "User wants a reminder to buy concert tickets at 12:00 PM.",
            "User wants reminder to message Backend Lead about sticking to v1.2.0."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 582.3545455932617,
          "generation_latency_ms": 2721.552610397339,
          "total_latency_ms": 3303.9071559906006
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When offered the option to filter the Sales Lead's distracting messages, what was the user's decision?",
          "answer_type": "short_answer",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants reminder to message Backend Lead about sticking to v1.2.0.",
            "Sales lead forgets to update CRM",
            "User asks if James sent over sales figures"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 496.2122440338135,
          "generation_latency_ms": 2790.1875972747803,
          "total_latency_ms": 3286.3998413085938
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-1",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Why did the user choose not to filter James's messages despite the distractions?",
          "answer_type": "short_answer",
          "gold_answer": "To stay in the loop",
          "predicted_answer": "The user needs critical information from James, such as quarterly numbers and sales figures.",
          "retrieved_context": [
            "User wants a reminder to message James later.",
            "User heard meowing in background during James's call",
            "User wants to ask James about quarterly numbers before he gets distracted."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 352.19264030456543,
          "generation_latency_ms": 4556.811809539795,
          "total_latency_ms": 4909.00444984436
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the 'PaymentGateway' service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking if the authentication service is hosted on Google or Amazon.",
            "User wants to know if analytics engine runs in same cloud environment as auth service",
            "User is asking about Google Cloud environment"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 610.3997230529785,
          "generation_latency_ms": 2213.56201171875,
          "total_latency_ms": 2823.9617347717285
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the name of the Backend Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Will is the backend lead.",
            "User wants to ping Will, who is the Backend Lead, because Will may have deployed a canary build without informing the user.",
            "Sales lead talks about cat Luna"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 456.2358856201172,
          "generation_latency_ms": 3085.9382152557373,
          "total_latency_ms": 3542.1741008758545
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What recurring weekly event does the user repeatedly ask the assistant to help prepare for?",
          "answer_type": "short_answer",
          "gold_answer": "Sync with Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to grab coffee in ten minutes",
            "User wants reminder to pick up dry cleaning tomorrow morning",
            "User wants a reminder to buy concert tickets at 12:00 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 423.8431453704834,
          "generation_latency_ms": 3484.3804836273193,
          "total_latency_ms": 3908.2236289978027
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Is the service managed by the Backend Lead hosted on the same platform as the Analytics Engine?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know if analytics engine is affected.",
            "Backend Lead thinks analytics engine is choking.",
            "User wants to know if analytics engine runs in same cloud environment as auth service"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 694.652795791626,
          "generation_latency_ms": 3820.272445678711,
          "total_latency_ms": 4514.925241470337
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 3,
            "accuracy": 0.3333333333333333
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_006.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 77.17189717292786,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'Backend Lead' investigating the database discrepancies?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia",
          "retrieved_context": [
            "User wants to know who the backend lead is",
            "User needs to get backend lead to investigate immediately.",
            "User says the backend lead is testing in production."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 635.4360580444336,
          "generation_latency_ms": 3558.1438541412354,
          "total_latency_ms": 4193.579912185669
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the organizational role of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "James is part of the sales team.",
          "retrieved_context": [
            "User says Luna the cat is basically the mascot of the sales team and the cat is basically the team mascot.",
            "James has a cat named Luna.",
            "James sent User a video of his cat Luna chasing a laser pointer during a client call"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 663.184404373169,
          "generation_latency_ms": 3773.9078998565674,
          "total_latency_ms": 4437.092304229736
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service associated with the CEO's login errors?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Unknown. Configuration files point to both AWS and Azure for PrimaryDB8, which",
          "retrieved_context": [
            "User wants to check which cloud provider the primary database is on",
            "User's cofounder had trouble logging in earlier.",
            "Sophia is investigating an anomaly with the primary database, which she believes is running in Oregon on Amazon, but the billing tag indicates Azure"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 651.9145965576172,
          "generation_latency_ms": 4027.8899669647217,
          "total_latency_ms": 4679.804563522339
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Is the primary database currently confirmed to be hosted in a single environment?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, it is not currently confirmed to be hosted in a single environment. Documentation is contradictory, configuration files point to both AWS and Azure, and there's an anomaly with billing tags.",
          "retrieved_context": [
            "User has a primary database",
            "User wants to check which cloud provider the primary database is on",
            "User is referring to the primary database."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 662.5931262969971,
          "generation_latency_ms": 3972.580909729004,
          "total_latency_ms": 4635.174036026001
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific activity caused the user's lack of sleep before the auth service check?",
          "answer_type": "short_answer",
          "gold_answer": "quarterly strategy call",
          "predicted_answer": "Unknown. The context states the user did not sleep much but does not specify the activity that caused it.",
          "retrieved_context": [
            "User believes the auth service was frozen at version 1.2.0",
            "User's cofounder had trouble logging in earlier.",
            "User wants to know if new Auth service was deployed"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 523.8220691680908,
          "generation_latency_ms": 2951.209306716919,
          "total_latency_ms": 3475.0313758850098
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What item did the user ask to be reminded to buy while checking the load balancer logs?",
          "answer_type": "short_answer",
          "gold_answer": "coffee beans",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to pull up logs for the load balancer.",
            "User wants to check if Load Balancer is seeing it",
            "The load balancer is acting up."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 874.4449615478516,
          "generation_latency_ms": 2649.9693393707275,
          "total_latency_ms": 3524.414300918579
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When reviewing the CEO's agendas, which department's representation does the user explicitly prioritize?",
          "answer_type": "short_answer",
          "gold_answer": "engineering",
          "predicted_answer": "Sales, UI/Product/Design, Finance/Revenue, and Engineering/Backend.",
          "retrieved_context": [
            "User wants a report for the CEO",
            "User is CEO",
            "User wants to know who is handling sales leads this week"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 597.6707935333252,
          "generation_latency_ms": 4558.872222900391,
          "total_latency_ms": 5156.543016433716
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's mechanic?",
          "answer_type": "short_answer",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is running on fumes",
            "User wants engineering side represented in all-hands.",
            "User is CEO"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 609.4911098480225,
          "generation_latency_ms": 3003.790855407715,
          "total_latency_ms": 3613.2819652557373
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the Analytics Engine currently running?",
          "answer_type": "short_answer",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know if analytics engine is on same stack.",
            "User wants a reminder to check the analytics engine throughput when User gets back.",
            "User's cofounder was asking about the analytics engine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 599.4162559509277,
          "generation_latency_ms": 3528.947114944458,
          "total_latency_ms": 4128.363370895386
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-9",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration is suspected to be the cause of the cofounder's login errors?",
          "answer_type": "short_answer",
          "gold_answer": "beta version traffic",
          "predicted_answer": "An upgraded authentication module or new login designs triggering the load balancer.",
          "retrieved_context": [
            "User's cofounder had trouble logging in earlier.",
            "User says Sophia might have upgraded the authentication module.",
            "User will involve Mia and Elijah to investigate whether new login designs are triggering the load balancer"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 619.0016269683838,
          "generation_latency_ms": 5319.063425064087,
          "total_latency_ms": 5938.065052032471
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What action did the user plan to take regarding the product roadmap agenda sent by the CEO?",
          "answer_type": "generation",
          "gold_answer": "review for engineering representation",
          "predicted_answer": "Unknown. The context states the user asked if Will sent over an agenda, but it does not specify if this was a product roadmap agenda or what action the user planned to take regarding it.",
          "retrieved_context": [
            "User intends to check in with Mia regarding user interface updates.",
            "User's cofounder wants the new feature rollout accelerated.",
            "User asked if Will sent over agenda."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 818.932056427002,
          "generation_latency_ms": 5194.114685058594,
          "total_latency_ms": 6013.046741485596
        }
      ],
      "summary": {
        "overall_accuracy": 0.36363636363636365,
        "total_score": 4,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_007.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 84.36707592010498,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that the user reviewed the infrastructure bill with?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cofounder is asking for an update on the infrastructure migration",
            "User's cofounder wants a breakdown of infrastructure costs.",
            "User's cofounder wants a full infrastructure audit before the board meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 760.0295543670654,
          "generation_latency_ms": 4587.486982345581,
          "total_latency_ms": 5347.5165367126465
        },
        {
          "probe_id": "world_modeling-category_membership-2",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Which employees require access to the beta database for their data science models?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas and James",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User needs data science guys to have access to the database for their models.",
            "User has a feeling Lucas and James will want to query GCP data for their models later.",
            "User wants to see which databases are currently sitting in the Microsoft cloud environment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 664.2055511474609,
          "generation_latency_ms": 4382.227897644043,
          "total_latency_ms": 5046.433448791504
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the database that is running the v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User wants to see which databases are currently sitting in the Microsoft cloud environment.",
            "User is running beta in prod and experiencing instability, unsure whether v1.2.0 or v2.0.0-beta is deployed.",
            "User wants logs for the Azure database running the beta version"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 674.2403507232666,
          "generation_latency_ms": 4071.6018676757812,
          "total_latency_ms": 4745.842218399048
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite earlier ambiguous logs, what version was PrimaryDB9 confirmed to be running during the final infrastructure audit?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to check if upgrade went through for PrimaryDB9.",
            "User will manually check upgrade of PrimaryDB9.",
            "User states that PrimaryDB9 should be strictly version v1.2.0, not beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 934.3986511230469,
          "generation_latency_ms": 4186.750650405884,
          "total_latency_ms": 5121.149301528931
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific distraction caused the Product Designer to fail to deliver the UI assets on time?",
          "answer_type": "short_answer",
          "gold_answer": "His cat Luna",
          "predicted_answer": "Luna, Oliver's cat, knocked a glass of water onto his keyboard.",
          "retrieved_context": [
            "Performance needs to be tight and user needs to fix issue fast",
            "Oliver sent new UI mockups this morning and was showing them to the user, apologizing for delay because Luna knocked a glass of water onto his keyboard",
            "Liam gets nervous when the demo environment lags"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 594.0234661102295,
          "generation_latency_ms": 4355.465650558472,
          "total_latency_ms": 4949.489116668701
        },
        {
          "probe_id": "temporal_episodic-sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder to buy cat treats before or after the final infrastructure audit?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context does not mention a \"final infrastructure audit.\"",
          "retrieved_context": [
            "User wants reminder to buy lottery ticket later.",
            "User wants reminder to pick up treats for Oliver's cat on way home.",
            "User wants to ask about cat"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 570.195198059082,
          "generation_latency_ms": 3281.6431522369385,
          "total_latency_ms": 3851.8383502960205
        },
        {
          "probe_id": "preference_learning-induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What specific beverage does the user consistently consume before starting their morning work?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Unknown. The context mentions the user drinks espresso and coffee, but doesn't specify a consistent beverage consumed *before starting morning work*.",
          "retrieved_context": [
            "Alex wants a reminder to grab an extra espresso before standup",
            "User is on their third espresso.",
            "User wants a reminder to pick up coffee beans later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 743.2310581207275,
          "generation_latency_ms": 4734.257936477661,
          "total_latency_ms": 5477.488994598389
        },
        {
          "probe_id": "preference_learning-explicit-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to rollback the Azure database to v1.2.0 immediately following the audit?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown. The context states the user wants to keep the main Azure database on beta until alerts fire, and mentions fixing a version mismatch on DB9, but does not specify a rollback to v1.2.0 or mention an audit.",
          "retrieved_context": [
            "User wants to keep the main Azure database on beta until alerts fire.",
            "User wants to know if the version mismatch on the Azure DB is critical.",
            "User asks about the version of the Azure database (PrimaryDB9)."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 766.9248580932617,
          "generation_latency_ms": 3911.144971847534,
          "total_latency_ms": 4678.069829940796
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the current running version of the primary database hosted on GCP?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know what is currently hosted on GCP.",
            "User wants to check PrimaryDB6 on GCP for cross-cloud latency issues.",
            "User wants to see which databases are currently sitting in the Microsoft cloud environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 690.0460720062256,
          "generation_latency_ms": 4080.3048610687256,
          "total_latency_ms": 4770.350933074951
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Liam is the Sales Lead.",
            "User asks if sales lead is in office today.",
            "User owes sales lead lunch."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 607.6359748840332,
          "generation_latency_ms": 3441.6799545288086,
          "total_latency_ms": 4049.315929412842
        },
        {
          "probe_id": "procedural_knowledge-task_tracking-11",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What shopping task did the user ask the assistant to add to their list regarding the Product Designer's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Buy treats for Luna",
          "predicted_answer": "Pick up treats for Oliver's cat.",
          "retrieved_context": [
            "User wants to add \"buy milk\" to personal grocery list.",
            "User wants to ask about cat",
            "User wants to add a task to ask Oliver if he's bringing his cat to the office party."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 651.5936851501465,
          "generation_latency_ms": 3067.181348800659,
          "total_latency_ms": 3718.7750339508057
        },
        {
          "probe_id": "procedural_knowledge-task_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What reminder did the user set for themselves before the standup meeting in the first recorded session?",
          "answer_type": "short_answer",
          "gold_answer": "Grab an extra espresso",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to grab a sandwich before the 1 PM meeting.",
            "Alex wants a reminder to grab an extra espresso before standup",
            "User wants a reminder to grab coffee in 15 minutes."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 647.3233699798584,
          "generation_latency_ms": 3125.6799697875977,
          "total_latency_ms": 3773.003339767456
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 9,
            "accuracy": 0.1111111111111111
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_008.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 89.25054979324341,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the name of the individual referred to as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "User's cofounder is Elijah",
            "User plans to catch cofounder later.",
            "User's cofounder wants to consolidate everything eventually."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 565.0608539581299,
          "generation_latency_ms": 3644.317150115967,
          "total_latency_ms": 4209.378004074097
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which analytics service is hosted on a different cloud provider than the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine9",
          "predicted_answer": "The analytics service that received the latency alert.",
          "retrieved_context": [
            "User asks if analytics engines are co-located with the primary database.",
            "User got a latency alert for analytics service hosted on Microsoft's cloud, not AWS",
            "AnalyticsEngine9 is billed through a different provider than the rest of the stack."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 678.2071590423584,
          "generation_latency_ms": 4457.96537399292,
          "total_latency_ms": 5136.172533035278
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud platform is the database instance that is signalling version v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Amazon",
          "retrieved_context": [
            "Database is reported as running version 2.0.0 beta.",
            "User asks if the primary database is hosted in Amazon environment.",
            "User is asking if the beta database version has gone live."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 601.4127731323242,
          "generation_latency_ms": 4962.372064590454,
          "total_latency_ms": 5563.784837722778
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What two specific version numbers are currently conflicting in the primary database logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown. The context states there are \"two different version strings\" and asks if the primary database",
          "retrieved_context": [
            "User believes main database version drifted during last deploy and suspects a version drift on the primary database.",
            "User sees conflicting reports on database versioning and cofounder is asking why two different version strings are reported.",
            "User is reviewing logs for PrimaryDB1 and notices an issue."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 650.4089832305908,
          "generation_latency_ms": 4399.386167526245,
          "total_latency_ms": 5049.795150756836
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user mention fixing a leaky faucet before or after the discussion about Azure latency?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User got a latency alert for analytics service hosted on Microsoft's cloud, not AWS",
            "User is dealing with cross-cloud latency.",
            "User believes latency should be low."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 831.7224979400635,
          "generation_latency_ms": 2949.96976852417,
          "total_latency_ms": 3781.6922664642334
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What domestic task did the user mention as the reason for their weekend feeling too short?",
          "answer_type": "short_answer",
          "gold_answer": "Fixing a leaky faucet",
          "predicted_answer": "Fixing a leaky faucet.",
          "retrieved_context": [
            "User spent half of the weekend fixing a leaky faucet.",
            "User needs another coffee before tackling the task.",
            "User did not sleep well."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 598.9079475402832,
          "generation_latency_ms": 3351.6335487365723,
          "total_latency_ms": 3950.5414962768555
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When requesting a reminder for coffee beans, what specific variety did the user specify?",
          "answer_type": "short_answer",
          "gold_answer": "Dark roast",
          "predicted_answer": "Dark roast.",
          "retrieved_context": [
            "User wants a reminder to pick up dark roast coffee beans on the way home tonight.",
            "User wants to buy dark roast coffee beans later.",
            "User wants to buy more coffee beans later"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 597.3312854766846,
          "generation_latency_ms": 3504.9116611480713,
          "total_latency_ms": 4102.242946624756
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user characterize the billing situation resulting from the multi-cloud setup?",
          "answer_type": "short_answer",
          "gold_answer": "Fragmented",
          "predicted_answer": "The billing is split across different providers, causing confusion and discrepancies, and necessitating",
          "retrieved_context": [
            "User is involved in a multi-cloud experiment.",
            "User notes multi-cloud split",
            "User is looking at the hosting bill."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 601.3543605804443,
          "generation_latency_ms": 5273.92315864563,
          "total_latency_ms": 5875.277519226074
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know who is leading the frontend team.",
            "Alex wants to know who leads the frontend team.",
            "User wants to ping the Frontend Lead."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 619.3563938140869,
          "generation_latency_ms": 3784.294605255127,
          "total_latency_ms": 4403.650999069214
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is the Load Balancer currently reporting?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown. The context states \"Dashboard indicates version v1.2.0,\" but does not specify if this version applies to the Load Balancer.",
          "retrieved_context": [
            "User wants to know how the load balancer is holding up with the database glitch and check current traffic on the main load balancer.",
            "Alex wants to know if the cache and load balancer are on the same provider as Engine 6.",
            "LoadBalancer1 is handling traffic on the AWS side correctly."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 602.6136875152588,
          "generation_latency_ms": 4348.544359207153,
          "total_latency_ms": 4951.158046722412
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What structural factor did the assistant identify as the cause of the latency on the Azure instance?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-region calls",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User got a latency alert for analytics service hosted on Microsoft's cloud, not AWS",
            "User is dealing with cross-cloud latency.",
            "User says frontend is trying to query analytics engine hosted on Azure, engine nine"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 670.2430248260498,
          "generation_latency_ms": 3951.4338970184326,
          "total_latency_ms": 4621.676921844482
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What action did the assistant suggest to address the latency issues observed by the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Draft migration plan",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex's cofounder was complaining about latency.",
            "Alex needs to coordinate with the frontend team about latency.",
            "User believes latency should be low."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 599.1599559783936,
          "generation_latency_ms": 4322.563171386719,
          "total_latency_ms": 4921.723127365112
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_009.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 92.35550689697266,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to by the user as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas",
          "retrieved_context": [
            "User has a cofounder",
            "User's cofounder is frantic.",
            "Lucas is cofounder."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 707.5026035308838,
          "generation_latency_ms": 3557.255268096924,
          "total_latency_ms": 4264.757871627808
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the database service that the Backend Lead named Oliver is concerned about?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "User says Oliver is managing PrimaryDB7 and AuthService1 and leading backend efforts.",
            "User acknowledges Oliver is the other backend lead",
            "User wants to know which cloud provider hosts the eighth primary db instance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 681.9255352020264,
          "generation_latency_ms": 3872.2198009490967,
          "total_latency_ms": 4554.145336151123
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Identify the team member who holds the same job title as the owner of the cat named Luna.",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Cat Luna is practically a team member",
            "Liam has a cat named Luna that walks on his keyboard",
            "User says Liam's pet is Luna"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 719.7427749633789,
          "generation_latency_ms": 4814.766883850098,
          "total_latency_ms": 5534.509658813477
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-2",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two specific version numbers are currently detected running simultaneously on the AWS database?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown. Only v1.2.0 is explicitly mentioned as a specific version number. A \"beta version\" is mentioned, but no specific number for it.",
          "retrieved_context": [
            "User is checking database status in AWS and sees weird versioning flags",
            "Liam needs to fix AWS db versioning",
            "Liam and Oliver are arguing about versioning on the AWS database."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 768.8002586364746,
          "generation_latency_ms": 5147.165536880493,
          "total_latency_ms": 5915.965795516968
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Based on the user's schedule, which task occurs earlier in the day: approving the budget report or picking up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Approving budget report",
          "predicted_answer": "Unknown. The context mentions scheduling a sync about the budget \"later,\" but does not mention approving a budget report or a specific time for it. Picking up dry cleaning is scheduled for 5 PM.",
          "retrieved_context": [
            "User might schedule a meeting after lunch.",
            "User wants a reminder to pick up dry cleaning at 5 PM",
            "User needs to grab a coffee before digging into the mess."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 590.1923179626465,
          "generation_latency_ms": 4464.498043060303,
          "total_latency_ms": 5054.690361022949
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-2",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What technical issue is identified as the cause of the latency on the PrimaryDB8 instance?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting versions",
          "predicted_answer": "Inconsistent versioning across PrimaryDB8 nodes, dashboard, and logs (half report v1.2.0, half report v2.0.0-beta; dashboard reports v1.2.0, logs show v2.0.0-beta).",
          "retrieved_context": [
            "User is checking status of PrimaryDB8.",
            "User reports latency on main database on Amazon and suspects beta version may be causing it",
            "User confirms the database is PrimaryDB8"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 579.3282985687256,
          "generation_latency_ms": 3629.6920776367188,
          "total_latency_ms": 4209.020376205444
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the beta push, do they currently prioritize rapid deployment or production stability?",
          "answer_type": "short_answer",
          "gold_answer": "Production stability",
          "predicted_answer": "Production stability.",
          "retrieved_context": [
            "User will ask cofounder Lucas about beta deployment authorization",
            "User reports latency on main database on Amazon and suspects beta version may be causing it",
            "User will check with Liam, the backend lead, about deployments."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 516.4942741394043,
          "generation_latency_ms": 4428.450584411621,
          "total_latency_ms": 4944.944858551025
        },
        {
          "probe_id": "preference_learning-explicit_preferences-2",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What beverage does the user explicitly rely on to 'wake up properly' in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "User is drinking coffee to wake up properly.",
            "User wants a reminder to drink water",
            "User needs to grab coffee first"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 857.5325012207031,
          "generation_latency_ms": 2922.988176345825,
          "total_latency_ms": 3780.5206775665283
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific running version number of the database hosted on Azure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "PrimaryDB7 is hosted on Azure.",
            "James is asking for access to an Azure-hosted database for his models.",
            "Legacy database on Azure is fine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 559.213399887085,
          "generation_latency_ms": 4065.1423931121826,
          "total_latency_ms": 4624.355792999268
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Product Designer's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User says Liam's pet is Luna",
            "Liam has a cat named Luna that walks on his keyboard",
            "User wants to double-check who is currently handling chief executive duties and who is owning product design."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 654.3209552764893,
          "generation_latency_ms": 4014.7125720977783,
          "total_latency_ms": 4669.033527374268
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific data does the user typically request to prepare for a strategy meeting with the CEO?",
          "answer_type": "short_answer",
          "gold_answer": "Latest metrics",
          "predicted_answer": "Quarterly projections and quarterly review data.",
          "retrieved_context": [
            "User needs to prepare for a strategy meeting with cofounder later",
            "User's cofounder is asking about quarterly projections.",
            "User's cofounder is the CEO and is stressing about the quarterly board meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 605.7538986206055,
          "generation_latency_ms": 4665.500640869141,
          "total_latency_ms": 5271.254539489746
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-2",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What was the user's explicit instruction regarding the deployment of the beta version to production?",
          "answer_type": "short_answer",
          "gold_answer": "Do not push",
          "predicted_answer": "Unknown. The user believes Oliver pushed the beta version by mistake, implying there was",
          "retrieved_context": [
            "User will ask cofounder Lucas about beta deployment authorization",
            "User believes Oliver pushed beta version by mistake.",
            "Liam is stressing out about the beta version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 711.6231918334961,
          "generation_latency_ms": 5098.579406738281,
          "total_latency_ms": 5810.202598571777
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 10,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_010.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 92.13600373268127,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service managed by the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants status of services running on Microsoft cloud.",
            "User wants to know if CacheLayer1 and CacheLayer3 are on the same cloud provider.",
            "User is uncertain which cloud provider CacheLayer6 is on."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 629.4076442718506,
          "generation_latency_ms": 4427.440643310547,
          "total_latency_ms": 5056.8482875823975
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the cofounder concerned about the recent spike in cloud costs?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "The cofounder.",
          "retrieved_context": [
            "User's cofounder asked why the cloud bill is fragmented.",
            "User's cofounder is asking why Azure bill spiked this month",
            "User says the multi-cloud setup is annoying."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 734.1208457946777,
          "generation_latency_ms": 3717.665433883667,
          "total_latency_ms": 4451.786279678345
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cat Luna was taken to the vet",
            "Elijah's cat Luna jumped on his desk during the standup.",
            "User reports that Luna is fine but grumpy."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 836.5509510040283,
          "generation_latency_ms": 3931.318521499634,
          "total_latency_ms": 4767.869472503662
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-1",
          "pillar": "declarative_reasoning",
          "subpillar": "task_state_tracking",
          "question": "Which two version numbers are simultaneously reported by the sixth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is looking at the dashboard for CacheLayer6 and notes that its version numbers look weird.",
            "User wants to know the current active version on CacheLayer6",
            "User is inquiring about cache layers 1, 3, and 6."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 735.18967628479,
          "generation_latency_ms": 3490.8838272094727,
          "total_latency_ms": 4226.073503494263
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific activity by the Data Scientist was identified as the cause of the Azure bill spike?",
          "answer_type": "short_answer",
          "gold_answer": "Running heavy models",
          "predicted_answer": "Running heavy models on Azure.",
          "retrieved_context": [
            "User's cofounder is asking why Azure bill spiked this month",
            "Azure bill spiked this month",
            "User's cofounder asked why the cloud bill is fragmented."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 623.7781047821045,
          "generation_latency_ms": 4487.060785293579,
          "total_latency_ms": 5110.838890075684
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user requested a ticket be created regarding the deployment manifest, who was the intended recipient?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User plans to have Elijah look at the deployment manifest.",
            "User guesses that someone deployed the beta.",
            "Backend lead convinced there is a deployment mismatch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 710.2057933807373,
          "generation_latency_ms": 3731.0080528259277,
          "total_latency_ms": 4441.213846206665
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user consistently request before morning meetings?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User needs another espresso before morning standup",
            "User needs to grab a coffee before falling asleep.",
            "User has had two espressos"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 695.4786777496338,
          "generation_latency_ms": 4130.71870803833,
          "total_latency_ms": 4826.197385787964
        },
        {
          "probe_id": "preference_learning-belief_revision-1",
          "pillar": "preference_learning",
          "subpillar": "belief_revision",
          "question": "What is the final confirmed time for the dentist appointment, overriding the initially suggested time?",
          "answer_type": "short_answer",
          "gold_answer": "3 PM",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants reminder to pick up sandwich before 1:00 PM meeting",
            "User wants a reminder to grab a latte in twenty minutes.",
            "User wants to check in on Backend Lead about a vet appointment for his cat this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 695.3325271606445,
          "generation_latency_ms": 3471.1060523986816,
          "total_latency_ms": 4166.438579559326
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is CacheLayer3 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know the current active version on CacheLayer6",
            "User is looking at the dashboard for CacheLayer6 and notes that its version numbers look weird.",
            "User asks if cache layers 1 and 3 are affected."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 611.0045909881592,
          "generation_latency_ms": 3688.356876373291,
          "total_latency_ms": 4299.36146736145
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current Project Manager for the Nebula organization?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Charlotte wants to go over quarterly projections",
            "User's cofounder wants to review quarterly roadmap",
            "User asks if Noah sent updated wireframes"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 923.5701560974121,
          "generation_latency_ms": 4416.802406311035,
          "total_latency_ms": 5340.372562408447
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Azure bill spike occur before or after the user first noticed the version conflict on CacheLayer6?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "The discussion about the Azure bill spike occurred before the user first noticed the version conflict on CacheLayer6.",
          "retrieved_context": [
            "User's cofounder is asking why Azure bill spiked this month",
            "User confirms CacheLayer3 is hosted on Azure.",
            "User wants to check the new caching layer deployed to Azure, which is running the beta version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 545.0127124786377,
          "generation_latency_ms": 4631.15930557251,
          "total_latency_ms": 5176.1720180511475
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What action by the Backend Lead likely caused the split versioning on the Azure cache instance?",
          "answer_type": "short_answer",
          "gold_answer": "Leaving canary deployment active",
          "predicted_answer": "Unknown. The context states the Backend Lead worked on the cache layer yesterday,",
          "retrieved_context": [
            "User wants to check the new caching layer deployed to Azure, which is running the beta version.",
            "User confirms CacheLayer3 is hosted on Azure.",
            "AnalyticsEngine6 is next to CacheLayer3 in Azure cluster."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 619.9607849121094,
          "generation_latency_ms": 5352.32138633728,
          "total_latency_ms": 5972.28217124939
        }
      ],
      "summary": {
        "overall_accuracy": 0.16666666666666666,
        "total_score": 2,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 10,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_011.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 7,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 62.20882606506348,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown. The context only refers to them by their role as \"cofounder.\"",
          "retrieved_context": [
            "User's cofounder is pestering about new beta.",
            "User's cofounder is breathing down User's neck about reliability today.",
            "User says cofounder will flip if sees beta software in production."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 609.9951267242432,
          "generation_latency_ms": 4972.8052616119385,
          "total_latency_ms": 5582.800388336182
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the engineer currently debugging the Amazon infrastructure?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Mia is currently debugging the Amazon infrastructure.",
            "User wants to check health of Amazon setup.",
            "DevOps lead is taking cat Luna to vet this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 904.1502475738525,
          "generation_latency_ms": 3936.9215965270996,
          "total_latency_ms": 4841.071844100952
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-2",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific missing asset is causing the Sales Lead to panic about the TechGlobal deal?",
          "answer_type": "short_answer",
          "gold_answer": "New dashboard",
          "predicted_answer": "The new dashboard.",
          "retrieved_context": [
            "Lucas is freaking out about closing the TechGlobal deal without the new dashboard",
            "Sales lead is stressed.",
            "User wants to keep Sales Lead out of this."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 584.0339660644531,
          "generation_latency_ms": 5058.54868888855,
          "total_latency_ms": 5642.582654953003
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-3",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific task is the Data Scientist, Sophia, currently working on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Data modeling.",
          "retrieved_context": [
            "Sophia is deep in data modeling.",
            "User says Data Scientist takes her time.",
            "User needs to catch Sophia later to see if she's done with new models."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 736.0494136810303,
          "generation_latency_ms": 5435.944080352783,
          "total_latency_ms": 6171.9934940338135
        },
        {
          "probe_id": "preference_learning-explicit_preferences-4",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What beverage does the user repeatedly request in the morning sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Espresso",
          "retrieved_context": [
            "User wants a double shot of espresso this morning",
            "User has not had coffee yet.",
            "User wants a quick status check before finishing coffee."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 746.6459274291992,
          "generation_latency_ms": 5837.228775024414,
          "total_latency_ms": 6583.874702453613
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-5",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions are currently conflicting on LoadBalancer8 according to the latest status check?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "stable v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "Alex wants to know the version of LoadBalancer8 on GCP.",
            "LoadBalancer8 reports conflicting versions: stable v1.2.0 and v2.0.0-beta.",
            "User is seeing conflicting signals on LoadBalancer8."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 659.7323417663574,
          "generation_latency_ms": 4435.142278671265,
          "total_latency_ms": 5094.874620437622
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-6",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific metric did the assistant check to verify the connection between the AWS auth service and the Google database?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Unknown. The context states that the user wants to check the connection but does not specify any metric used for this check.",
          "retrieved_context": [
            "Mia's load balancer and auth service need to ensure they are communicating with Google Cloud components correctly.",
            "User wants to check the auth service on the Amazon cluster.",
            "User has an authentication service running on Amazon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 602.7681827545166,
          "generation_latency_ms": 4072.34263420105,
          "total_latency_ms": 4675.110816955566
        }
      ],
      "summary": {
        "overall_accuracy": 0.14285714285714285,
        "total_score": 1,
        "count": 7,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 6,
            "accuracy": 0.16666666666666666
          },
          "abstain": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_012.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 15.369293689727783,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:cachelayer_8 hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "GCP",
          "retrieved_context": [
            "CacheLayer8 is on GCP",
            "Sophia and Mia claim CacheLayer7 is slowing down their workflow",
            "User wants to know if LoadBalancer6 and CacheLayer8 are in the same environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 574.5580196380615,
          "generation_latency_ms": 3769.710063934326,
          "total_latency_ms": 4344.268083572388
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User asks whether Liam closed the big deal.",
            "User says hold off.",
            "User plans to talk to DevOps lead about maintenance windows"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 802.4048805236816,
          "generation_latency_ms": 3944.0643787384033,
          "total_latency_ms": 4746.469259262085
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 2,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_013.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 14.52296233177185,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:lucas has role?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Backend lead.",
          "retrieved_context": [
            "Lucas is the backend lead.",
            "User caught up with Lucas",
            "User wants to know how Lucas is doing today"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 695.4808235168457,
          "generation_latency_ms": 3579.3635845184326,
          "total_latency_ms": 4274.844408035278
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asked for a recipe yesterday.",
            "User wants to know if they picked up their dry cleaning yesterday",
            "User wants to discuss cloud consolidation with James later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 581.8426609039307,
          "generation_latency_ms": 3988.3010387420654,
          "total_latency_ms": 4570.143699645996
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 2,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_014.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 16.497376918792725,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:charlotte has role?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Charlotte is leaning into the CEO role.",
          "retrieved_context": [
            "User finished a long strategy session with Charlotte.",
            "User had a call with Charlotte.",
            "Charlotte is worried about the backend team structure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 950.782060623169,
          "generation_latency_ms": 4627.825498580933,
          "total_latency_ms": 5578.607559204102
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User noted.",
            "User asks if Liam sent mockups yet",
            "User is asking whether their cofounder has confirmed the budget meeting time."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 597.7764129638672,
          "generation_latency_ms": 3957.0348262786865,
          "total_latency_ms": 4554.811239242554
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 2,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_015.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 94.63795375823975,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the team member responsible for the 'messy' deployment scripts mentioned by the user?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas",
          "retrieved_context": [
            "User suspects Lucas messed up deployment tags",
            "User wants to ping Lucas, who is in charge of DevOps and likely triggered the update and forgot to clean it up",
            "User thinks Lucas probably didn't clean up the canary deployment"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 681.1590194702148,
          "generation_latency_ms": 4221.790552139282,
          "total_latency_ms": 4902.949571609497
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the database service that is NOT co-located with the Auth service?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "AuthService6 and the second primary DB are on Google's cloud.",
            "PrimaryDB2 and AuthService6 are hosted on GCP",
            "User knows AuthService and PrimaryDB2 are in same region as load balancer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 671.5703010559082,
          "generation_latency_ms": 4658.094167709351,
          "total_latency_ms": 5329.664468765259
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the user's diagnosis, which specific database service is likely contributing to the latency reported by the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Lucas is concerned about latency between clouds and is trying to fix it.",
            "User needs to ask Liam about frontend latency",
            "User is experiencing weird latency on LoadBalancer9"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 551.184892654419,
          "generation_latency_ms": 4860.434532165527,
          "total_latency_ms": 5411.619424819946
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What is the confirmed operational strategy for LoadBalancer9's versioning, clarifying the initial ambiguity?",
          "answer_type": "short_answer",
          "gold_answer": "Canary testing",
          "predicted_answer": "Unknown. The context describes a version mismatch and a user's desire for the beta version, but no confirmed operational strategy for versioning LoadBalancer9 is provided.",
          "retrieved_context": [
            "User wants to know the current version of LoadBalancer9.",
            "User is dealing with LoadBalancer9.",
            "User wants LoadBalancer9 traffic to be fully on the beta version by now"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 593.6605930328369,
          "generation_latency_ms": 5031.608104705811,
          "total_latency_ms": 5625.2686977386475
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the hybrid cloud latency occur before or after the user asked to ping the DevOps lead regarding the alerts?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The discussion about the hybrid cloud latency occurred before the user asked to ping the DevOps lead regarding the alerts.",
          "retrieved_context": [
            "Lucas is concerned about latency between clouds and is trying to fix it.",
            "Latency spikes mostly around 9 AM",
            "Sophia was asking about dashboard latency earlier this morning"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 703.9055824279785,
          "generation_latency_ms": 4970.282316207886,
          "total_latency_ms": 5674.187898635864
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific architectural factor did the user identify as the cause for the dashboard latency around 9 AM?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid cloud setup",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Sophia was asking about dashboard latency earlier this morning",
            "Latency spikes mostly around 9 AM",
            "User needs to ask Liam about frontend latency"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 551.6586303710938,
          "generation_latency_ms": 5042.872905731201,
          "total_latency_ms": 5594.531536102295
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the CacheLayer4 service currently running?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "CacheLayer4 is up.",
            "User wants to check caching layer number four on Azure to ensure it's not cold",
            "User asks if cache layer is near Azure instance"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 644.3417072296143,
          "generation_latency_ms": 4585.150718688965,
          "total_latency_ms": 5229.492425918579
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Frontend lead loves cat.",
            "User is asking if the Sales Lead is free around noon.",
            "Sales lead is stressed."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 739.1872406005859,
          "generation_latency_ms": 4292.099237442017,
          "total_latency_ms": 5031.2864780426025
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-9",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration has been identified as problematic for latency when running the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hosting",
          "predicted_answer": "The cross-cloud hop.",
          "retrieved_context": [
            "Lucas is concerned about latency between clouds and is trying to fix it.",
            "User is experiencing weird latency on LoadBalancer9",
            "Latency spikes mostly around 9 AM"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 793.3118343353271,
          "generation_latency_ms": 5831.722259521484,
          "total_latency_ms": 6625.0340938568115
        },
        {
          "probe_id": "preference_learning-preference_drift-10",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "At what time does the user currently want to be reminded about their dry cleaning, considering their most recent request?",
          "answer_type": "short_answer",
          "gold_answer": "5:30 PM",
          "predicted_answer": "On the way home tonight.",
          "retrieved_context": [
            "User wants a reminder to pick up dry cleaning at 5 PM.",
            "User wants a reminder to pick up dry cleaning on the way home tonight.",
            "User wants reminder to buy milk later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 670.8500385284424,
          "generation_latency_ms": 4055.128574371338,
          "total_latency_ms": 4725.97861289978
        },
        {
          "probe_id": "preference_learning-explicit_preferences-11",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user prefer the assistant to handle communication with the DevOps lead when he is offline?",
          "answer_type": "short_answer",
          "gold_answer": "Ping when active",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User will talk to DevOps guy about following protocol.",
            "User will yell at DevOps lead.",
            "User will catch the DevOps lead at lunch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 689.4435882568359,
          "generation_latency_ms": 4525.519132614136,
          "total_latency_ms": 5214.962720870972
        }
      ],
      "summary": {
        "overall_accuracy": 0.45454545454545453,
        "total_score": 5,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 11,
            "accuracy": 0.45454545454545453
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_016.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 13,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 116.24353814125061,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific system is the user referring to as the 'traffic manager in our Google environment'?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "LoadBalancer2",
          "retrieved_context": [
            "LoadBalancer2 is the traffic manager in the Google environment and is behaving weirdly.",
            "User believes the main load balancer set up first is on Google's platform.",
            "User is asking about the location of the analytics engine"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 605.6613922119141,
          "generation_latency_ms": 3681.605100631714,
          "total_latency_ms": 4287.266492843628
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who are the two individuals identified as arguing over the button push for the beta release?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah and James",
          "predicted_answer": "Elijah and James",
          "retrieved_context": [
            "User told Elijah not to push beta to production yet.",
            "Elijah and James are arguing over who pushed the button.",
            "User acknowledges a version conflict."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 705.9822082519531,
          "generation_latency_ms": 3737.6863956451416,
          "total_latency_ms": 4443.668603897095
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who kept the leadership team late going over quarterly strategy?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Noah kept the leadership team late last night going over the quarterly strategy.",
            "User's cofounder is pacing around the office asking about uptime.",
            "James handles deployment pipelines."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 614.6190166473389,
          "generation_latency_ms": 4106.8830490112305,
          "total_latency_ms": 4721.502065658569
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "On which cloud provider is the service hosted that the user described as the 'outlier on Microsoft's cloud'?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "The authentication service.",
          "retrieved_context": [
            "User says authentication service is on Microsoft's cloud",
            "User finds it a headache to have infrastructure on Google while the main database and analytics engine are on Amazon.",
            "User believes the main load balancer set up first is on Google's platform."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 626.6145706176758,
          "generation_latency_ms": 4556.446552276611,
          "total_latency_ms": 5183.061122894287
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-5",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two specific software versions are currently conflicting on the GCP load balancer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta",
          "retrieved_context": [
            "Elijah and James are arguing about the versioning on that GCP load balancer",
            "Secondary load balancer is running in the region, User thinks there is a version mismatch, is on GCP, dashboard shows v1.2.0, logs show v2.0.0-beta.",
            "Elijah and James report weird behavior with load balancer on Google Cloud, which reports version 1.2.0 sometimes and flips to 2.0 beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 805.9999942779541,
          "generation_latency_ms": 5455.901622772217,
          "total_latency_ms": 6261.901617050171
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the infrastructure review discussion, why is latency expected between the primary database and the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "Hosted on different clouds",
          "predicted_answer": "Latency is expected because the main database is on Amazon, while other infrastructure (likely including the authentication service) is on Google, leading to cross-cloud latency.",
          "retrieved_context": [
            "User is worried about cross-cloud latency between main database and authentication service.",
            "Latency from the Google front-end to primarydb3 is noticeable.",
            "User wants to check latency of primary DB on Amazon's infrastructure"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 934.9231719970703,
          "generation_latency_ms": 4985.707759857178,
          "total_latency_ms": 5920.630931854248
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-7",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What personal task did the user ask to be reminded of during the conversation about cross-cloud latency?",
          "answer_type": "short_answer",
          "gold_answer": "Call dentist",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is worried about cross-cloud latency between main database and authentication service.",
            "User is seeing weird latency between Azure instance and AWS backend.",
            "User wants to check latency of primary DB on Amazon's infrastructure"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 676.9723892211914,
          "generation_latency_ms": 6014.488935470581,
          "total_latency_ms": 6691.4613246917725
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the assistant's preparation for the meeting, what type of infrastructure breakdown does the cofounder usually prefer?",
          "answer_type": "short_answer",
          "gold_answer": "High-level summary",
          "predicted_answer": "A breakdown related to multi-cloud footprint, consolidation, and multi-cloud sprawl, likely to address billing complexity.",
          "retrieved_context": [
            "User needs to prepare for an infrastructure review with cofounder later today.",
            "User's cofounder is pacing around the office asking about uptime.",
            "User is concerned about infrastructure latency for the board deck, and cofounder Noah is stressing about it."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 618.6819076538086,
          "generation_latency_ms": 5058.07638168335,
          "total_latency_ms": 5676.758289337158
        },
        {
          "probe_id": "preference_learning-preference_scope-9",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user have a stated preference for which cloud provider to review first during infrastructure checks?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Yes, the user wants to review the AWS bill.",
          "retrieved_context": [
            "User believes the main load balancer set up first is on Google's platform.",
            "User wants to consolidate infrastructure",
            "User finds it a headache to have infrastructure on Google while the main database and analytics engine are on Amazon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 695.5955028533936,
          "generation_latency_ms": 5432.038068771362,
          "total_latency_ms": 6127.633571624756
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the current running version of the authentication service on Azure?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown. The context mentions \"authservice3 is a headache,\" but does not specify it as the current running version.",
          "retrieved_context": [
            "User asks about authentication service on Azure",
            "User is asking if any services are running on Azure.",
            "User says authentication service is on Microsoft's cloud"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 1461.9908332824707,
          "generation_latency_ms": 6071.53058052063,
          "total_latency_ms": 7533.521413803101
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-11",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the team lead for the Analytics Engine?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking about the location of the analytics engine",
            "User wants the analytics engine checked.",
            "primarydb3 and analyticsengine7 are located next to each other, aiding reporting jobs."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 864.6576404571533,
          "generation_latency_ms": 5367.84029006958,
          "total_latency_ms": 6232.497930526733
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What specific action did the assistant propose to help resolve the confusion while the DevOps team argued?",
          "answer_type": "short_answer",
          "gold_answer": "Flag the discrepancy",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to ping DevOps team.",
            "User will get the DevOps guys on the version mismatch.",
            "James handles deployment pipelines."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 637.0227336883545,
          "generation_latency_ms": 4105.354309082031,
          "total_latency_ms": 4742.377042770386
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-13",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the user's routine for preparing for meetings with the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Review infrastructure overview",
          "predicted_answer": "Unknown. The context only states that the user \"needs to prepare for an infrastructure review with cofounder later today,\" but does not",
          "retrieved_context": [
            "User's cofounder is stressed about the upcoming board meeting",
            "User's cofounder is pacing around the office asking about uptime.",
            "User needs to prepare for an infrastructure review with cofounder later today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 657.4828624725342,
          "generation_latency_ms": 4650.084018707275,
          "total_latency_ms": 5307.56688117981
        }
      ],
      "summary": {
        "overall_accuracy": 0.46153846153846156,
        "total_score": 6,
        "count": 13,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 12,
            "accuracy": 0.5
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 3,
            "accuracy": 0.3333333333333333
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_017.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 34,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 106.60520887374878,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as 'my cofounder' by the user?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User had a strategy session with cofounder.",
            "User's cofounder is asking about auth services 6 and 7.",
            "User wants to talk to cofounder about consolidating infrastructure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 611.2284660339355,
          "generation_latency_ms": 4028.9080142974854,
          "total_latency_ms": 4640.136480331421
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who shares the Backend Lead responsibilities with the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "User says the other backend lead has a cat named Luna.",
            "Luna is the cat.",
            "Luna is a mascot."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 571.7344284057617,
          "generation_latency_ms": 4433.314561843872,
          "total_latency_ms": 5005.048990249634
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that is currently reporting conflicting versions v1.2.0 and v2.0.0-beta?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User notes that the multi-cloud setup is getting out of hand and feels hosting is fragmented.",
            "User is asking whether the third analytics engine instance is running on Amazon's infrastructure.",
            "User believes the GCP instance is legacy"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 572.5729465484619,
          "generation_latency_ms": 4701.255559921265,
          "total_latency_ms": 5273.828506469727
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which cloud provider hosts the primary database mentioned during the infrastructure map review?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User is asking for location of primary database",
            "PrimaryDB5 is located in Azure, causing high latency and inefficiency.",
            "User asks about main database"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 653.7063121795654,
          "generation_latency_ms": 4718.28556060791,
          "total_latency_ms": 5371.991872787476
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the user, what was the specific cause of Will's distraction during their earlier call?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumping on desk",
          "predicted_answer": "His cat.",
          "retrieved_context": [
            "User was on a call with Will earlier",
            "User thinks Will got distracted by his cat.",
            "User needs to talk to Will."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1201.3733386993408,
          "generation_latency_ms": 4006.6745281219482,
          "total_latency_ms": 5208.047866821289
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user requested a 'sanity check' on the infrastructure map, where did the assistant locate AuthService6 and AuthService7?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AuthService6 and AuthService7 are on AWS.",
          "retrieved_context": [
            "User asks where AuthService6 and AuthService7 are hosted.",
            "User wants to check AuthService1.",
            "User wants a rundown of auth services 1, 6, and 7."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 740.004301071167,
          "generation_latency_ms": 4736.268997192383,
          "total_latency_ms": 5476.27329826355
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user imply they need before dealing with infrastructure latency or complex messes?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "User needs to grab coffee before dealing with latency.",
            "User thinks James is complaining about latency.",
            "User needs another coffee before tackling the budget."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 767.2266960144043,
          "generation_latency_ms": 4688.462257385254,
          "total_latency_ms": 5455.688953399658
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user explicitly state a preference for using GCP over AWS for authentication services?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, the user explicitly states that AuthService1, which is on GCP,",
          "retrieved_context": [
            "User is looking at infrastructure and asks if all auth services are on AWS",
            "AuthService1 is located in GCP, causing high latency and inefficiency.",
            "AuthService6 and AuthService7 are on AWS, and the user is using every major cloud provider."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 665.0650501251221,
          "generation_latency_ms": 5571.275949478149,
          "total_latency_ms": 6236.3409996032715
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the final score of the game the user missed last night?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if Giants won last night",
            "User thinks James is complaining about latency.",
            "User didn't have a good answer to Olivia's question"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 691.619873046875,
          "generation_latency_ms": 4137.934446334839,
          "total_latency_ms": 4829.554319381714
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the development environment for the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Frontend Lead is probably testing the beta integration today.",
            "AnalyticsEngine3 is hosted on Amazon's cloud.",
            "User thinks being a backend lead is a headache."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 594.4325923919678,
          "generation_latency_ms": 5084.410190582275,
          "total_latency_ms": 5678.842782974243
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What general observation did the assistant make about the difficulty of working from home when discussing Will's situation?",
          "answer_type": "short_answer",
          "gold_answer": "Challenging with a cat",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User agrees that Will's situation sounds about right.",
            "Will was supposed to handle migration.",
            "User thinks Will got distracted by his cat."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 598.2506275177002,
          "generation_latency_ms": 5295.446872711182,
          "total_latency_ms": 5893.697500228882
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the documented procedure for migrating AuthService1 from GCP to AWS?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown. The context does not provide a documented procedure for migrating AuthService1 from GCP to AWS.",
          "retrieved_context": [
            "AuthService6 and AuthService7 are on AWS, and the user is using every major cloud provider.",
            "User is looking at infrastructure and asks if all auth services are on AWS",
            "AuthService1 is located in GCP, causing high latency and inefficiency."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 597.8124141693115,
          "generation_latency_ms": 5204.8821449279785,
          "total_latency_ms": 5802.69455909729
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_018.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 108.47590041160583,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user's cofounder that was concerned about the AWS credits?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown. The context does not state that the cofounder was concerned about AWS credits.",
          "retrieved_context": [
            "User asks if the cofounder approved the budget for extra instances.",
            "User acknowledges multi-cloud strategy; cofounder will ask about latency between the Amazon-hosted cache and the Google cloud database during the all-hands.",
            "User thinks the cofounder is probably swamped with meetings."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 667.7329540252686,
          "generation_latency_ms": 5497.992992401123,
          "total_latency_ms": 6165.725946426392
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the Frontend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "The frontend lead is obsessed with a pet.",
            "Lucas is the Frontend Lead and is always on camera",
            "User wants to find out who is handling product design for the new dashboard."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 725.9640693664551,
          "generation_latency_ms": 4718.170881271362,
          "total_latency_ms": 5444.134950637817
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Does the service currently running v2.0.0-beta reside on the same cloud provider as PrimaryDB5?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. The user knows primary databases (like PrimaryDB5) are not hosted in",
          "retrieved_context": [
            "User asks if cache is syncing correctly with PrimaryDB5 on Google Cloud.",
            "User wants to verify the hosting zone for PrimaryDB5.",
            "PrimaryDB5 and PrimaryDB8 are showing latency spikes on Google cloud"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 627.1934509277344,
          "generation_latency_ms": 4998.972415924072,
          "total_latency_ms": 5626.165866851807
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the latest diagnostics, is CacheLayer7 running a single stable version?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User believes CacheLayer7 is stable on v1.2.0 and asks if an update was pushed.",
            "User wants a quick status check of CacheLayer7 before grabbing their third coffee.",
            "User is seeing weird telemetry on CacheLayer7."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 608.3340644836426,
          "generation_latency_ms": 5151.937007904053,
          "total_latency_ms": 5760.271072387695
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the Frontend Lead running late in the session where the user verified the database providers?",
          "answer_type": "short_answer",
          "gold_answer": "Cat knocked over monitor",
          "predicted_answer": "Unknown. The context does not state that the Frontend Lead was running late, nor does it connect him to the session where the user verified database providers.",
          "retrieved_context": [
            "User attributes latency to the frontend lead testing and forgetting to rollback",
            "Lucas mentioned frontend latency was spiking.",
            "User's cofounder was worried about latency in Google-hosted databases"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 658.6501598358154,
          "generation_latency_ms": 5525.730848312378,
          "total_latency_ms": 6184.381008148193
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user identify the conflicting versions on CacheLayer7 before or after learning about Lucas's monitor accident?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User thought CacheLayer7 update was frozen.",
            "User will grab Lucas to help investigate cache issue later",
            "User believes CacheLayer7 is stable on v1.2.0 and asks if an update was pushed."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1089.0815258026123,
          "generation_latency_ms": 4700.998306274414,
          "total_latency_ms": 5790.079832077026
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine requests, what beverage do they frequently consume?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee (specifically espresso).",
          "retrieved_context": [
            "User needs to grab coffee before 2 PM",
            "User is barely awake and needs a third espresso to function.",
            "User is experiencing a foggy brain before coffee."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 762.5532150268555,
          "generation_latency_ms": 4904.639959335327,
          "total_latency_ms": 5667.193174362183
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Has the user explicitly requested to move the caching layers to GCP?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User believes caching layers are hosted on Amazon",
            "User wants information specifically for storage and caching layers",
            "User wants to leave checking cache layer 6 for now"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 672.1000671386719,
          "generation_latency_ms": 5753.154277801514,
          "total_latency_ms": 6425.254344940186
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts PrimaryDB9?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "PrimaryDB5 and PrimaryDB8 are showing latency spikes on Google cloud",
            "User wants to verify the hosting zone for PrimaryDB5.",
            "User wants only PrimaryDB5 data for snapshot."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 576.1888027191162,
          "generation_latency_ms": 4130.655288696289,
          "total_latency_ms": 4706.844091415405
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of VP of Marketing?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to find out who is handling product design for the new dashboard.",
            "User is the CEO.",
            "Lucas is the Frontend Lead and is always on camera"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 588.510274887085,
          "generation_latency_ms": 4277.406215667725,
          "total_latency_ms": 4865.91649055481
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What action does the assistant consistently take when the user mentions buying items like coffee beans or cat food?",
          "answer_type": "short_answer",
          "gold_answer": "Adds to shopping list",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to buy coffee beans later",
            "User wants a reminder to buy cat food for neighbor",
            "User needs to grab coffee before dealing with issue."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 751.7745494842529,
          "generation_latency_ms": 4584.083080291748,
          "total_latency_ms": 5335.857629776001
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What is the recurring cause of interruptions during the Frontend Lead's standups?",
          "answer_type": "short_answer",
          "gold_answer": "His cat Luna",
          "predicted_answer": "Unknown. The context mentions Lucas (the Frontend Lead) is always on camera during standups, but does not describe any recurring interruptions during them.",
          "retrieved_context": [
            "Lucas mentioned frontend latency was spiking.",
            "User thinks the cofounder is probably swamped with meetings.",
            "User says someone walked across Lucas's keyboard and deployed a hotfix, and that person is the best frontend engineer"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 668.0653095245361,
          "generation_latency_ms": 4498.179197311401,
          "total_latency_ms": 5166.2445068359375
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 7,
            "accuracy": 0.14285714285714285
          },
          "boolean": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_019.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 110.89736127853394,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking if their cofounder has arrived.",
            "User wants to know if cofounder said anything about board meeting.",
            "User wants to sync with cofounder this afternoon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 654.8895835876465,
          "generation_latency_ms": 4745.849847793579,
          "total_latency_ms": 5400.739431381226
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud platform hosts the database instance specifically used for legacy enterprise clients?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User asks if the ninth database instance is on Microsoft's cloud.",
            "User wants status of main database instance hosted on Amazon",
            "User is asking if third primary DB is on Microsoft's cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 733.7472438812256,
          "generation_latency_ms": 4532.58752822876,
          "total_latency_ms": 5266.334772109985
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person responsible for the sales strategy?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "Alex wants to know who is leading sales.",
            "User is trying to remember who is leading the sales push this quarter and needs to forward a lead to that person.",
            "User needs to coordinate with Sales on that environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 722.5513458251953,
          "generation_latency_ms": 5510.481119155884,
          "total_latency_ms": 6233.032464981079
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-0",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Does the user consider the conflicting version flags on PrimaryDB1 to be an error or an expected state?",
          "answer_type": "short_answer",
          "gold_answer": "Expected state",
          "predicted_answer": "The user considers the conflicting version flags on PrimaryDB1 to be an error or a problematic state, as it is linked to \"weird latency\" and \"increasing costs.\"",
          "retrieved_context": [
            "PrimaryDB1 is reporting v1.2.0 and v2.0.0-beta simultaneously.",
            "User is experiencing weird latency on PrimaryDB1.",
            "User wants to check PrimaryDB3."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 577.9035091400146,
          "generation_latency_ms": 5986.065626144409,
          "total_latency_ms": 6563.969135284424
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user explain the reason for the version conflict on PrimaryDB1 before or after adding coffee beans to their shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context does not mention the user explaining the reason for the version conflict on PrimaryDB1, nor does it mention coffee beans or a shopping list.",
          "retrieved_context": [
            "User is tired from debugging PrimaryDB1 all night.",
            "PrimaryDB1 is reporting v1.2.0 and v2.0.0-beta simultaneously.",
            "User refers to PrimaryDB3"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 671.9958782196045,
          "generation_latency_ms": 4543.026447296143,
          "total_latency_ms": 5215.022325515747
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific errand did the user ask to be reminded about for 5:30 PM?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to call mom at 6:00 PM",
            "User wants a reminder to order lunch in an hour.",
            "User wants reminder to buy concert tickets at 2 PM"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 752.225399017334,
          "generation_latency_ms": 4672.193765640259,
          "total_latency_ms": 5424.419164657593
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What communication style does the user imply is preferred during early morning interactions before they have had caffeine?",
          "answer_type": "short_answer",
          "gold_answer": "Brief",
          "predicted_answer": "Direct, concise, and to the point.",
          "retrieved_context": [
            "User needs to get through this morning",
            "User will grab another coffee before next meeting",
            "User is fine and needs more coffee."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 847.2204208374023,
          "generation_latency_ms": 5342.966556549072,
          "total_latency_ms": 6190.186977386475
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user imply is necessary for them to function effectively in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "User needs to get through this morning",
            "User is fine and needs more coffee.",
            "User is going to grab a coffee."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 669.2163944244385,
          "generation_latency_ms": 5288.559913635254,
          "total_latency_ms": 5957.776308059692
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current VP of Engineering?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Olivia is Product Design.",
            "Alex wants to know who is leading sales.",
            "User wants to confirm whether Oliver moved the analytics engine without telling User."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 659.9342823028564,
          "generation_latency_ms": 4800.978660583496,
          "total_latency_ms": 5460.9129428863525
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts PrimaryDB2?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking if third primary DB is on Microsoft's cloud.",
            "User wants to know location of PrimaryDB9 and PrimaryDB3.",
            "User wants to check PrimaryDB3."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 741.0423755645752,
          "generation_latency_ms": 4778.3942222595215,
          "total_latency_ms": 5519.436597824097
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical process explains why a database might report two different versions simultaneously according to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Blue-green deployment",
          "predicted_answer": "The context does not explain a specific technical process for why a database might report two different versions simultaneously. It only states that \"version reporting was acting up yesterday.\"",
          "retrieved_context": [
            "PrimaryDB1 is reporting v1.2.0 and v2.0.0-beta simultaneously.",
            "User will ask Sophia to hold off on UI updates until database versioning is fixed.",
            "User is concerned that Sophia's team should not hit a half-baked database"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 728.4939289093018,
          "generation_latency_ms": 5364.14647102356,
          "total_latency_ms": 6092.640399932861
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Who does the user identify as the point of contact for UI issues caused by API changes?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia",
          "retrieved_context": [
            "User wants the product designer to be invited to discuss UI latency.",
            "User wants to confirm whether Oliver moved the analytics engine without telling User.",
            "User needs to ping Olivia about new dashboard designs"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 536.0507965087891,
          "generation_latency_ms": 4985.342264175415,
          "total_latency_ms": 5521.393060684204
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_020.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 116.23591661453247,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who did the user plan to meet for coffee prior to the board meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "The cofounder.",
          "retrieved_context": [
            "User needs to grab lunch before next meeting",
            "User thought they had a strategy sync with cofounder this morning",
            "User wants a reminder at 2 PM to grab coffee with cofounder before board meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 700.7036209106445,
          "generation_latency_ms": 3887.401819229126,
          "total_latency_ms": 4588.1054401397705
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the cache instance identified as causing the cross-cloud hop latency?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Cross-cloud latency issues are not present.",
          "retrieved_context": [
            "Cross-cloud latency issues are not present",
            "User's sixth cache layer is on Azure and may be causing latency.",
            "Alex and cofounder are seeing jitter on the main cache layer hosted on AWS."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 718.3613777160645,
          "generation_latency_ms": 5374.222755432129,
          "total_latency_ms": 6092.584133148193
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's colleague's cat Luna walked across keyboard during pitch.",
            "James loves Luna the cat.",
            "Luna had a vet appointment"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 699.6541023254395,
          "generation_latency_ms": 5508.2502365112305,
          "total_latency_ms": 6207.90433883667
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-3",
          "pillar": "declarative_reasoning",
          "subpillar": "task_state_tracking",
          "question": "Which two software versions were detected running simultaneously on the AWS cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown. The context states that two different versions were detected, but does not specify what those versions were.",
          "retrieved_context": [
            "User suspects a connection issue with the Azure cache service and notes it is reporting two different version numbers",
            "User suspects that deployment of CacheLayer7 to Amazon's cloud got messy",
            "User's logs for cache layer 7 show two different versions responding to requests simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 668.1973934173584,
          "generation_latency_ms": 4434.099435806274,
          "total_latency_ms": 5102.296829223633
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What architectural configuration did the user identify as the primary cause of backend latency?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hop",
          "predicted_answer": "The user identified two different versions responding to requests simultaneously on cache layer 7 as a cause",
          "retrieved_context": [
            "User's sixth cache layer is on Azure and may be causing latency.",
            "User is seeing weird latency in the logs.",
            "User attributes latency spikes to drift."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 604.1903495788574,
          "generation_latency_ms": 6068.086385726929,
          "total_latency_ms": 6672.276735305786
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user notice the deployment glitch on CacheLayer7 before or after the discussion about the board meeting?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown. The context does not mention a discussion about a board meeting.",
          "retrieved_context": [
            "User suspects that deployment of CacheLayer7 to Amazon's cloud got messy",
            "User wants to know if CacheLayer7 is behaving.",
            "CacheLayer7 is running hot"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 710.0355625152588,
          "generation_latency_ms": 4875.450849533081,
          "total_latency_ms": 5585.48641204834
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about latency, which infrastructure setup do they appear to dislike?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid setup",
          "predicted_answer": "Their sixth cache layer on Azure.",
          "retrieved_context": [
            "User's sixth cache layer is on Azure and may be causing latency.",
            "User's cofounder is stressing out about latency spikes seen this morning",
            "Cross-cloud latency issues are not present"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 659.9645614624023,
          "generation_latency_ms": 5310.530424118042,
          "total_latency_ms": 5970.494985580444
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Why did the user explicitly state they wanted to buy a toy for Luna?",
          "answer_type": "short_answer",
          "gold_answer": "To please James",
          "predicted_answer": "Unknown. The context states the user wants to pick up a toy for Luna, but does not provide a reason why.",
          "retrieved_context": [
            "User wants to pick up a toy for Luna later.",
            "Luna had a vet appointment",
            "User heard about Luna this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1507.7323913574219,
          "generation_latency_ms": 5090.521335601807,
          "total_latency_ms": 6598.2537269592285
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Data Scientist's pet?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User intends to get a data scientist to analyze traffic patterns.",
            "User has a colleague named Mia",
            "Charlotte is looking into data anomalies now"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 612.3237609863281,
          "generation_latency_ms": 5419.7962284088135,
          "total_latency_ms": 6032.119989395142
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the database layer?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "CacheLayer7 runs on Amazon.",
            "User's sixth cache layer is on Azure and may be causing latency.",
            "Azure and AWS are split."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 592.0453071594238,
          "generation_latency_ms": 5038.907289505005,
          "total_latency_ms": 5630.952596664429
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What risk did the assistant identify regarding the concurrent execution of two versions on the same instance?",
          "answer_type": "short_answer",
          "gold_answer": "Data inconsistency",
          "predicted_answer": "Unknown. The context describes risks identified by the user and their cofounder, but not by an assistant.",
          "retrieved_context": [
            "User's cofounder is worried about version mismatches causing latency",
            "User's logs for cache layer 7 show two different versions responding to requests simultaneously.",
            "User says deployment script glitched and that beta and v1.2.0 should not be running simultaneously, but the version tag is locked at v1.2.0 while logs report a beta build 2.0.0."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 908.9303016662598,
          "generation_latency_ms": 5675.4467487335205,
          "total_latency_ms": 6584.37705039978
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant verify the status of the traffic distributor on Amazon?",
          "answer_type": "short_answer",
          "gold_answer": "Pulled up metrics",
          "predicted_answer": "Unknown. The context states the user *asked* for the status and metrics, but not *how* the assistant verified it.",
          "retrieved_context": [
            "User wants metrics for main traffic distributor hosted on Amazon.",
            "User asks if LoadBalancer1 is handling traffic okay on Amazon side.",
            "User's cofounder is worried traffic isn't routing correctly."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 538.2847785949707,
          "generation_latency_ms": 6238.997220993042,
          "total_latency_ms": 6777.281999588013
        }
      ],
      "summary": {
        "overall_accuracy": 0.16666666666666666,
        "total_score": 2,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 10,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_021.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 122.38992667198181,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that requested a status update on the production environment?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cofounder is asking for a status update on the production environment again.",
            "User is giving a quick update on the project",
            "User wants to know if cofounder sent updated quarterly goals."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 679.8460483551025,
          "generation_latency_ms": 5001.47271156311,
          "total_latency_ms": 5681.318759918213
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to buy cat treats for Luna.",
            "User hasn't seen Will's cat Luna in a while.",
            "User says Will's cat Luna is usually pretty chill"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 780.5995941162109,
          "generation_latency_ms": 5473.581552505493,
          "total_latency_ms": 6254.181146621704
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service managed by the Data Scientist hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Sophia manages the data science models",
            "User asks who the Data Scientist is.",
            "User wants to verify the location of the auth service and asks if it is all on Microsoft's cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 715.3604030609131,
          "generation_latency_ms": 6001.615524291992,
          "total_latency_ms": 6716.975927352905
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Which two versions of the analytics engine are currently reporting conflicting signals in the logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and the beta version.",
          "retrieved_context": [
            "Production logs show analytics engine version v1.2.0.",
            "User thinks beta version of Analytics Engine is unstable.",
            "User is seeing weird telemetry from the analytics engine"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 612.8683090209961,
          "generation_latency_ms": 5527.090311050415,
          "total_latency_ms": 6139.958620071411
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the Backend Lead exhausted during the conversation by the coffee machine?",
          "answer_type": "short_answer",
          "gold_answer": "His cat kept him awake",
          "predicted_answer": "Unknown. The context states that Will was pulling all-nighters on backend code and dealing with immediate fire, making him exhausted, but it does not explicitly state that Will is the Backend Lead or that the Backend Lead was exhausted during the coffee machine conversation.",
          "retrieved_context": [
            "User says backend lead is out.",
            "User asks if the frontend lead is blaming the load balancer again.",
            "User needs a second coffee before tackling the incident queue today"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 939.7997856140137,
          "generation_latency_ms": 5808.1347942352295,
          "total_latency_ms": 6747.934579849243
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the same conversation where the user asked for a reminder to call their mom, what was the status of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Mixed signals",
          "predicted_answer": "The user wanted to check the status of the analytics engine.",
          "retrieved_context": [
            "User wants a reminder to call mom at 6 PM today.",
            "User wants to check status of analytics engine.",
            "User is asking whether the analytics engine is stable."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 556.0328960418701,
          "generation_latency_ms": 4680.818557739258,
          "total_latency_ms": 5236.851453781128
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the logs, which version of the analytics engine do they prefer to be active in production?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "Unknown. The context does not describe the user's reaction to the logs, nor does",
          "retrieved_context": [
            "User is uncertain whether AnalyticsEngine1 deployment succeeded and wants to know its current version.",
            "Production logs show analytics engine version v1.2.0.",
            "Alex wants to check the current version of the Analytics Engine."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 657.8488349914551,
          "generation_latency_ms": 6291.065454483032,
          "total_latency_ms": 6948.914289474487
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Has the user explicitly requested to filter the Auth Service logs to a single cloud provider?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User asks if auth service is still split across clouds.",
            "User wants to know if the auth service is impacted.",
            "User wants confirmation that Auth Service is on Azure"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 795.7448959350586,
          "generation_latency_ms": 6601.9697189331055,
          "total_latency_ms": 7397.714614868164
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the Load Balancer currently running?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to check where LoadBalancer4 is hosted.",
            "User asks if the frontend lead is blaming the load balancer again.",
            "LoadBalancer4 is on Google's cloud"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 725.0978946685791,
          "generation_latency_ms": 5684.467554092407,
          "total_latency_ms": 6409.565448760986
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the Auth Service currently hosted exclusively on Azure?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. AuthService2 is now hosted on GCP.",
          "retrieved_context": [
            "User wants confirmation that Auth Service is on Azure",
            "User wants to verify the location of the auth service and asks if it is all on Microsoft's cloud.",
            "User asks if auth service is still split across clouds."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 567.4548149108887,
          "generation_latency_ms": 6056.708574295044,
          "total_latency_ms": 6624.163389205933
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Who should be contacted regarding discrepancies in the data science model deployments?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia",
          "retrieved_context": [
            "Sophia manages the data science models",
            "User plans to involve Data Scientist to investigate version mismatch on the Analytics Engine",
            "Sophia is always trying to get the latest models running for the data science team"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 692.1834945678711,
          "generation_latency_ms": 5022.960424423218,
          "total_latency_ms": 5715.143918991089
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What routine action does the user typically take before starting the daily triage?",
          "answer_type": "short_answer",
          "gold_answer": "Grab coffee",
          "predicted_answer": "The user grabs a quick coffee.",
          "retrieved_context": [
            "User wants to grab a quick coffee before starting the daily triage.",
            "User is barely awake and wants help sorting alerts before finishing coffee.",
            "User needs a second coffee before tackling the incident queue today"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 577.0480632781982,
          "generation_latency_ms": 4737.64705657959,
          "total_latency_ms": 5314.695119857788
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "boolean": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_022.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 30,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 139.95926094055176,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "User has a cofounder.",
            "User says the cofounder is always moving fast.",
            "Elijah is the cofounder."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 669.1875457763672,
          "generation_latency_ms": 6144.65594291687,
          "total_latency_ms": 6813.843488693237
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the Sales Lead who is known for watching the dashboards?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if Ava or Sophia mentioned sales dashboard lagging.",
            "User asks if sales crew (Ava or Sophia) are seeing issues.",
            "User asks about rest of sales leads."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 972.8744029998779,
          "generation_latency_ms": 5181.1769008636475,
          "total_latency_ms": 6154.051303863525
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service running conflicting versions v1.2.0 and v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User says at least one cloud is behaving.",
            "LoadBalancer3 is reporting two versions running at once: v1.2.0 and v2.0.0-beta.",
            "User is worried about the deployment manifest and asks if the instance is behaving."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 685.5924129486084,
          "generation_latency_ms": 6260.199069976807,
          "total_latency_ms": 6945.791482925415
        },
        {
          "probe_id": "declarative_reasoning-composition-4",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which specific service is hosted on the same cloud platform as 'AnalyticsEngine9'?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer3",
          "predicted_answer": "Unknown. The context states AnalyticsEngine9 is on Azure, and CacheLayer5 is on GCP.",
          "retrieved_context": [
            "User asks if AnalyticsEngine9 is hosted with LoadBalancer1.",
            "User asks where analytics engine is pointing",
            "User says analytics engine is running on Microsoft's stack."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 562.558650970459,
          "generation_latency_ms": 6682.930946350098,
          "total_latency_ms": 7245.489597320557
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What item did the user ask to be reminded to buy immediately after discussing Elijah's confusion about the load balancer's location?",
          "answer_type": "short_answer",
          "gold_answer": "Milk",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Elijah sent a message asking if the Azure load balancer is acting up again",
            "Elijah was convinced that LoadBalancer3 was on Google.",
            "User asks about traffic on the new load balancer deployment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 679.9607276916504,
          "generation_latency_ms": 5601.389646530151,
          "total_latency_ms": 6281.350374221802
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to be reminded about 'dry cleaning' before or after they asked to be reminded about 'milk'?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "The user asked to be reminded about 'milk' before they asked to be reminded about 'dry cleaning'.",
          "retrieved_context": [
            "User wants a reminder to pick up milk later",
            "User wants a reminder to buy milk on the way home.",
            "User wants a reminder to pick up dry cleaning on the way home."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 616.0657405853271,
          "generation_latency_ms": 5644.753217697144,
          "total_latency_ms": 6260.818958282471
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What consumable item does the user explicitly state is their 'first priority' before they can function?",
          "answer_type": "short_answer",
          "gold_answer": "Caffeine",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "User plans to grab a coffee first",
            "User needs another coffee before looking at a screen today.",
            "User needs a double shot of espresso to function today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 675.5962371826172,
          "generation_latency_ms": 8622.266292572021,
          "total_latency_ms": 9297.862529754639
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Regarding the conflicting versions on LoadBalancer3, does the user prefer to keep the beta percentage high or low?",
          "answer_type": "short_answer",
          "gold_answer": "Low",
          "predicted_answer": "The user wants to keep the beta percentage low.",
          "retrieved_context": [
            "User believes LoadBalancer3 is acting up",
            "LoadBalancer3 is reporting two versions running at once: v1.2.0 and v2.0.0-beta.",
            "User wants to check versioning on LoadBalancer3."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 916.0006046295166,
          "generation_latency_ms": 5750.468730926514,
          "total_latency_ms": 6666.46933555603
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is 'CacheLayer9' hosted on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if the seventh cache layer is on GCP.",
            "User asks if the fifth cache layer is on Azure.",
            "CacheLayer5 and LoadBalancer7 are running on Google's cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 705.5039405822754,
          "generation_latency_ms": 5867.844104766846,
          "total_latency_ms": 6573.348045349121
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What specific version number is System:LoadBalancer_7 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to check versioning on LoadBalancer3.",
            "User will handle load balancer versioning",
            "User thought the load balancer was locked to v1.2.0"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 1078.8276195526123,
          "generation_latency_ms": 7127.90584564209,
          "total_latency_ms": 8206.733465194702
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What specific action did the user decide to take to resolve the jitter caused by the split configuration on LoadBalancer3?",
          "answer_type": "short_answer",
          "gold_answer": "Manually kill the beta instance",
          "predicted_answer": "Alex will manually check the third load balancer.",
          "retrieved_context": [
            "User believes LoadBalancer3 is acting up",
            "User is seeing oscillating metrics on the third load balancer.",
            "User has the third load balancer running on Microsoft stack that feels off."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 675.1675605773926,
          "generation_latency_ms": 6667.172908782959,
          "total_latency_ms": 7342.340469360352
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What is the current versioning state of the load balancer hosted on Azure?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting versions",
          "predicted_answer": "Unknown. The context states a user wants to check versioning on LoadBalancer3, which is on a Microsoft stack (implying Azure), but does not provide the current versioning state.",
          "retrieved_context": [
            "User wants to check the current state of the Azure infrastructure load balancer.",
            "User wants to know if the seventh load balancer is on Azure.",
            "User wants to check if LoadBalancer3 is still hosted on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1341.644048690796,
          "generation_latency_ms": 6227.311849594116,
          "total_latency_ms": 7568.955898284912
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 10,
            "accuracy": 0.2
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_023.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 8,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 92.0879282951355,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' the user intends to sync with regarding the cloud footprint?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "User is mapping current cloud footprint for board meeting with cofounder",
            "User wants to check if cofounder is free for a quick sync.",
            "User is trying to consolidate their cloud footprint"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 720.391035079956,
          "generation_latency_ms": 4734.019041061401,
          "total_latency_ms": 5454.410076141357
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member is associated with the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Both the User and Charlotte are associated with the pet named Luna in the context.",
          "retrieved_context": [
            "User has a cat named Luna who is a cat",
            "The cat is the team mascot.",
            "User is unsure if Luna would sleep in."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 705.5504322052002,
          "generation_latency_ms": 5946.753263473511,
          "total_latency_ms": 6652.303695678711
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the database that the user described as an 'outlier'?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User is annoyed that PrimaryDB3 is on Azure while other databases are on Google.",
            "User believes there is a database straggler not on Google's cloud and wants to migrate it.",
            "User mixes up which database we stick on Azure"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 708.0790996551514,
          "generation_latency_ms": 5284.96527671814,
          "total_latency_ms": 5993.044376373291
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two conflicting versions is AnalyticsEngine3 currently reporting?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants confirmation of version numbers for AnalyticsEngine3.",
            "User is asking whether AnalyticsEngine3 is fully updated.",
            "User wants to check AnalyticsEngine2 stability."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 753.0877590179443,
          "generation_latency_ms": 5910.322427749634,
          "total_latency_ms": 6663.410186767578
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What external factor did the Data Scientist blame for the model latency issues?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User attributes weird latency to frontend team deployment.",
            "User says Elijah was complaining about latency on sales dashboards yesterday.",
            "User suspects the data scientist toggled a flag to test new models on the beta version of AnalyticsEngine3."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 677.6337623596191,
          "generation_latency_ms": 6709.690093994141,
          "total_latency_ms": 7387.32385635376
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific item did the user request a reminder to purchase on the way home?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "Coffee beans",
          "retrieved_context": [
            "User wants a reminder to buy coffee beans on their way home",
            "User wants a reminder to buy cat treats later.",
            "User wants reminder to pick up dry cleaning on the way home."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 571.3255405426025,
          "generation_latency_ms": 6436.139822006226,
          "total_latency_ms": 7007.465362548828
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the Sales Lead's infrastructure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks which database instance was deployed to Azure.",
            "User asks if Sales Lead updated quarterly projections.",
            "User's cofounder wants a breakdown of current cloud footprint before noon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 688.3690357208252,
          "generation_latency_ms": 5728.322505950928,
          "total_latency_ms": 6416.691541671753
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who did the assistant plan to contact regarding the UI glitches caused by the engine version conflict?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia and Ava",
          "predicted_answer": "Frontend leads.",
          "retrieved_context": [
            "User wants to ping frontend leads about UI glitches.",
            "User needs to fix the issue before Elijah demos it to a client.",
            "User's cofounder was asking about versioning on the analytics engine"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 981.0042381286621,
          "generation_latency_ms": 6452.8703689575195,
          "total_latency_ms": 7433.874607086182
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 4,
        "count": 8,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 7,
            "accuracy": 0.42857142857142855
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_024.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 29,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 118.97123885154724,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "User needs to sync with cofounder later.",
            "User wants to know who is handling the UI team.",
            "User's cofounder is concerned about the infrastructure bill."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 729.2923927307129,
          "generation_latency_ms": 4559.371471405029,
          "total_latency_ms": 5288.663864135742
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who shares the Sales Lead role with Oliver?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking if frontend leads are still just Olivia.",
            "Mia runs sales with Oliver.",
            "Elijah thinks Oliver and Mia leading the sales department is redundant."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 658.2291126251221,
          "generation_latency_ms": 6273.165464401245,
          "total_latency_ms": 6931.394577026367
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that the user described as 'sitting all alone'?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Unknown. The context does not explicitly state which service the user described as 'sitting all alone'.",
          "retrieved_context": [
            "User has a question about cloud setup.",
            "User says multi-cloud setup is a nightmare.",
            "User is looking at a database hosted on Microsoft's cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 693.39919090271,
          "generation_latency_ms": 6798.104524612427,
          "total_latency_ms": 7491.503715515137
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which specific database resource is hosted on the platform provided by Microsoft?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "PrimaryDB7",
          "retrieved_context": [
            "User is looking at a database hosted on Microsoft's cloud.",
            "User wants specs for database running on Microsoft's cloud.",
            "User believes the Azure deployment hosts the main database."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 752.4824142456055,
          "generation_latency_ms": 6697.582006454468,
          "total_latency_ms": 7450.064420700073
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is currently designated as the Backend Lead?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know who is handling the UI team.",
            "User is asking if frontend leads are still just Olivia.",
            "Elijah is the CEO"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 795.5870628356934,
          "generation_latency_ms": 5544.39115524292,
          "total_latency_ms": 6339.978218078613
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is currently running on AuthService1?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User requests status of AuthService1 and wants to check if it is showing latency, noting that it is isolated on Google Cloud.",
            "User wants to ensure AuthService1 syncs correctly with others.",
            "Alex wants to know if AuthService1 and LoadBalancer5 share the same provider."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 637.7615928649902,
          "generation_latency_ms": 6453.782081604004,
          "total_latency_ms": 7091.543674468994
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation about the 'multi-cloud mess', what item did the user ask to be reminded to buy?",
          "answer_type": "short_answer",
          "gold_answer": "Milk",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a quick summary of the multi-cloud mess before coffee.",
            "User's multi-cloud bill is causing headache",
            "User says multi-cloud setup is a nightmare."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1505.0225257873535,
          "generation_latency_ms": 5827.640533447266,
          "total_latency_ms": 7332.663059234619
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder to call their mom before or after requesting a reminder for espresso pods?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants reminder to buy coffee beans later because they are out at home",
            "User wants a reminder to buy coffee beans before the shop closes.",
            "User wants to order more espresso pods for the office kitchen later because they are running low."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 718.008279800415,
          "generation_latency_ms": 6187.769174575806,
          "total_latency_ms": 6905.777454376221
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's greetings in the session logs, at what time of day do they typically discuss infrastructure status?",
          "answer_type": "short_answer",
          "gold_answer": "Morning",
          "predicted_answer": "Before coffee.",
          "retrieved_context": [
            "User wants to get a handle on infrastructure spread before the weekly sync, noting that the infrastructure is messy.",
            "User had a phone call with cofounder who wants status update on infrastructure migration by end of day.",
            "User needs uptime reports."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 618.7679767608643,
          "generation_latency_ms": 7157.804489135742,
          "total_latency_ms": 7776.572465896606
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Which two cloud providers does the user repeatedly confuse regarding service locations?",
          "answer_type": "short_answer",
          "gold_answer": "Azure and GCP",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User often mixes up cloud providers for PrimaryDB7",
            "User acknowledges risk of splitting clouds and multi-cloud strategy, noting that services are spread across GCP, AWS, and Azure.",
            "User says multi-cloud setup is a nightmare."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 740.9048080444336,
          "generation_latency_ms": 5386.448621749878,
          "total_latency_ms": 6127.3534297943115
        }
      ],
      "summary": {
        "overall_accuracy": 0.4,
        "total_score": 4,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 8,
            "accuracy": 0.25
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_025.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 119.94667434692383,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is identified as the user's cofounder who worries about stability?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "User's cofounder",
          "retrieved_context": [
            "User's cofounder was asking about stability earlier.",
            "User's cofounder is worried about the stability of the authentication service.",
            "User's cofounder is paranoid about uptime for the demo"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 686.1679553985596,
          "generation_latency_ms": 5474.804162979126,
          "total_latency_ms": 6160.972118377686
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the DevOps engineer who works alongside Lucas?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "Lucas is handling DevOps tickets today.",
            "Lucas and Oliver handle the ops side of things.",
            "User believes Lucas is doing a canary deployment without telling them."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 591.9923782348633,
          "generation_latency_ms": 3788.926362991333,
          "total_latency_ms": 4380.918741226196
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that is currently experiencing a version mismatch with a beta release?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "The beta version of AuthService7 was not supposed to stick.",
            "User finds multi-cloud setup a headache.",
            "AuthService7 is reporting two different versions active simultaneously on Google Cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 648.7479209899902,
          "generation_latency_ms": 3822.8063583374023,
          "total_latency_ms": 4471.554279327393
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which analytics engine is deployed on the same cloud platform as the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine3",
          "predicted_answer": "AnalyticsEngine3",
          "retrieved_context": [
            "AnalyticsEngine3 is with the auth service on GCP",
            "User wants to know where AnalyticsEngine5 was deployed.",
            "We need to unify the analytics engines"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 945.6794261932373,
          "generation_latency_ms": 4308.711767196655,
          "total_latency_ms": 5254.391193389893
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was Oliver distracted during the standup meeting mentioned in the session where the user discussed quarterly goals?",
          "answer_type": "short_answer",
          "gold_answer": "Luna knocked over water",
          "predicted_answer": "Luna knocked over his water.",
          "retrieved_context": [
            "Oliver was distracted during standup because Luna knocked over his water",
            "User is trying to track down Oliver",
            "User asks if Oliver sent a message about deployment logs"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 605.4818630218506,
          "generation_latency_ms": 4834.241628646851,
          "total_latency_ms": 5439.723491668701
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about paying invoices to every cloud provider, where was AnalyticsEngine5 stated to be hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown. The provided context does not include a conversation where the user complained about paying invoices to every cloud provider.",
          "retrieved_context": [
            "User wants to know where AnalyticsEngine5 was deployed.",
            "User wants hosting details for analytics engines.",
            "AnalyticsEngine6 on Amazon cloud is lagging, while number 5 on Azure and number 3 on GCP are stable, and Mia is trying to merge data from Engine 6 on Amazon's servers."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 600.6119251251221,
          "generation_latency_ms": 8211.66729927063,
          "total_latency_ms": 8812.279224395752
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's comments about the infrastructure map, how do they feel about the current multi-cloud setup?",
          "answer_type": "short_answer",
          "gold_answer": "dislikes it",
          "predicted_answer": "They find it a headache, a nightmare, and a mess.",
          "retrieved_context": [
            "User finds multi-cloud setup a headache.",
            "User refers to the multi-cloud setup as a nightmare",
            "User's cofounder is asking for a status update on the infrastructure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 697.3223686218262,
          "generation_latency_ms": 9005.669832229614,
          "total_latency_ms": 9702.99220085144
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is AnalyticsEngine6 currently running?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a check on AnalyticsEngine3.",
            "User wants to know where AnalyticsEngine5 was deployed.",
            "User wants hosting details for analytics engines."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 842.6151275634766,
          "generation_latency_ms": 6977.377653121948,
          "total_latency_ms": 7819.992780685425
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of CFO at Nebula Corp?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User needs to sync on Nebula's roadmap",
            "User's cofounder is stressing about quarterly goals",
            "Noah is still leading the product design team"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 630.0325393676758,
          "generation_latency_ms": 6729.960680007935,
          "total_latency_ms": 7359.99321937561
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific action did the assistant perform when the user requested to be prompted about calling the dentist?",
          "answer_type": "short_answer",
          "gold_answer": "set a reminder",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to call dentist at 2 PM.",
            "User needs to hop on a call with cofounder first.",
            "User has to jump on a call."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 637.0949745178223,
          "generation_latency_ms": 7460.111379623413,
          "total_latency_ms": 8097.206354141235
        }
      ],
      "summary": {
        "overall_accuracy": 0.6,
        "total_score": 6,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_026.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 171.83021521568298,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Will",
          "retrieved_context": [
            "Luna is a cat.",
            "User is asking about Luna's status.",
            "User mentions cat Luna walked across the keyboard during deployment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 770.8079814910889,
          "generation_latency_ms": 7725.534200668335,
          "total_latency_ms": 8496.342182159424
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the load balancer that is currently running a beta version?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks whether LoadBalancer8 is hosted on Google Cloud.",
            "User wants to know which load balancers are in Google's cloud.",
            "User wants to review load balancers hosted on Google Cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 627.741813659668,
          "generation_latency_ms": 7652.588844299316,
          "total_latency_ms": 8280.330657958984
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who had to take their pet to the vet?",
          "answer_type": "short_answer",
          "gold_answer": "DevOps",
          "predicted_answer": "DevOps lead",
          "retrieved_context": [
            "User hopes the DevOps lead's cat is okay.",
            "Will was taking Luna to the vet",
            "User mentions cat Luna walked across the keyboard during deployment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 803.0023574829102,
          "generation_latency_ms": 7144.438743591309,
          "total_latency_ms": 7947.441101074219
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which specific service is hosted on the Azure platform?",
          "answer_type": "short_answer",
          "gold_answer": "AuthService9",
          "predicted_answer": "Auth service number 9 (Auth9)",
          "retrieved_context": [
            "User wants status of auth service on Azure",
            "Auth service number 9 is on Azure.",
            "User wants to know if LoadBalancer8 is on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 641.1764621734619,
          "generation_latency_ms": 7270.427942276001,
          "total_latency_ms": 7911.604404449463
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What event caused the DevOps lead to be delayed in starting his work day?",
          "answer_type": "short_answer",
          "gold_answer": "Vet visit",
          "predicted_answer": "Unknown, but the user hopes the DevOps lead's cat is okay, implying a potential issue with the cat.",
          "retrieved_context": [
            "User will have DevOps lead fix something tomorrow.",
            "User asks if the DevOps lead is in yet.",
            "User needs to ping DevOps lead"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 710.7396125793457,
          "generation_latency_ms": 8061.226367950439,
          "total_latency_ms": 8771.965980529785
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the most recent infrastructure review, which load balancer was confirmed to still be running on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer8",
          "predicted_answer": "LoadBalancer8",
          "retrieved_context": [
            "User is asking about stability of Amazon load balancer",
            "User wants to review load balancers hosted on Google Cloud.",
            "User wants Will to look at AWS load balancer later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 672.6374626159668,
          "generation_latency_ms": 8789.772510528564,
          "total_latency_ms": 9462.409973144531
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific beverage does the user require before they can focus on infrastructure or meetings?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Double espresso or coffee.",
          "retrieved_context": [
            "User needs a double espresso before the board meeting.",
            "User needs a distraction before the status meeting",
            "User hasn't had coffee yet."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 695.7972049713135,
          "generation_latency_ms": 9119.129419326782,
          "total_latency_ms": 9814.926624298096
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's initial request in Session 25, which cloud provider's resources do they prioritize reviewing first?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "User wants to double-check infrastructure hosted on Google's cloud.",
            "User wants to start with Google Cloud resources.",
            "User is concerned about multi-cloud strategy"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 760.6661319732666,
          "generation_latency_ms": 6978.044509887695,
          "total_latency_ms": 7738.710641860962
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is currently running on LoadBalancer8?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know where LoadBalancer8 is running",
            "User wants to know if load balancer 4 is showing the new version.",
            "User asks about load balancer number 8"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 556.4272403717041,
          "generation_latency_ms": 5996.946573257446,
          "total_latency_ms": 6553.37381362915
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Lucas's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Luna is a cat.",
            "User asks what Lucas and Noah have been working on",
            "Luna is getting big"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 768.2247161865234,
          "generation_latency_ms": 7773.223638534546,
          "total_latency_ms": 8541.44835472107
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "If a load balancer flags two different versions simultaneously, what type of deployment does the user identify this as?",
          "answer_type": "short_answer",
          "gold_answer": "Canary deployment",
          "predicted_answer": "Bad rollout or unfinished deployment.",
          "retrieved_context": [
            "User is dealing with load balancers.",
            "LoadBalancer4 has conflicting version reports: v1.2.0 and v2.0.0-beta.",
            "User wants to know if load balancer 4 is showing the new version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 698.448896408081,
          "generation_latency_ms": 6292.985677719116,
          "total_latency_ms": 6991.434574127197
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific state configuration indicated to the assistant that a canary deployment was active on LoadBalancer4?",
          "answer_type": "short_answer",
          "gold_answer": "Two different versions",
          "predicted_answer": "Conflicting reports about the live version of LoadBalancer4.",
          "retrieved_context": [
            "User wants to check status of LoadBalancer4.",
            "User wants to know where LoadBalancer8 is running",
            "User wants to drill down on LoadBalancer4 and notes conflicting reports about the live version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 626.5072822570801,
          "generation_latency_ms": 7522.9432582855225,
          "total_latency_ms": 8149.4505405426025
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_027.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 25.18427562713623,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:loadbalancer_2 running version?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "Dashboard reports v1.2.0, Terminal output reports v2.0.0-beta. There is a version mismatch.",
          "retrieved_context": [
            "Dashboard reports second load balancer version v1.2.0",
            "Terminal output reports second load balancer version v2.0.0-beta",
            "User wants to check status of load balancers"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 682.5516223907471,
          "generation_latency_ms": 7058.440208435059,
          "total_latency_ms": 7740.991830825806
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User wants to know if Oliver has reviewed quarterly goals.",
            "User asks if Ava uploaded new mockups.",
            "User asks if it's raining outside."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 579.0824890136719,
          "generation_latency_ms": 7454.893112182617,
          "total_latency_ms": 8033.975601196289
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_028.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 181.18860960006714,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service where the Product Designer requires write access?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks whether AuthService9 is hosted on Google or Amazon.",
            "User says the rest of services are on Amazon, including problematic database.",
            "User is reviewing cloud spend and wants to know where core data services are located."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 785.0093841552734,
          "generation_latency_ms": 5195.86181640625,
          "total_latency_ms": 5980.871200561523
        },
        {
          "probe_id": "world_modeling-category_membership-1",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Which specific infrastructure services are currently confirmed to be running on the Google Cloud Platform?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer4 and AnalyticsEngine1",
          "predicted_answer": "Load balancer number 4.",
          "retrieved_context": [
            "Alex wants to know if analytics and fourth load balancer are still on Google cloud.",
            "User wants current infrastructure status for Nebula",
            "User asks whether AuthService9 is hosted on Google or Amazon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 672.8239059448242,
          "generation_latency_ms": 6717.378854751587,
          "total_latency_ms": 7390.202760696411
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the professional role of the individual who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Luna is a cat that is getting huge",
            "User reports that Charlotte brought her cat Luna into the office today.",
            "Charlotte's cat Luna walked across the camera during a call with sales leads"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 640.3822898864746,
          "generation_latency_ms": 8889.881610870361,
          "total_latency_ms": 9530.263900756836
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the latest system logs discussed, what is the version status of the second primary database?",
          "answer_type": "short_answer",
          "gold_answer": "Split between v1.2.0 and v2.0.0-beta",
          "predicted_answer": "PrimaryDB2 is running v2.0.0-beta alongside the old version.",
          "retrieved_context": [
            "Dashboard shows mixed signals about the update status of PrimaryDB2.",
            "Alex is unsure if upgrade finalized on second primary database.",
            "User is inquiring whether PrimaryDB2 is still on the old version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 772.040605545044,
          "generation_latency_ms": 8387.35294342041,
          "total_latency_ms": 9159.393548965454
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific technical issue is causing the user's cofounder to worry about the platform's stability?",
          "answer_type": "short_answer",
          "gold_answer": "Version mismatch on PrimaryDB2",
          "predicted_answer": "Latency.",
          "retrieved_context": [
            "User's cofounder is worried about reliability for upcoming launch.",
            "User expresses frustration about beta software in production and that Oliver will be upset if design tools lag.",
            "User believes fragmented hosting setup makes debugging a nightmare."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 876.5332698822021,
          "generation_latency_ms": 9106.992483139038,
          "total_latency_ms": 9983.52575302124
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the conversation regarding the stuck migration, did the user complain about the coffee before or after requesting permission updates for Liam and Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User has a migration headache and migration is stuck",
            "User says Liam complained about login issues.",
            "User wants to make sure Charlotte recovered slide."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 904.977560043335,
          "generation_latency_ms": 7831.241846084595,
          "total_latency_ms": 8736.21940612793
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the office machine, what is their implied preference regarding coffee strength?",
          "answer_type": "short_answer",
          "gold_answer": "Strong coffee",
          "predicted_answer": "Unknown. The user's reaction to the office machine is about its grinding noise and need for",
          "retrieved_context": [
            "User needs a double shot of espresso before looking at a screen today.",
            "User will finish coffee before it gets cold",
            "User wants dark roast"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 731.1670780181885,
          "generation_latency_ms": 8456.36248588562,
          "total_latency_ms": 9187.529563903809
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-7",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "What specific access level constraint did the user define for the Sales Leads regarding the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "Read access only",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Sales leads need read access",
            "User asks who the other sales lead is",
            "Alex wants to know about auth service."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 608.6592674255371,
          "generation_latency_ms": 7819.302082061768,
          "total_latency_ms": 8427.961349487305
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is the company's email server hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks whether AuthService9 is hosted on Google or Amazon.",
            "User says the rest of services are on Amazon, including problematic database.",
            "User asks whether PrimaryDB2 is hosted on Amazon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 934.8480701446533,
          "generation_latency_ms": 7961.473226547241,
          "total_latency_ms": 8896.321296691895
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the Sales Lead named Liam?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown. The context states Luna is Charlotte's cat, and Liam is helping wrangle her. It does not mention a pet owned by Liam.",
          "retrieved_context": [
            "User mentions Liam as a sales lead",
            "User asks who the other sales lead is",
            "Liam and Charlotte make a good sales team."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 630.8131217956543,
          "generation_latency_ms": 8600.422620773315,
          "total_latency_ms": 9231.23574256897
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-10",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What follow-up action did the assistant commit to regarding the Product Designer's mockups?",
          "answer_type": "short_answer",
          "gold_answer": "Ping him",
          "predicted_answer": "Unknown. The context does not state any follow-up action the assistant committed to regarding the Product Designer's mockups.",
          "retrieved_context": [
            "User will review mockups with Mia later if she's free.",
            "User says to leave the designer alone for now.",
            "User will review new UI mocks with Oliver in about an hour"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 616.8999671936035,
          "generation_latency_ms": 8092.399835586548,
          "total_latency_ms": 8709.299802780151
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific system mechanism does the assistant use to verify permissions for services hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "IAM roles",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to check if AuthService9 has errors on AWS.",
            "Auth service and primary databases are on AWS.",
            "User asks whether AuthService9 is hosted on Google or Amazon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 729.2768955230713,
          "generation_latency_ms": 7925.3129959106445,
          "total_latency_ms": 8654.589891433716
        }
      ],
      "summary": {
        "overall_accuracy": 0.16666666666666666,
        "total_score": 2,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 12,
            "accuracy": 0.16666666666666666
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_029.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 555.3938343524933,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that the user mentions is paranoid about vendor lock-in?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User's cofounder asked about cloud redundancy and is paranoid about vendor lock-in.",
            "Cofounder hates being out of the loop",
            "Alex's cofounder asks why we don't use one provider."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 834.059476852417,
          "generation_latency_ms": 6797.477006912231,
          "total_latency_ms": 7631.536483764648
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider's infrastructure are the databases PrimaryDB6 and PrimaryDB7 hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "PrimaryDB7 and PrimaryDB6 hosted on Azure are struggling under load.",
            "Alex wants confirmation of primary database hosting.",
            "PrimaryDB2 is stuck on Google's cloud and adding latency."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 612.1158599853516,
          "generation_latency_ms": 3944.3745613098145,
          "total_latency_ms": 4556.490421295166
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which specific service hosted on AWS is currently reporting conflicting version headers?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "The main AWS cache service.",
          "retrieved_context": [
            "User is seeing weird behavior on the caching service hosted on AWS.",
            "User is seeing conflicting logs on the main AWS cache service.",
            "User wants to check the cache service on AWS; Elijah mentioned he was messing with the deployment manifest."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 655.2979946136475,
          "generation_latency_ms": 4067.141532897949,
          "total_latency_ms": 4722.439527511597
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "The user implies a hard constraint against running which software version in the production environment?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "The beta version (specifically v2.0.0-beta).",
          "retrieved_context": [
            "User expects production to be on v1.2.0, not beta",
            "User thinks only Microsoft instances are problematic.",
            "User says Ava thinks the versioning is messed up."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 893.2709693908691,
          "generation_latency_ms": 3911.652088165283,
          "total_latency_ms": 4804.923057556152
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the session immediately following the discussion about the failed blue-green deployment, what personal task did the user ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "To pick up espresso beans later.",
          "retrieved_context": [
            "User believes the issue is a failed blue-green deployment and that the DevOps lead is either already looking at it or distracted by her cat.",
            "Alex wants a reminder to update his cofounder on infrastructure spend later",
            "User will ping the DevOps lead about the dashboard flickering."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 837.8500938415527,
          "generation_latency_ms": 4837.399482727051,
          "total_latency_ms": 5675.2495765686035
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What deployment strategy did the user suspect caused the split-brain situation on the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Blue-green deployment",
          "predicted_answer": "Accidental deployment of the beta branch by Ava.",
          "retrieved_context": [
            "User observes split-brain versioning on caching service",
            "User is concerned about the caching layer on the fourth node, which usually gives trouble.",
            "Alex is worried about the cache layer"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 605.121374130249,
          "generation_latency_ms": 4237.777471542358,
          "total_latency_ms": 4842.898845672607
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's repeated requests across sessions, what specific beverage is essential to their morning routine?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Espresso (or double espresso).",
          "retrieved_context": [
            "User wants a reminder to pick up espresso beans later.",
            "User needs another coffee before tackling board slides",
            "User has a double espresso."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 566.8783187866211,
          "generation_latency_ms": 3813.2669925689697,
          "total_latency_ms": 4380.145311355591
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user approve of running the beta version of the cache layer in the production environment?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, the user expects production to be on v1.2.0, not beta.",
          "retrieved_context": [
            "Alex is aware that the beta version of the cache service was not supposed to hit the production environment yet.",
            "User asks if CacheLayer4 is still running on Amazon.",
            "Alex is worried about the cache layer"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 743.2346343994141,
          "generation_latency_ms": 4437.95371055603,
          "total_latency_ms": 5181.188344955444
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is PrimaryDB2 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex wants confirmation of primary database hosting.",
            "PrimaryDB2 is stuck on Google's cloud and adding latency.",
            "PrimaryDB7 and PrimaryDB6 hosted on Azure are struggling under load."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 627.7782917022705,
          "generation_latency_ms": 8677.51669883728,
          "total_latency_ms": 9305.29499053955
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex says the sales lead is relentless",
            "User does not want Sales Lead to freak out before his call",
            "User wants sales figures from Liam."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 641.3979530334473,
          "generation_latency_ms": 7823.567152023315,
          "total_latency_ms": 8464.965105056763
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Which team lead was identified as requiring the beta version of the cache layer for a new UI?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "Alex wonders if the cache issue is affecting the UI",
            "Alex is aware that the beta version of the cache service was not supposed to hit the production environment yet.",
            "CacheLayer4 version is conflicting: Elijah says v1.2.0, Alex sees traces of v2.0.0 beta"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 827.6779651641846,
          "generation_latency_ms": 3084.401845932007,
          "total_latency_ms": 3912.0798110961914
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user requested a health check on the 'databases hosted on Azure', which specific system identifiers did the assistant report on?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7 and PrimaryDB6",
          "predicted_answer": "PrimaryDB7 and PrimaryDB6.",
          "retrieved_context": [
            "Alex wants a quick health check on databases hosted on Azure",
            "Databases are parked in Azure",
            "User asks about Azure instances performance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 657.6876640319824,
          "generation_latency_ms": 4260.100364685059,
          "total_latency_ms": 4917.788028717041
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_030.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 26,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 14.662556886672974,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:loadbalancer_3 hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks about load balancer on AWS behavior.",
            "Load balancer 7 is on AWS",
            "User is asking about location of LoadBalancer7"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 681.5226078033447,
          "generation_latency_ms": 3838.7889862060547,
          "total_latency_ms": 4520.311594009399
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User has a hike planned tomorrow",
            "User plans to catch Sophia later",
            "User asks if the weather report changed for this afternoon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 680.7386875152588,
          "generation_latency_ms": 4015.3698921203613,
          "total_latency_ms": 4696.10857963562
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_031.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 13.766170501708984,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:user has role?",
          "answer_type": "short_answer",
          "gold_answer": "CTO",
          "predicted_answer": "Unknown. The context describes actions and questions by \"User\" but does not explicitly state their",
          "retrieved_context": [
            "User is trying to figure that out.",
            "Will needs AuthService8 stable for his data science models today",
            "Liam just messaged User."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 655.1125049591064,
          "generation_latency_ms": 4697.640657424927,
          "total_latency_ms": 5352.753162384033
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User is thinking about golfing.",
            "User is worried about cloud spend this month.",
            "Noah was asking about quarterly projections earlier this morning."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 740.3521537780762,
          "generation_latency_ms": 3089.7936820983887,
          "total_latency_ms": 3830.145835876465
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_032.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 29,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 66.347971200943,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning their 'cofounder' who is concerned about quarterly report data?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Oliver",
          "retrieved_context": [
            "User says cofounder will flip if quarterly reports are wrong because of version drift",
            "User has a sync later with cofounder",
            "User is a founder"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 567.0039653778076,
          "generation_latency_ms": 3371.934652328491,
          "total_latency_ms": 3938.938617706299
        },
        {
          "probe_id": "world_modeling-category_membership-2",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Among the discussed systems, which service is the outlier regarding its hosting provider compared to the primary databases?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer1",
          "predicted_answer": "CacheLayer1 is on Google Cloud, while the primary databases are on Amazon.",
          "retrieved_context": [
            "User states that primary databases 4 and 7 are on Amazon.",
            "User feels hosting strategy is fragmented.",
            "The split between cloud services is causing latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 612.8213405609131,
          "generation_latency_ms": 3416.881322860718,
          "total_latency_ms": 4029.702663421631
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the team member who holds the role of Product Designer?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "Olivia posted a photo of her pet sleeping on her keyboard during a design review.",
            "User asks if product designer sent new mockups",
            "User wants to know the cat's name."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 573.0912685394287,
          "generation_latency_ms": 3773.944139480591,
          "total_latency_ms": 4347.0354080200195
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the system signals discussed in the latest sessions, which two versions of the analytics engine are running simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "1.2.0 and 2.0 beta",
          "retrieved_context": [
            "User is asking about the rollout status and current version of the third analytics engine.",
            "There is a version mismatch on the third analytics engine, showing both 1.2.0 and 2.0 beta.",
            "User wants to double-check which beta version was on the analytics engine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 779.240608215332,
          "generation_latency_ms": 2762.2954845428467,
          "total_latency_ms": 3541.5360927581787
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event caused the delay in the Product Designer uploading the new assets?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat knocked over a plant",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if product designer sent new mockups",
            "Mia complained about query latency for her models.",
            "User wants to know if Olivia uploaded new UI assets."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 527.4677276611328,
          "generation_latency_ms": 3702.6779651641846,
          "total_latency_ms": 4230.145692825317
        },
        {
          "probe_id": "temporal_episodic-sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Google Cloud latency occur before or after the user first mentioned the dual-version issue with the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Before.",
          "retrieved_context": [
            "User reports one cache layer is stranded on Google's cloud",
            "The split between cloud services is causing latency.",
            "Cache layer is experiencing high latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 671.722412109375,
          "generation_latency_ms": 4550.2190589904785,
          "total_latency_ms": 5221.9414710998535
        },
        {
          "probe_id": "preference_learning-induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about the infrastructure, what architectural arrangement does the user implicitly dislike?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hosting",
          "predicted_answer": "Multi-cloud architecture or fragmented hosting strategy.",
          "retrieved_context": [
            "Liam hates the multi-cloud setup because of latency",
            "User feels hosting strategy is fragmented.",
            "User asks about the status of the infrastructure board"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 630.9373378753662,
          "generation_latency_ms": 4298.702239990234,
          "total_latency_ms": 4929.639577865601
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is the company's email server hosted on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if the box is still on Amazon's cloud",
            "User asks if AnalyticsEngine3 instance is hosted on Amazon",
            "User knows that CacheLayer1 is hosted on Google's cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 574.5813846588135,
          "generation_latency_ms": 3478.9817333221436,
          "total_latency_ms": 4053.563117980957
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the current VP of Marketing?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User acknowledges Liam as the backend lead.",
            "User is asking about the rollout status and current version of the third analytics engine.",
            "User asks if product designer sent new mockups"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 708.6489200592041,
          "generation_latency_ms": 4074.6474266052246,
          "total_latency_ms": 4783.296346664429
        },
        {
          "probe_id": "procedural_knowledge-task_state-10",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "Why is the DevOps lead currently unable to assist with migrating the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Swamped with deployment pipeline",
          "predicted_answer": "Will is swamped with the deployment pipeline.",
          "retrieved_context": [
            "User may need Will to look into migrating CacheLayer1, but Will is swamped with the deployment pipeline.",
            "User has not migrated the cache layer yet.",
            "User feels the cache layer isn't integrated with the rest of the stack."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 512.6683712005615,
          "generation_latency_ms": 2896.0635662078857,
          "total_latency_ms": 3408.7319374084473
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 5,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 8,
            "accuracy": 0.375
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_033.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 29,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 81.76580262184143,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "User will let cofounder know.",
            "User's cofounder asked about latency.",
            "User asks if the cofounder approved the budget for the consolidation project."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 691.8189525604248,
          "generation_latency_ms": 3992.602586746216,
          "total_latency_ms": 4684.421539306641
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider is the service managed by the backend lead testing the beta version hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User asks which cloud the auth service is on.",
            "Will is testing the beta version of the cache layer, which is on Azure.",
            "User wants to know whether Lucas deployed the beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 680.1881790161133,
          "generation_latency_ms": 3603.332996368408,
          "total_latency_ms": 4283.5211753845215
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the DevOps team member who was worried about the update?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Mia was worried the update didn't go through.",
            "User asks about Lucas, the other DevOps person, who handles deployments for that sector",
            "User wants to know which DevOps team members are available to fix the split-brain situation."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 616.7597770690918,
          "generation_latency_ms": 3859.7373962402344,
          "total_latency_ms": 4476.497173309326
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What specific two-word phrase did the user use to describe the situation where the cache layer was reporting two different version headers?",
          "answer_type": "verbatim",
          "gold_answer": "split-brain situation",
          "predicted_answer": "two versions",
          "retrieved_context": [
            "User sees weird logs from CacheLayer5 and suspects a version mismatch causing issues, asking if fully on beta.",
            "CacheLayer5 is showing two different versions.",
            "User will ping the team about the cache layer issue"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 778.5413265228271,
          "generation_latency_ms": 4892.360687255859,
          "total_latency_ms": 5670.9020137786865
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder about the dentist appointment before or after asking for the dry cleaning reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The user stated they forgot about the dentist appointment, but did not ask for a reminder *about* the appointment itself. They later asked for a reminder to *call* the dentist.",
          "retrieved_context": [
            "User wants a reminder to pick up dry cleaning later.",
            "User wants a reminder to pick up dry cleaning at 5 PM.",
            "User forgot about the dentist appointment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 670.7165241241455,
          "generation_latency_ms": 4751.317739486694,
          "total_latency_ms": 5422.03426361084
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user suspect a version mismatch was causing issues with the dashboard?",
          "answer_type": "short_answer",
          "gold_answer": "Lag / Slowness",
          "predicted_answer": "The user suspected a version mismatch was causing issues because they were seeing weird logs from CacheLayer5.",
          "retrieved_context": [
            "User says the dashboard gives conflicting info.",
            "Mia from ops thinks there is a version mismatch; she sees logs for v2.0.0-beta while documentation says we should still be on v1.2.0.",
            "User had a long strategy session with cofounder and they are trying to figure out why the dashboard is acting up."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 646.6491222381592,
          "generation_latency_ms": 3816.8203830718994,
          "total_latency_ms": 4463.469505310059
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on previous orders, what food does the user prefer for team lunches?",
          "answer_type": "short_answer",
          "gold_answer": "Pizza",
          "predicted_answer": "Unknown. The context only mentions the user wants a reminder to order pizza for the team later, not what has been ordered previously.",
          "retrieved_context": [
            "User wants a reminder to order pizza lunch for the team later.",
            "User wants to book a table for lunch tomorrow at an Italian place downtown at 12:30.",
            "User wants a reminder to book a table for lunch at noon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 546.6907024383545,
          "generation_latency_ms": 3226.2723445892334,
          "total_latency_ms": 3772.963047027588
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to message Mia personally about the update or have the assistant do it?",
          "answer_type": "short_answer",
          "gold_answer": "Personally",
          "predicted_answer": "The user prefers to message Mia personally about the update.",
          "retrieved_context": [
            "User plans to message Mia",
            "User wants a reminder to ask Mia how Luna is doing",
            "User wants reminder to message Mia after grabbing coffee."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 667.8447723388672,
          "generation_latency_ms": 3763.8614177703857,
          "total_latency_ms": 4431.706190109253
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of James's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to start with James.",
            "James owes User lunch.",
            "User will ask James, who leads the backend team, to look into database queries."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 594.3360328674316,
          "generation_latency_ms": 3270.8346843719482,
          "total_latency_ms": 3865.17071723938
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is running on the primary database hosted on Azure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to check status of the primary DB in Azure.",
            "User wants to know location of primary database not on Azure.",
            "User wants performance of other database on Azure"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 859.8530292510986,
          "generation_latency_ms": 3427.8974533081055,
          "total_latency_ms": 4287.750482559204
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-10",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Who are the two individuals usually responsible for handling infrastructure tickets?",
          "answer_type": "short_answer",
          "gold_answer": "Mia and Lucas",
          "predicted_answer": "Mia and Lucas",
          "retrieved_context": [
            "Mia and Lucas are handling the infrastructure pipelines",
            "User is looking at the infrastructure map",
            "User wants to know which DevOps team members are available to fix the split-brain situation."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 673.7706661224365,
          "generation_latency_ms": 3869.447708129883,
          "total_latency_ms": 4543.218374252319
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who should be contacted regarding issues with the backend beta push?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Lucas, Mia (DevOps), and James.",
          "retrieved_context": [
            "User asks if someone pushed the beta to production.",
            "User claims the beta was rolled back yesterday.",
            "User wants to know whether Lucas deployed the beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 617.4073219299316,
          "generation_latency_ms": 4330.857515335083,
          "total_latency_ms": 4948.264837265015
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_034.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 74.97472882270813,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User identifies Sophia as the DevOps lead.",
            "User's cofounder is pressuring user to fix issue before demo.",
            "User was on a call with cofounder until late."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 651.4937877655029,
          "generation_latency_ms": 2828.7746906280518,
          "total_latency_ms": 3480.2684783935547
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member is the owner of the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "The cat Luna is the real boss of the design team.",
            "User asks whether Mia found a vet for Luna.",
            "User wants to know what kind of treats Luna likes"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 351.44805908203125,
          "generation_latency_ms": 3501.0104179382324,
          "total_latency_ms": 3852.4584770202637
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which infrastructure component is hosted on a different cloud provider than the Primary Database and Cache Layer?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer8",
          "predicted_answer": "LoadBalancer8",
          "retrieved_context": [
            "User feels disjointed because PrimaryDB3 and CacheLayer4 are on Google's cloud.",
            "User is annoyed that LoadBalancer8 is hosted on AWS while the primary database and cache layer are on GCP.",
            "Alex wants to check which cloud provider is handling traffic ingress and feels latency is slightly off."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 670.4354286193848,
          "generation_latency_ms": 3190.0603771209717,
          "total_latency_ms": 3860.4958057403564
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Is the authentication service currently running a single version or multiple versions simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Multiple versions",
          "predicted_answer": "Multiple versions are running simultaneously (v1.2.0 and v2.0.0-",
          "retrieved_context": [
            "User asks what is running on auth service.",
            "User wants to know what version of the auth service is running.",
            "User's cofounder is asking about authentication service versioning and believes the version mismatch on the login service is causing bugs."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 626.3418197631836,
          "generation_latency_ms": 4126.538276672363,
          "total_latency_ms": 4752.880096435547
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the production logs, what is the primary architectural cause of the observed latency?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hop",
          "predicted_answer": "Cross-cloud latency due to traffic ingress on Amazon while heavy lifting occurs elsewhere.",
          "retrieved_context": [
            "User is experiencing latency on the edge",
            "Oliver complains that latency is killing his sales pitches, and the user is considering consolidation",
            "Alex wants to check which cloud provider is handling traffic ingress and feels latency is slightly off."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 817.3375129699707,
          "generation_latency_ms": 4196.387052536011,
          "total_latency_ms": 5013.724565505981
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did Luna knock over a plant before or after she was reported getting stuck in a box?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User says Luna is poor",
            "User says Mia is probably dealing with a domestic crisis and that Luna knocked a coffee mug onto Mia's laptop.",
            "Luna jumped onto Mia's keyboard during a video call."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 522.2272872924805,
          "generation_latency_ms": 3234.8732948303223,
          "total_latency_ms": 3757.1005821228027
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the frontend service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a quick rundown of where main backend services are currently hosted.",
            "User asks whether load balancer is still on Amazon's cloud.",
            "User wants confirmation of where the ingress point is hosted."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 603.6231517791748,
          "generation_latency_ms": 3252.185344696045,
          "total_latency_ms": 3855.8084964752197
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the Sales Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to tell Sales Lead not to worry.",
            "The Sales Lead wants the shiny new thing immediately, does not have to carry the pager, and is promising unfinished features.",
            "User identifies Sophia as the DevOps lead."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 568.0568218231201,
          "generation_latency_ms": 3167.451858520508,
          "total_latency_ms": 3735.508680343628
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user prefer maintaining the current multi-cloud setup or consolidating the infrastructure?",
          "answer_type": "short_answer",
          "gold_answer": "Consolidating",
          "predicted_answer": "Consolidating the infrastructure.",
          "retrieved_context": [
            "User is shifting to infrastructure.",
            "User asks whether load balancer is still on Amazon's cloud.",
            "User wants to hold off on changing the mixed deployment for now"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 836.7137908935547,
          "generation_latency_ms": 4015.0306224823,
          "total_latency_ms": 4851.7444133758545
        },
        {
          "probe_id": "preference_learning-preference_induction-2",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user feel about the topology map showing the load balancer on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "Annoyed",
          "predicted_answer": "The user is annoyed that the topology map shows LoadBalancer8 on AWS while the rest of the stack is on GCP, and finds the map itself \"hurting their eyes\" and \"fragmented.\"",
          "retrieved_context": [
            "User asks whether load balancer is still on Amazon's cloud.",
            "User needs to explain why paying for AWS load balancer while rest of stack is on Google",
            "User notes that LoadBalancer8 looks out of place and is inquiring why it is on Amazon's infrastructure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 679.1238784790039,
          "generation_latency_ms": 3711.270809173584,
          "total_latency_ms": 4390.394687652588
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What general observation did the assistant make regarding the nature of architectures that span AWS and GCP?",
          "answer_type": "short_answer",
          "gold_answer": "They can be complex",
          "predicted_answer": "Unknown. The provided context does not contain any general observations made by the assistant regarding architectures that span AWS",
          "retrieved_context": [
            "User needs to explain why paying for AWS load balancer while rest of stack is on Google",
            "User is annoyed that LoadBalancer8 is hosted on AWS while the primary database and cache layer are on GCP.",
            "User asks whether load balancer is still on Amazon's cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 576.3516426086426,
          "generation_latency_ms": 4008.7122917175293,
          "total_latency_ms": 4585.063934326172
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user mentioned buying treats for Luna, what action did the assistant confirm taking?",
          "answer_type": "short_answer",
          "gold_answer": "Added a reminder",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know what kind of treats Luna likes",
            "User wants a reminder to pick up treats for Luna on their way home.",
            "User asks whether Mia found a vet for Luna."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 834.613561630249,
          "generation_latency_ms": 2973.388433456421,
          "total_latency_ms": 3808.00199508667
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 10,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_035.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 36,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 62.300779581069946,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention a meeting with their cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "User has a meeting with cofounder in an hour",
            "User is asking if the cofounder mentioned lunch today.",
            "User's cofounder has impeccable timing."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 625.701904296875,
          "generation_latency_ms": 2737.5521659851074,
          "total_latency_ms": 3363.2540702819824
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "Sales Lead has a new cat named Luna who is tearing up furniture.",
            "User thinks Sales is taking a break.",
            "User is asking whether the sales lead has sent projections."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 346.54998779296875,
          "generation_latency_ms": 3210.306406021118,
          "total_latency_ms": 3556.856393814087
        },
        {
          "probe_id": "preference_learning-preference_induction-3",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routines, what beverage do they require to function properly?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Espresso",
          "retrieved_context": [
            "User needs another espresso before functioning properly today.",
            "User needs to grab coffee soon",
            "User wants a reminder to grab coffee before debugging"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 756.5708160400391,
          "generation_latency_ms": 3254.8563480377197,
          "total_latency_ms": 4011.427164077759
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-4",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "On which cloud provider is the primary database hosted?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know where the auth service is hosted (AWS or GCP).",
            "CacheLayer6 is hosted on Amazon's servers.",
            "User requests status of caching service hosted on Amazon and asks if CacheLayer6 is the only thing strictly on Amazon's servers."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 817.941427230835,
          "generation_latency_ms": 3379.631519317627,
          "total_latency_ms": 4197.572946548462
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-5",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "When the user requests the 'morning status checks', which service is verified immediately after the load balancers?",
          "answer_type": "short_answer",
          "gold_answer": "AuthService9",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks if authentication service is included in load balancer checks.",
            "User wants to start traffic management checks and asks if both load balancers are still in Google cloud.",
            "User feels latency on load balancers is creeping up again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 512.3491287231445,
          "generation_latency_ms": 3424.8340129852295,
          "total_latency_ms": 3937.183141708374
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user mentioned being 'barely awake', what specific data were they preparing to review with Mia?",
          "answer_type": "short_answer",
          "gold_answer": "Quarterly figures",
          "predicted_answer": "Unknown. The context does not state that the user mentioned being 'barely awake'. However, the user wants to pull up the infrastructure dashboard to double-check a few things before syncing with Mia.",
          "retrieved_context": [
            "User asks if Mia has come in yet.",
            "User wants to update Mia before the board meeting.",
            "Alex has a meeting with Mia first."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 740.5211925506592,
          "generation_latency_ms": 3953.4358978271484,
          "total_latency_ms": 4693.957090377808
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-7",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Given that the authentication service is co-located with the load balancers, which cloud provider hosts it?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud Platform (GCP)",
          "retrieved_context": [
            "User asks if authentication service is included in load balancer checks.",
            "User needs to explain that load balancers and auth service are hosted on Google's platform.",
            "User wants to know where the auth service is hosted (AWS or GCP)."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 562.7932548522949,
          "generation_latency_ms": 3545.1223850250244,
          "total_latency_ms": 4107.915639877319
        },
        {
          "probe_id": "world_modeling-relationship_mapping-8",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Sales Lead has a new cat named Luna who is tearing up furniture.",
            "Olivia's cat Luna jumped on keyboard during client call",
            "Cat Luna belongs to Olivia"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 650.1269340515137,
          "generation_latency_ms": 4268.510580062866,
          "total_latency_ms": 4918.63751411438
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the single definitive running version of CacheLayer6?",
          "answer_type": "short_answer",
          "gold_answer": "Ambiguous",
          "predicted_answer": "CacheLayer6 is reporting versions v1.2.0 and v2.0.0-beta at the same time. There is no single definitive running version.",
          "retrieved_context": [
            "User wants to migrate CacheLayer6 eventually.",
            "User says the beta version of CacheLayer6 was not supposed to go live yet.",
            "User is asking if CacheLayer6 is fully stable and knows Oliver might have pushed experimental code to backend and asks if CacheLayer6 is the only thing strictly on Amazon's servers."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 579.0739059448242,
          "generation_latency_ms": 3356.118679046631,
          "total_latency_ms": 3935.192584991455
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-10",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the session where the user reviewed architecture diagrams, did they ask about the load balancers before or after asking for a lunch reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context does not mention a lunch reminder.",
          "retrieved_context": [
            "User is reviewing architecture diagrams before a weekly sync with their cofounder.",
            "User asks if both load balancers are handling traffic evenly.",
            "User feels latency on load balancers is creeping up again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 615.9038543701172,
          "generation_latency_ms": 3443.986654281616,
          "total_latency_ms": 4059.8905086517334
        }
      ],
      "summary": {
        "overall_accuracy": 0.3,
        "total_score": 3,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 9,
            "accuracy": 0.2222222222222222
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_036.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 11,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 64.98711848258972,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to by the user as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "User had a phone call with cofounder James",
            "User's cofounder asked about stability metrics.",
            "User has a sync with cofounder James in an hour"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 751.5032291412354,
          "generation_latency_ms": 3107.9959869384766,
          "total_latency_ms": 3859.499216079712
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What professional role is shared by both Emma and Mia?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Leading sales efforts.",
          "retrieved_context": [
            "User asks whether Emma and Mia share responsibilities.",
            "Emma and Mia have been complaining all week",
            "User is asking whether Emma or Mia sent quarterly figures."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 920.9907054901123,
          "generation_latency_ms": 2724.731922149658,
          "total_latency_ms": 3645.7226276397705
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two specific cloud providers are generating the high egress fees due to cross-cloud traffic?",
          "answer_type": "short_answer",
          "gold_answer": "Azure and AWS",
          "predicted_answer": "Google and Azure.",
          "retrieved_context": [
            "Alex is concerned about infrastructure costs from using three different clouds: auth service on Google, main database on Azure",
            "User says traffic ingress is handled by Amazon.",
            "User is concerned about multi-cloud architecture."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 312.81113624572754,
          "generation_latency_ms": 4011.413335800171,
          "total_latency_ms": 4324.224472045898
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which infrastructure component is violating the production stability constraint by running a beta version?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer5",
          "predicted_answer": "CacheLayer5 (or the cache fleet/nodes).",
          "retrieved_context": [
            "User told them not to mix the beta in production.",
            "v2.0.0-beta is leaking into production",
            "User says beta version shouldn't be live yet"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 666.5031909942627,
          "generation_latency_ms": 4068.2647228240967,
          "total_latency_ms": 4734.767913818359
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific disruption involving a pet occurred during the frontend sync meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Olivia's cat Luna jumped on her keyboard.",
          "retrieved_context": [
            "User believes the frontend lead is busy with Luna the cat, which is high maintenance.",
            "User wants to ask the frontend lead about a UI glitch.",
            "User needs to schedule a sync with the frontend team later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 573.5337734222412,
          "generation_latency_ms": 2601.2067794799805,
          "total_latency_ms": 3174.7405529022217
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the user specifically worried about James discovering the current state of the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Testing beta in production",
          "predicted_answer": "James already thinks the cache layer is acting up, and the user is about to inform him of a version mismatch on the cache.",
          "retrieved_context": [
            "James thinks the cache layer is acting up.",
            "User is looking at the caching layer.",
            "User is concerned about CacheLayer5"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 954.3731212615967,
          "generation_latency_ms": 3409.961938858032,
          "total_latency_ms": 4364.335060119629
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the company's email server?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Nebula Corp has a multi-cloud setup.",
            "Alex is concerned about infrastructure costs from using three different clouds: auth service on Google, main database on Azure",
            "User is concerned about multi-cloud architecture."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 636.3232135772705,
          "generation_latency_ms": 2686.617851257324,
          "total_latency_ms": 3322.9410648345947
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the specific version number currently running on the PrimaryDB5 instance?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is worried about latency on PrimaryDB5",
            "User wants to check the main database.",
            "User wants to know what versions are running."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 379.6515464782715,
          "generation_latency_ms": 2963.7670516967773,
          "total_latency_ms": 3343.418598175049
        },
        {
          "probe_id": "preference_learning-preference_induction-9",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When the assistant offered to draft a message to James regarding the version mismatch, how did the user prefer to handle the communication?",
          "answer_type": "short_answer",
          "gold_answer": "Handle it personally",
          "predicted_answer": "The user preferred to handle James himself.",
          "retrieved_context": [
            "User will let James know about the version mismatch on the cache.",
            "User has a sync with cofounder James in an hour",
            "James thinks we might have deployed the wrong version."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 579.2784690856934,
          "generation_latency_ms": 3139.0130519866943,
          "total_latency_ms": 3718.2915210723877
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the user's routine, what is the required duration for steeping tea?",
          "answer_type": "short_answer",
          "gold_answer": "Ten minutes",
          "predicted_answer": "Ten minutes.",
          "retrieved_context": [
            "User wants a timer set for ten minutes to steep tea.",
            "User already drank tea.",
            "User's coffee hasn't kicked in yet"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 797.1477508544922,
          "generation_latency_ms": 2394.8302268981934,
          "total_latency_ms": 3191.9779777526855
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Based on the user's discussions about the cloud bill, what architectural choice is identified as the primary cause of high costs?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud data transfer",
          "predicted_answer": "Multi-cloud architecture.",
          "retrieved_context": [
            "Alex is concerned about infrastructure costs from using three different clouds: auth service on Google, main database on Azure",
            "User is concerned about multi-cloud architecture.",
            "User wants to pull up the infrastructure map to explain the cloud bill to James later today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 423.71273040771484,
          "generation_latency_ms": 3904.8943519592285,
          "total_latency_ms": 4328.607082366943
        }
      ],
      "summary": {
        "overall_accuracy": 0.5454545454545454,
        "total_score": 6,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_037.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 72.76232862472534,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific individual is the user referring to as their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "James is user's cofounder.",
            "User had a long sync with cofounder James",
            "User is asking if the cofounder said anything about the board deck."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 565.558910369873,
          "generation_latency_ms": 2939.645528793335,
          "total_latency_ms": 3505.204439163208
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member flagged the latency issues on the database hosted on Google Cloud?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia, backend lead",
          "retrieved_context": [
            "Olivia, backend lead, flagged heavy latency on main database in Google Cloud environment",
            "User is worried about database latency",
            "User is looking for the database running on Google Cloud"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 616.6837215423584,
          "generation_latency_ms": 2655.080795288086,
          "total_latency_ms": 3271.7645168304443
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite earlier conflicting headers, what version is the GCP load balancer confirmed to be running after the update?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a status check of the new GCP load balancer.",
            "User feels there might be a mismatch in versioning on the GCP balancer",
            "User is asking if LoadBalancer5 is hosted on GCP"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 345.06726264953613,
          "generation_latency_ms": 3395.6480026245117,
          "total_latency_ms": 3740.715265274048
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the 'segregated' load balancer hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is interested in the other load balancer that hasn't migrated from Microsoft's cloud yet",
            "User mentioned that the load balancer is hosted on Google Cloud.",
            "User wants to identify the load balancer on Microsoft cloud that Will was asking about."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 620.2051639556885,
          "generation_latency_ms": 2647.897958755493,
          "total_latency_ms": 3268.1031227111816
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific action taken by the DevOps lead caused LoadBalancer9 to switch to the beta version?",
          "answer_type": "short_answer",
          "gold_answer": "pushed an update",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Olivia noticed a version mismatch on LoadBalancer9",
            "LoadBalancer9 is flickering between v1.2.0 and v2.0.0-beta",
            "User intends to flag a potential issue with LoadBalancer9 version conflict"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 387.5772953033447,
          "generation_latency_ms": 3450.662851333618,
          "total_latency_ms": 3838.240146636963
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request the reminder to water plants before or after the reminder to pick up dry cleaning mentioned in Session llm-22?",
          "answer_type": "short_answer",
          "gold_answer": "after",
          "predicted_answer": "Before.",
          "retrieved_context": [
            "User wants a reminder to water the plants in the office lobby later today",
            "User wants a reminder to pick up dry cleaning later",
            "User wants reminder to text James about board meeting prep"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 588.5303020477295,
          "generation_latency_ms": 3315.1893615722656,
          "total_latency_ms": 3903.719663619995
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When preparing reports for the CEO, what specific metric does he prefer to focus on?",
          "answer_type": "short_answer",
          "gold_answer": "uptime",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cofounder is pinging about the quarterly review and is hyper-focused on infrastructure costs.",
            "User's cofounder is anxious about the quarterly review",
            "User wants to provide an answer to cofounder about dashboard weirdness."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 704.1323184967041,
          "generation_latency_ms": 3377.192497253418,
          "total_latency_ms": 4081.324815750122
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-1",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "The user is nervous about beta versions but accepts them under what condition?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia signed off",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User believes beta version should not be live yet.",
            "User's cofounder will flip if he sees beta in live environment",
            "User thinks the beta deployment is dangerous and believes Will deployed it by accident."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 655.2975177764893,
          "generation_latency_ms": 3719.7937965393066,
          "total_latency_ms": 4375.091314315796
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Product Designer named Lucas?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Lucas is obsessed with Luna",
            "User says Lucas is covering design work for the sprint.",
            "Lucas wanted to finish product review."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 649.5785713195801,
          "generation_latency_ms": 4114.9091720581055,
          "total_latency_ms": 4764.487743377686
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the Azure-hosted load balancer currently running?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User wants to know which load balancer is on Azure.",
            "User asks whether Azure load balancer shows similar version conflicts.",
            "User is asking if the legacy Azure balancer is still running."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 804.0833473205566,
          "generation_latency_ms": 4191.225051879883,
          "total_latency_ms": 4995.308399200439
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What condition does the Backend Lead typically verify before allowing a rollout?",
          "answer_type": "short_answer",
          "gold_answer": "backend is solid",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User intends to loop in backend lead to ensure APIs sync across clouds.",
            "User is asking whether the update was rolled out to the main load balancer.",
            "User's cofounder will flip if he sees beta in live environment"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 652.5096893310547,
          "generation_latency_ms": 3496.7682361602783,
          "total_latency_ms": 4149.277925491333
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant determine the version conflict on LoadBalancer9?",
          "answer_type": "short_answer",
          "gold_answer": "checking logs",
          "predicted_answer": "Olivia noticed a version mismatch on LoadBalancer9, which is flickering between v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "User intends to flag a potential issue with LoadBalancer9 version conflict",
            "User asks whether Azure load balancer shows similar version conflicts.",
            "Olivia noticed a version mismatch on LoadBalancer9"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 793.6902046203613,
          "generation_latency_ms": 3192.7876472473145,
          "total_latency_ms": 3986.477851867676
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 12,
            "accuracy": 0.25
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_038.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 76.8669753074646,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which executive team member is reportedly concerned about the infrastructure costs?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "The CEO, Alex's cofounder, and the User's cofounder.",
          "retrieved_context": [
            "Sophia is stressing about infrastructure costs",
            "Alex's cofounder is stressed about the cloud bill being split and thinks it is inefficient.",
            "CEO is upset about the bill."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 578.571081161499,
          "generation_latency_ms": 4184.814453125,
          "total_latency_ms": 4763.385534286499
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which service is co-located on the same cloud provider as the system handling traffic flow?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer3",
          "predicted_answer": "The caching layer.",
          "retrieved_context": [
            "User wants to verify traffic routing and asks if load balancer is on same provider as AnalyticsEngine2",
            "User confirms AnalyticsEngine2 is hosted on Amazon's cloud (AWS), and the number 6 primary database is also hosted on Amazon's side, complicating networking due to the multi-cloud setup.",
            "Alex's load balancer and cache layer are hosted on Google Cloud Platform"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 620.574951171875,
          "generation_latency_ms": 3443.6864852905273,
          "total_latency_ms": 4064.2614364624023
        },
        {
          "probe_id": "declarative_reasoning-multi_hop-1",
          "pillar": "declarative_reasoning",
          "subpillar": "multi_hop_reasoning",
          "question": "The database connected to the Google-hosted load balancer is running on which infrastructure provider?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Amazon Web Services",
          "retrieved_context": [
            "Alex's load balancer and cache layer are hosted on Google Cloud Platform",
            "User confirms that the load balancer and caching layer are on Google's cloud.",
            "User wants to verify traffic routing and asks if load balancer is on same provider as AnalyticsEngine2"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 898.6775875091553,
          "generation_latency_ms": 3370.486259460449,
          "total_latency_ms": 4269.1638469696045
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions of the caching service are currently active according to the latest diagnostics?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "CacheLayer3 is v1.2.0.",
          "retrieved_context": [
            "User is looking at the GCP dashboard and wants to know what's going on with the caching service.",
            "User saw weird logs from the cache service.",
            "User saw alerts on phone about new caching service acting up again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 665.4946804046631,
          "generation_latency_ms": 3140.2013301849365,
          "total_latency_ms": 3805.6960105895996
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific deployment failure did the user identify as the cause of the cache latency spikes?",
          "answer_type": "short_answer",
          "gold_answer": "Stuck canary deployment",
          "predicted_answer": "Unknown. The user suspects the database not being in the same cloud environment as the cache",
          "retrieved_context": [
            "User's latency issues are mostly on the caching layer, and the latency of the third cache layer has been weird all morning.",
            "User suspects latency is worse because the database is not in the same cloud environment as the cache and wants to move the database to the same cloud as the cache.",
            "Elijah thinks the caching layer might be the culprit"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 684.7772598266602,
          "generation_latency_ms": 4615.464687347412,
          "total_latency_ms": 5300.241947174072
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user identify the stuck canary deployment before or after requesting a lunch reminder?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown. The context does not mention a lunch reminder.",
          "retrieved_context": [
            "User believes there is a stuck canary deployment.",
            "User wants a reminder to message the backend team about the stuck canary deployment in an hour.",
            "User wants a reminder to ping Will later, who needs uptime stats for a prospective client call."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 699.2015838623047,
          "generation_latency_ms": 2814.0556812286377,
          "total_latency_ms": 3513.2572650909424
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How did the user respond to the assistant's offer to block calendar time for the hiking trip?",
          "answer_type": "short_answer",
          "gold_answer": "Deferred it",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants a reminder to ask Mia about latency between clouds.",
            "User wants a reminder to buy coffee beans later.",
            "User wants to check for open slots before noon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 306.97035789489746,
          "generation_latency_ms": 3517.287015914917,
          "total_latency_ms": 3824.2573738098145
        },
        {
          "probe_id": "preference_learning-preference_induction-2",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What specific administrative action does the user consistently request across multiple sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Setting reminders",
          "predicted_answer": "Unknown. The context does not show the user consistently requesting a specific administrative action across multiple sessions.",
          "retrieved_context": [
            "User suspects Elijah didn't finish the rollback and wants to flag the issue for him.",
            "User saw alerts on phone about new caching service acting up again.",
            "User finds managing credentials for two clouds painful."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 977.3261547088623,
          "generation_latency_ms": 4682.659864425659,
          "total_latency_ms": 5659.9860191345215
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the frontend application?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex's primary database and analytics engine are hosted on Amazon Web Services",
            "Alex's load balancer and cache layer are hosted on Google Cloud Platform",
            "User confirms AnalyticsEngine2 is hosted on Amazon's cloud (AWS), and the number 6 primary database is also hosted on Amazon's side, complicating networking due to the multi-cloud setup."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 590.2283191680908,
          "generation_latency_ms": 4024.113416671753,
          "total_latency_ms": 4614.341735839844
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "User's cat is named Luna.",
            "User says cat is basically mascot.",
            "User asks what Mia is working on"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 519.1633701324463,
          "generation_latency_ms": 2522.1033096313477,
          "total_latency_ms": 3041.266679763794
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What method did the user employ to check for cross-cloud bottlenecks between GCP and AWS?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-reference with database",
          "predicted_answer": "Looking at the GCP dashboard.",
          "retrieved_context": [
            "User is relieved that the issue is isolated to GCP environment.",
            "User confirms that the load balancer and caching layer are on Google's cloud.",
            "User asks if GCP service is communicating okay with analytics engine."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 346.76671028137207,
          "generation_latency_ms": 4157.069683074951,
          "total_latency_ms": 4503.836393356323
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What configuration state was determined to be the root cause of the conflicting signals in the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Split state",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "The third cache layer is misbehaving.",
            "Elijah thinks the caching layer might be the culprit",
            "User's latency issues are mostly on the caching layer, and the latency of the third cache layer has been weird all morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 580.5141925811768,
          "generation_latency_ms": 4078.460454940796,
          "total_latency_ms": 4658.974647521973
        }
      ],
      "summary": {
        "overall_accuracy": 0.08333333333333333,
        "total_score": 1,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 12,
            "accuracy": 0.08333333333333333
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_039.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 29,
      "num_probes": 2,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 12.77497148513794,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:mia has role?",
          "answer_type": "short_answer",
          "gold_answer": "Frontend Lead",
          "predicted_answer": "Mia works on UI.",
          "retrieved_context": [
            "User owes Mia a favor.",
            "User says Mia is obsessed with the cat.",
            "User wants to check if Mia is online to ask her about UI components."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 744.7056770324707,
          "generation_latency_ms": 3562.145709991455,
          "total_latency_ms": 4306.851387023926
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "No.",
          "retrieved_context": [
            "User is asking if cofounder said anything about board meeting",
            "User said noted.",
            "User is mapping cloud footprint for a meeting with cofounder later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 654.9060344696045,
          "generation_latency_ms": 3683.2122802734375,
          "total_latency_ms": 4338.118314743042
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_040.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 73.06232929229736,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service that the Data Scientist complained about regarding latency?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "James was worried about latency on Azure side",
            "User is debugging latency.",
            "User thinks the first load balancer was moved off Google Cloud"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 891.4215564727783,
          "generation_latency_ms": 4033.2460403442383,
          "total_latency_ms": 4924.667596817017
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the Backend Lead responsible for the service currently experiencing a version leak?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "User knows James is leading the backend team.",
            "James is the backend lead.",
            "User will bug James, who leads the backend team, about the load balancer issue."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 711.5941047668457,
          "generation_latency_ms": 3261.7850303649902,
          "total_latency_ms": 3973.379135131836
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the individual who set the Friday deadline for the visual language drafts?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Emma wants drafts by Friday.",
            "Sophia is part of the design team.",
            "Alex wants to know if Sophia or Will sent new mockups."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 670.8650588989258,
          "generation_latency_ms": 3142.194986343384,
          "total_latency_ms": 3813.0600452423096
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What specific software versions were observed running on LoadBalancer2 during the latest infrastructure check?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta",
          "retrieved_context": [
            "User notes confusion about which version of LoadBalancer2 is live.",
            "User confirms load balancer is LoadBalancer2",
            "User asks about LoadBalancer2 status."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 642.9507732391357,
          "generation_latency_ms": 2539.2067432403564,
          "total_latency_ms": 3182.157516479492
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Azure budget email occur before or after the user requested a dry cleaning reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The discussion about the Azure budget (Emma's worry about cloud spend) occurred after the user requested a dry cleaning reminder.",
          "retrieved_context": [
            "James pinged User about Azure setup.",
            "User wants reminder to ping cofounder about quarterly review later",
            "User wants a reminder to pick up dry cleaning at 5."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 574.160099029541,
          "generation_latency_ms": 3805.4866790771484,
          "total_latency_ms": 4379.646778106689
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific infrastructure issue caused the user to request a flag be sent to the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Beta version traffic in production",
          "predicted_answer": "Unknown. The context states \"User wants to flag issue for James\" but does not specify what that issue is.",
          "retrieved_context": [
            "User is dealing with infrastructure stuff",
            "User will ping the Backend Lead to see if he's testing in prod again",
            "User intends to ping James, the backend team lead, about a failed canary deployment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 636.0728740692139,
          "generation_latency_ms": 3805.1135540008545,
          "total_latency_ms": 4441.186428070068
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What physical activity does the user imply helps them prepare mentally for the work day?",
          "answer_type": "short_answer",
          "gold_answer": "Running",
          "predicted_answer": "A quick run.",
          "retrieved_context": [
            "User believes the design team has a smoother day than the user.",
            "User wants a quick run",
            "User is stressed."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 807.4510097503662,
          "generation_latency_ms": 2647.2930908203125,
          "total_latency_ms": 3454.7441005706787
        },
        {
          "probe_id": "preference_learning-preference_scope-1",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user express a preference for managing financial details like Azure bills personally?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown. The user is auditing cloud spend and has to explain cloud costs, but does not express a preference for personally managing financial details or bills.",
          "retrieved_context": [
            "Emma is worried about cloud spend, especially on Azure components.",
            "Alex is looking at Azure infrastructure.",
            "User asks if AuthService9 is on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 629.1308403015137,
          "generation_latency_ms": 4245.530843734741,
          "total_latency_ms": 4874.661684036255
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Product Designer named Will?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User acknowledges that Will and Sophia are designers.",
            "User has colleagues Sophia and Will.",
            "Alex wants to know if Sophia or Will sent new mockups."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 590.6655788421631,
          "generation_latency_ms": 3055.58180809021,
          "total_latency_ms": 3646.247386932373
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the currently running version of AuthService3?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks about AuthService3.",
            "User wants to check AuthService3.",
            "User asks where AuthService3 is running"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 541.132926940918,
          "generation_latency_ms": 3100.449562072754,
          "total_latency_ms": 3641.582489013672
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "According to the user's instructions during the beta leak incident, who is the point of contact for resolving backend configuration errors?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James, the backend team lead.",
          "retrieved_context": [
            "User will bug James, who leads the backend team, about the load balancer issue.",
            "User believes the beta leak shouldn't be happening in production yet.",
            "User acknowledges beta software in production."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 567.1336650848389,
          "generation_latency_ms": 3903.9759635925293,
          "total_latency_ms": 4471.109628677368
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-1",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What is the current deadline status for the visual language drafts assigned to Sophia and Will?",
          "answer_type": "short_answer",
          "gold_answer": "Due Friday",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "Sophia and Will are handling new visual language.",
            "User acknowledges that Will and Sophia are designers.",
            "Alex wants to know if Sophia or Will sent new mockups."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 638.4508609771729,
          "generation_latency_ms": 3401.6528129577637,
          "total_latency_ms": 4040.1036739349365
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_041.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 32,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 73.54803442955017,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the job title of the individual the user refers to as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cofounder is dragging User into a strategy meeting in ten minutes.",
            "User's cofounder is asking for a stability report later today.",
            "User says cofounder loves impromptu syncs."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 682.7824115753174,
          "generation_latency_ms": 3398.369550704956,
          "total_latency_ms": 4081.1519622802734
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What professional role does the owner of the cat named 'Luna' hold?",
          "answer_type": "short_answer",
          "gold_answer": "Frontend Lead",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User notes that Luna the cat is getting fluffy.",
            "User says Ava is working from home today with cat Luna.",
            "The cat is basically the mascot."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 608.2568168640137,
          "generation_latency_ms": 3900.810956954956,
          "total_latency_ms": 4509.06777381897
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service currently running the v2.0.0-beta version?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "User wants status of Azure-hosted service",
            "User asks about the location of auth services in the multi-cloud topology.",
            "User asks about the status of the AWS environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 574.6500492095947,
          "generation_latency_ms": 2765.5091285705566,
          "total_latency_ms": 3340.1591777801514
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What type of service is hosted on the Azure platform?",
          "answer_type": "short_answer",
          "gold_answer": "Auth service",
          "predicted_answer": "An authentication service.",
          "retrieved_context": [
            "User wants status of Azure-hosted service",
            "User wants to know about authentication service on Azure",
            "User says there is one straggler auth service on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 385.0419521331787,
          "generation_latency_ms": 3614.76469039917,
          "total_latency_ms": 3999.8066425323486
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific event was the CEO anxious about regarding platform stability?",
          "answer_type": "short_answer",
          "gold_answer": "Investor demo",
          "predicted_answer": "The cofounder was anxious about the investor demo and pitching to the board.",
          "retrieved_context": [
            "User's cofounder was stressing about uptime",
            "User's cofounder is asking for a stability report later today.",
            "User does not want downtime while cofounder pitches to board"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 555.0038814544678,
          "generation_latency_ms": 4168.212175369263,
          "total_latency_ms": 4723.2160568237305
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask about the location of LoadBalancer9 before or after the discussion about the 'happy Tuesday' log discrepancies?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context does not mention any discussion about 'happy Tuesday' log discrepancies.",
          "retrieved_context": [
            "User wants to confirm where LoadBalancer9 and LoadBalancer5 are hosted.",
            "User wants to check load balancer near analytics engine.",
            "LoadBalancer9 is stable"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 611.3262176513672,
          "generation_latency_ms": 2952.5973796844482,
          "total_latency_ms": 3563.9235973358154
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user characterize their stance on running the split version setup on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Risky but necessary",
          "predicted_answer": "The user reports that a split version situation is currently running and believes it is causing a cost spike. They are concerned about the analytics setup and are seeing weird logs, implying a negative view of the current split version.",
          "retrieved_context": [
            "User asks if single stable version is running on analytics engine",
            "User is concerned about analytics setup",
            "User believes the beta version of Analytics Engine is for testing new backend features and is fine if hosted on Google Cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 704.8156261444092,
          "generation_latency_ms": 4449.290990829468,
          "total_latency_ms": 5154.106616973877
        },
        {
          "probe_id": "preference_learning-explicit_preferences-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which cloud provider has the user explicitly designated as their preferred choice for all future services?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown. The context does not explicitly state a preferred cloud provider for all future services.",
          "retrieved_context": [
            "User asks about the location of auth services in the multi-cloud topology.",
            "User is looking at AWS infrastructure now",
            "User thinks there is an auth service hosted on Microsoft's cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 616.5289878845215,
          "generation_latency_ms": 4074.0368366241455,
          "total_latency_ms": 4690.565824508667
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the system 'LoadBalancer9' currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to confirm where LoadBalancer9 and LoadBalancer5 are hosted.",
            "LoadBalancer9 is stable",
            "User asks which load balancer is over there."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 518.1581974029541,
          "generation_latency_ms": 2688.9827251434326,
          "total_latency_ms": 3207.1409225463867
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the DevOps Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User will catch DevOps lead later",
            "Charlotte is the DevOps lead.",
            "User didn't realize DevOps lead had finalized migration to Azure"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 337.8949165344238,
          "generation_latency_ms": 3034.8658561706543,
          "total_latency_ms": 3372.760772705078
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who does the user intend to contact to resolve the race condition on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is seeing weird logs from the analytics engine.",
            "User wants to focus on stabilizing analytics engine.",
            "User believes AnalyticsEngine2 is the instance stuck on GCP"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 533.4475040435791,
          "generation_latency_ms": 3457.1330547332764,
          "total_latency_ms": 3990.5805587768555
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-12",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What method does the assistant use to verify the current status and version of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Checking telemetry",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants current state of analytics engine",
            "User asks if single stable version is running on analytics engine",
            "User wants telemetry for AnalyticsEngine2"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 687.7400875091553,
          "generation_latency_ms": 3816.1017894744873,
          "total_latency_ms": 4503.841876983643
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "abstain": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_042.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 27,
      "num_probes": 9,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 54.74913930892944,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the 'cofounder' that was reviewing the billing dashboard?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Will",
          "retrieved_context": [
            "User's cofounder was confused about invoices from three different cloud providers",
            "User plans to ask Lucas about new dashboard mockups later",
            "User states Oliver is the point person for the backend team"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 649.8880386352539,
          "generation_latency_ms": 3046.5166568756104,
          "total_latency_ms": 3696.4046955108643
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who leads the sales department?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "Sales Lead mentioned her cat Luna was acting up this morning",
            "User got distracted because Sales Lead brought her pet to video call",
            "User notes Emma is stressed about sales targets"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 382.56096839904785,
          "generation_latency_ms": 2678.148031234741,
          "total_latency_ms": 3060.708999633789
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the core data storage system managed by the Backend Lead hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "System is hosted on Azure.",
            "User told cofounder that core data storage and traffic distribution are on Microsoft stack",
            "PrimaryDB2 is on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 398.68664741516113,
          "generation_latency_ms": 4119.989395141602,
          "total_latency_ms": 4518.676042556763
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite the logs suggesting a beta version, what version number does the user assert is the stable release for the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Half the system reports the database is running stable v1.2.0, while the other half reports v2.0.0 beta.",
            "User's cofounder mentioned main database instance; User sees conflicting info on PrimaryDB2 version: v1.2.0 or v2.0.0-beta",
            "User asks what version the primary DB is running and says Will thinks it was upgraded"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 585.6969356536865,
          "generation_latency_ms": 4171.743392944336,
          "total_latency_ms": 4757.4403285980225
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "What personal task did the user request a reminder for immediately before asking about the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Call mom",
          "predicted_answer": "User wants reminder set for dentist appointment later",
          "retrieved_context": [
            "User wants a reminder to grab a double espresso before sync with cofounder later.",
            "User wants a reminder set for dentist appointment later",
            "User wants reminder to pick up coffee before sync with Will"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 581.0127258300781,
          "generation_latency_ms": 4413.590431213379,
          "total_latency_ms": 4994.603157043457
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user suggest the Azure portal might be lagging during the conversation about infrastructure confusion?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting version info",
          "predicted_answer": "Unknown. The context does not state that the user suggested the Azure portal might be lagging.",
          "retrieved_context": [
            "User wants a quick status check on Azure infrastructure",
            "System is hosted on Azure.",
            "Alex wants status of main database and load balancer on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 607.8112125396729,
          "generation_latency_ms": 3834.0892791748047,
          "total_latency_ms": 4441.9004917144775
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on previous interactions, how does the user view the presence of Emma's pet during video calls?",
          "answer_type": "short_answer",
          "gold_answer": "Hilarious",
          "predicted_answer": "Unknown. The context only states that the user knows Emma brings her cat Luna to video calls, but does not express the user's view on this.",
          "retrieved_context": [
            "User says Emma brings her cat Luna to video calls",
            "User is asking if Emma mentioned bringing her pet to office tomorrow",
            "User hopes Emma's cat is okay."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 546.9775199890137,
          "generation_latency_ms": 3165.7769680023193,
          "total_latency_ms": 3712.754487991333
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is the LoadBalancer8 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants verification of LoadBalancer8 traffic load.",
            "User wants Oliver to check LoadBalancer8.",
            "LoadBalancer8 should be fine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 284.20376777648926,
          "generation_latency_ms": 3352.2815704345703,
          "total_latency_ms": 3636.4853382110596
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-8",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What protocol did the assistant agree to follow regarding the conflicting database versions in the production environment?",
          "answer_type": "generation",
          "gold_answer": "Treat v1.2.0 as active",
          "predicted_answer": "Unknown. The context does not mention any protocol the assistant agreed to follow.",
          "retrieved_context": [
            "User's cofounder mentioned main database instance; User sees conflicting info on PrimaryDB2 version: v1.2.0 or v2.0.0-beta",
            "Oliver sees conflicting reports on the main database version.",
            "Lucas is asking about database version for new prototype."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 610.2478504180908,
          "generation_latency_ms": 3119.8277473449707,
          "total_latency_ms": 3730.0755977630615
        }
      ],
      "summary": {
        "overall_accuracy": 0.4444444444444444,
        "total_score": 4,
        "count": 9,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 7,
            "accuracy": 0.42857142857142855
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_043.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 71.15589761734009,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the organizational role of the person who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Unknown. The context states that the \"User\" and \"Liam\" both own cats",
          "retrieved_context": [
            "Luna is a cat considered the mascot.",
            "User's cat Luna is fine but tearing up furniture.",
            "Liam has a new cat named Luna"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 592.6237106323242,
          "generation_latency_ms": 4713.43469619751,
          "total_latency_ms": 5306.058406829834
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which other service is hosted on the same cloud provider as AuthService6?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know where AuthService6 is running.",
            "Sophia asked whether AuthService6 and AnalyticsEngine5 are on the same cluster.",
            "User wants to check if AuthService6 is handling load"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 623.4486103057861,
          "generation_latency_ms": 3366.3010597229004,
          "total_latency_ms": 3989.7496700286865
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service that is currently exhibiting version mismatches hosted?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google",
          "retrieved_context": [
            "User is asking if there are version mismatches on the Google instance",
            "User saw alerts earlier about version mismatches on the Google instance",
            "User wants to know if cache service number 4 is hosted on Amazon"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 532.1769714355469,
          "generation_latency_ms": 2649.0094661712646,
          "total_latency_ms": 3181.1864376068115
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the logs, which two versions are simultaneously reported for the fourth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "stable v1.2.0 and beta v2.0.0-beta.",
          "retrieved_context": [
            "User is dealing with fourth cache layer and sees erratic logs for CacheLayer4.",
            "User asks where the fourth cache layer is running.",
            "Will mentioned a version mismatch on the cache layer"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 318.73369216918945,
          "generation_latency_ms": 3481.076955795288,
          "total_latency_ms": 3799.8106479644775
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What weather condition did the user mention while discussing the ambiguous metrics of the fourth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Rain",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is referring to CacheLayer4.",
            "User asks where the fourth cache layer is running.",
            "User is dealing with fourth cache layer and sees erratic logs for CacheLayer4."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 791.0454273223877,
          "generation_latency_ms": 3109.5633506774902,
          "total_latency_ms": 3900.608777999878
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user specifically request a breakdown of infrastructure by provider before 10 AM?",
          "answer_type": "short_answer",
          "gold_answer": "Meeting with Lucas",
          "predicted_answer": "Unknown. The context states the user wants a breakdown of infrastructure by provider and that the cofounder",
          "retrieved_context": [
            "User wants a breakdown of infrastructure by provider",
            "User is requesting the current infrastructure map.",
            "User wants to double-check infrastructure map for cache services hosting providers."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 684.7805976867676,
          "generation_latency_ms": 3994.0898418426514,
          "total_latency_ms": 4678.870439529419
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When mapping out infrastructure for the cloud spend discussion, did the user prefer a breakdown by service usage or by provider?",
          "answer_type": "short_answer",
          "gold_answer": "By provider",
          "predicted_answer": "By provider.",
          "retrieved_context": [
            "User wants a breakdown of infrastructure by provider",
            "Lucas has asked about the company's cloud spend",
            "User wants to double-check infrastructure map for cache services hosting providers."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 584.4988822937012,
          "generation_latency_ms": 3338.3853435516357,
          "total_latency_ms": 3922.884225845337
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine, what specific type of coffee do they appear to favor?",
          "answer_type": "short_answer",
          "gold_answer": "Double espresso",
          "predicted_answer": "Double espresso.",
          "retrieved_context": [
            "User is going to grab coffee now.",
            "User is going to grab another coffee before diving in.",
            "User had a double espresso."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 567.277193069458,
          "generation_latency_ms": 2731.4629554748535,
          "total_latency_ms": 3298.7401485443115
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is currently running on AnalyticsEngine5?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know the location of AnalyticsEngine5 for Noah's data pipelines.",
            "AnalyticsEngine5 has not been migrated yet.",
            "User wants the status of the analytics engine."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 277.51660346984863,
          "generation_latency_ms": 3099.1361141204834,
          "total_latency_ms": 3376.652717590332
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of Frontend Lead in the organization?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex knows that Will and Sophia share the Backend Lead role.",
            "User wants to know who is running the backend team right now",
            "User wants to loop in both Backend Lead and Sophia and finds it confusing to have two Backend Leads."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 750.962495803833,
          "generation_latency_ms": 2629.0199756622314,
          "total_latency_ms": 3379.9824714660645
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical metric was the team testing by splitting the cache layers across different providers?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Latency",
          "retrieved_context": [
            "User knows the cache layers are split to test latency",
            "User wants to double-check infrastructure map for cache services hosting providers.",
            "User asks where the fourth cache layer is running."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 405.6258201599121,
          "generation_latency_ms": 2728.177070617676,
          "total_latency_ms": 3133.802890777588
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What future task did the assistant schedule regarding the user's mother?",
          "answer_type": "short_answer",
          "gold_answer": "Call in one hour",
          "predicted_answer": "A reminder to call mom in one hour.",
          "retrieved_context": [
            "User requests adding a reminder to buy a birthday gift for their mom.",
            "User wants to call mom at 6 PM",
            "User asks if they were reminded to buy a birthday gift for their mom."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 395.5411911010742,
          "generation_latency_ms": 4384.905099868774,
          "total_latency_ms": 4780.446290969849
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 12,
            "accuracy": 0.4166666666666667
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_044.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 73.82456564903259,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning their 'cofounder' who is worried about login stability?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "User's cofounder is panicking about login stability for board meeting.",
            "User's cofounder James was asking about the stability of the new build.",
            "User's cofounder was up late"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 648.9417552947998,
          "generation_latency_ms": 3594.3515300750732,
          "total_latency_ms": 4243.293285369873
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who holds the Frontend Lead role?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know who is heading the frontend team.",
            "Sophia is the Backend Lead.",
            "James is a CEO"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 651.080846786499,
          "generation_latency_ms": 3430.643320083618,
          "total_latency_ms": 4081.724166870117
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service that is currently running a beta version hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex asks whether the supporting services for auth are in the same cloud region.",
            "User wants to verify what is running on Azure before speaking to the cofounder.",
            "User wants to know if load balancers are split across different clouds."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 635.3366374969482,
          "generation_latency_ms": 3373.692750930786,
          "total_latency_ms": 4009.0293884277344
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which specific service did the user identify as causing infrastructure fragmentation due to its hosting location on Google Cloud?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "LoadBalancer2",
          "retrieved_context": [
            "Alex notes that there is a stray load balancer on Google Cloud.",
            "LoadBalancer2 is hosted on Google Cloud.",
            "Alex asks if the second load balancer is on Google's infrastructure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 515.0763988494873,
          "generation_latency_ms": 3185.5416297912598,
          "total_latency_ms": 3700.618028640747
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event caused the Frontend Lead to get distracted while debugging the login UI?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Unknown. The context states Emma was debugging the login UI, and the User asked if",
          "retrieved_context": [
            "Emma was trying to debug login UI earlier.",
            "User's cofounder is panicking about login stability for board meeting.",
            "Alex feels they are seeing double on the dashboard for the auth service."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 874.0940093994141,
          "generation_latency_ms": 4644.144773483276,
          "total_latency_ms": 5518.23878288269
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to check the Azure deployment slots before or after inquiring about the status of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context does not mention the user asking to check Azure deployment slots.",
          "retrieved_context": [
            "User wants to know if the analytics piece is on Azure.",
            "User wants to verify what is running on Azure before speaking to the cofounder.",
            "Alex asks if AnalyticsEngine6 is running okay."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 745.9056377410889,
          "generation_latency_ms": 3199.9692916870117,
          "total_latency_ms": 3945.8749294281006
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific beverage did the user state they required before the standup meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Double espresso",
          "predicted_answer": "Double espresso",
          "retrieved_context": [
            "User needs a double espresso before standup.",
            "User needs to grab coffee before next call",
            "User will be pulled into a strategy meeting"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 717.5447940826416,
          "generation_latency_ms": 3145.838499069214,
          "total_latency_ms": 3863.3832931518555
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about fragmentation, which cloud provider is implied to be the non-preferred or 'odd' location for the load balancer?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "User wants to know if load balancers are split across different clouds.",
            "Alex asks about the load balancing situation across providers.",
            "LoadBalancer2 is hosted on Google Cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 508.991003036499,
          "generation_latency_ms": 4785.128116607666,
          "total_latency_ms": 5294.119119644165
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the Analytics Engine currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "v1.2.0",
          "retrieved_context": [
            "Alex asks if AnalyticsEngine6 is running okay.",
            "User asks if Sophia looked at backend logs for AnalyticsEngine6",
            "User asks whether analytics engine is causing latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 997.351884841919,
          "generation_latency_ms": 2721.90260887146,
          "total_latency_ms": 3719.254493713379
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the tools used by the Sales Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Alex asks whether the supporting services for auth are in the same cloud region.",
            "User wants to verify what is running on Azure before speaking to the cofounder.",
            "User's identity provider is running on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 369.4300651550293,
          "generation_latency_ms": 3055.4587841033936,
          "total_latency_ms": 3424.888849258423
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Which monitoring service does the assistant use to check the status of the AWS-hosted analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "CloudWatch",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User reports analytics engine fine on AWS.",
            "User wants to check engine on AWS",
            "Latency of Analytics Engine on AWS is killing dashboard load times."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 376.2698173522949,
          "generation_latency_ms": 3767.5797939300537,
          "total_latency_ms": 4143.849611282349
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What does the assistant identify as a 'classic hazard' for the Frontend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know who is heading the frontend team.",
            "The frontend lead needs to know if the API contract changed.",
            "Sophia is the Backend Lead."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 806.6051006317139,
          "generation_latency_ms": 2637.8509998321533,
          "total_latency_ms": 3444.456100463867
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_045.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 31,
      "num_probes": 10,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 58.631990909576416,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which team member is referred to as the 'DevOps specialist' responsible for patching the database?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Will",
          "retrieved_context": [
            "Will is the user's DevOps guy.",
            "User wants DevOps guy to investigate the issue immediately.",
            "Will was doing maintenance on the main database this morning"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 497.99156188964844,
          "generation_latency_ms": 2434.464931488037,
          "total_latency_ms": 2932.4564933776855
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider is the service hosted that the CEO is specifically concerned about regarding budget fragmentation?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "James is stressing about the cloud budget",
            "User is stressed about infrastructure costs due to huge bill on AWS",
            "User is worried about infrastructure costs due to huge bill on AWS"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 465.34204483032227,
          "generation_latency_ms": 2836.5936279296875,
          "total_latency_ms": 3301.9356727600098
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales",
          "retrieved_context": [
            "User refers to Luna as the cat.",
            "The cat's name is Luna.",
            "User says Luna is a cute cat."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 384.0677738189697,
          "generation_latency_ms": 4512.215852737427,
          "total_latency_ms": 4896.2836265563965
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-0",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "The analytics engine is causing latency because it is hosted on a different provider than the primary database. Where is the primary database hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User is annoyed that the analytics engine is on AWS while the primary database is on Azure.",
            "The user knows the connection to AnalyticsEngine1 is solid.",
            "James forgot that the analytics engine is hosted on AWS"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 618.7288761138916,
          "generation_latency_ms": 2715.0723934173584,
          "total_latency_ms": 3333.80126953125
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user decided to hold off on emailing James, what personal reminder was set?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist appointment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants reminder to slack Emma later; Emma needs to fix button alignment before demo.",
            "User needs to make sure they didn't miss any action items assigned by the cofounder.",
            "User had a stressful meeting with James this morning"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 307.04188346862793,
          "generation_latency_ms": 4072.2694396972656,
          "total_latency_ms": 4379.311323165894
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the DevOps specialist describe his work as a 'nightmare' in recent discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Will complains that latency issues, caused by the cross-cloud setup, make his DevOps work a nightmare.",
          "retrieved_context": [
            "Will thinks cross-cloud setup is to blame for latency issues and complains that latency makes his DevOps work a nightmare.",
            "User's cofounder is panicked about the quarterly review.",
            "User wants DevOps guy to investigate the issue immediately."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 759.3045234680176,
          "generation_latency_ms": 2750.3552436828613,
          "total_latency_ms": 3509.659767150879
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine comments, what specific beverage do they require to function?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "User wants reminder to order a salad and is trying to eat healthier",
            "User's coffee hasn't kicked in yet and everything is a blur.",
            "User skipped breakfast."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 352.6484966278076,
          "generation_latency_ms": 3515.587568283081,
          "total_latency_ms": 3868.2360649108887
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the service managed by the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Will insists the database is hosted on Azure.",
            "The caching service is split across clouds.",
            "Will needs to clarify the cloud location for the component"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 562.7329349517822,
          "generation_latency_ms": 2881.296396255493,
          "total_latency_ms": 3444.0293312072754
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the PrimaryDB9 system currently running exclusively on version v2.0.0-beta?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. PrimaryDB9 has logs indicating both stable v1.2.0 and traces of v2.0.0-beta, and the dashboard shows v1.2.0.",
          "retrieved_context": [
            "User is unsure if PrimaryDB9 was upgraded and is trying to determine if the main instance is running the beta or stable release.",
            "PrimaryDB9 has logs indicating both stable v1.2.0 and traces of v2.0.0-beta, with dashboard showing v1.2.0 but logs indicating beta features active",
            "User was looking at main database PrimaryDB9."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 440.0365352630615,
          "generation_latency_ms": 3642.0819759368896,
          "total_latency_ms": 4082.118511199951
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How does the assistant determine the version status of the PrimaryDB9 when asked?",
          "answer_type": "short_answer",
          "gold_answer": "Checks headers",
          "predicted_answer": "By checking the logs of PrimaryDB9, which indicate both stable v1.2.0 and traces of v2.0.0-beta, and also show beta features active despite the dashboard reporting v1.2.0.",
          "retrieved_context": [
            "User is unsure if PrimaryDB9 was upgraded and is trying to determine if the main instance is running the beta or stable release.",
            "User is looking at system health dashboard and concerned about primary database reporting two different versions.",
            "User was looking at main database PrimaryDB9."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 523.6320495605469,
          "generation_latency_ms": 3717.3779010772705,
          "total_latency_ms": 4241.009950637817
        }
      ],
      "summary": {
        "overall_accuracy": 0.6,
        "total_score": 6,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_046.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 70.81209754943848,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the cofounder the user plans to have lunch with?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User had coffee with cofounder.",
            "User needs to prep for meeting with cofounder, who wants a map of infrastructure",
            "User wants to talk to their cofounder about hiring a release manager."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1065.502405166626,
          "generation_latency_ms": 3958.577871322632,
          "total_latency_ms": 5024.080276489258
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which infrastructure provider hosts the database that Sophia is concerned about regarding latency?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google",
          "retrieved_context": [
            "Sophia is worried about latency to main database on Google infrastructure",
            "User will have Olivia and Sophia look into latency between clouds.",
            "User is seeing latency on the main database."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 631.2127113342285,
          "generation_latency_ms": 2879.744291305542,
          "total_latency_ms": 3510.9570026397705
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two specific version numbers are simultaneously active on the authentication service hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and beta",
          "retrieved_context": [
            "Alex wants to know which version the authentication service on AWS is currently running",
            "User wants status of auth service on AWS",
            "User is asking whether auth service version is v1.2.0 or v2.0.0-beta"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 602.11181640625,
          "generation_latency_ms": 2804.649591445923,
          "total_latency_ms": 3406.761407852173
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the name of the pet belonging to the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is backend lead",
            "User wants to know who is covering backend lead role",
            "Alex wants to know who is handling the backend team"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 397.92585372924805,
          "generation_latency_ms": 2943.434715270996,
          "total_latency_ms": 3341.360569000244
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to set a reminder for their mom's birthday before or after the reminder for dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "After.",
          "retrieved_context": [
            "User wants a reminder about dry cleaning again in an hour.",
            "User wants a reminder to pick up dry cleaning later.",
            "User wants reminder to pick up dry cleaning at 5 PM"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 337.6750946044922,
          "generation_latency_ms": 3042.6547527313232,
          "total_latency_ms": 3380.3298473358154
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific complaint did the user make about the coffee in session 23?",
          "answer_type": "short_answer",
          "gold_answer": "Tastes like battery acid",
          "predicted_answer": "Unknown. The context does not specify \"session 23.\"",
          "retrieved_context": [
            "User's coffee tastes like battery acid.",
            "User needs more coffee",
            "User had coffee with cofounder."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 909.6682071685791,
          "generation_latency_ms": 3009.3672275543213,
          "total_latency_ms": 3919.0354347229004
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the morning sessions, what beverage does the user consistently require before working?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee/caffeine.",
          "retrieved_context": [
            "User wants a quick sanity check on infrastructure before caffeine kicks in.",
            "Alex needs to grab coffee before next meeting.",
            "User is going to grab lunch before tackling tasks"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 360.9805107116699,
          "generation_latency_ms": 4093.637228012085,
          "total_latency_ms": 4454.617738723755
        },
        {
          "probe_id": "preference_learning-explicit_preferences-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "At what time did the user specifically request to be reminded to call their mom?",
          "answer_type": "short_answer",
          "gold_answer": "6 PM",
          "predicted_answer": "6 PM and 5 PM",
          "retrieved_context": [
            "User wants a reminder to call mom for her birthday at 6 PM",
            "User wants reminder to call mom at 5 PM.",
            "User wants reminder to grab coffee in ten minutes"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 575.9050846099854,
          "generation_latency_ms": 3299.7732162475586,
          "total_latency_ms": 3875.678300857544
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is running on the LoadBalancer4 service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know how LoadBalancer4 is holding up.",
            "User refers to load balancer number 4.",
            "User is asking whether load balancer number 4 is on Amazon"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 695.4925060272217,
          "generation_latency_ms": 3493.8952922821045,
          "total_latency_ms": 4189.387798309326
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the specific team lead for the DevOps engineers Emma and Mia?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Emma is on the DevOps crew",
            "User needs DevOps folks Emma or Mia to fix deployment pipeline fast",
            "Emma and Mia handle the pipes."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 577.4259567260742,
          "generation_latency_ms": 3241.405248641968,
          "total_latency_ms": 3818.831205368042
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "When the user references the 'Microsoft cloud', which specific load balancer identifier is relevant?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer4",
          "predicted_answer": "LoadBalancer4",
          "retrieved_context": [
            "Alex wants to know if load balancer is present on Microsoft cloud",
            "User asks if LoadBalancer4 is still in Microsoft cloud",
            "Load balancer is hosted on Microsoft Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 757.5259208679199,
          "generation_latency_ms": 2675.222873687744,
          "total_latency_ms": 3432.748794555664
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "How did the user categorize the state of the deployment on the AWS auth service in the final session?",
          "answer_type": "short_answer",
          "gold_answer": "Stuck deployment",
          "predicted_answer": "The user suspected authentication layer issues.",
          "retrieved_context": [
            "User wants status of auth service on AWS",
            "Alex wants to know which version the authentication service on AWS is currently running",
            "User is asking whether AuthService7 is on AWS"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 346.6176986694336,
          "generation_latency_ms": 3731.8224906921387,
          "total_latency_ms": 4078.4401893615723
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 10,
            "accuracy": 0.1
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_047.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 30,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 74.79774308204651,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning the 'cofounder' who texted late at night about bills?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown. The context states \"User's cofounder texted User late last night about",
          "retrieved_context": [
            "User's cofounder texted User late last night about infrastructure bills.",
            "User wants to know if cofounder is coming in late.",
            "User's cofounder loves to hear himself talk."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 692.9988861083984,
          "generation_latency_ms": 4204.520225524902,
          "total_latency_ms": 4897.519111633301
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific infrastructure services are currently hosted on the Amazon platform?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB4 and CacheLayer7",
          "predicted_answer": "CacheLayer7, PrimaryDB4",
          "retrieved_context": [
            "User's main infrastructure cost is the Amazon bill.",
            "CacheLayer7 is hosted on AWS.",
            "User asks if auth service is on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 661.4639759063721,
          "generation_latency_ms": 4239.954471588135,
          "total_latency_ms": 4901.418447494507
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Product Designer",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Luna is Noah's cat",
            "Noah is worried about his cat Luna",
            "Alex says Luna is practically an employee"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 621.1385726928711,
          "generation_latency_ms": 3746.018409729004,
          "total_latency_ms": 4367.156982421875
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite the dashboard displaying v1.2.0, what version do the logs suggest PrimaryDB4 is actually running?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "v1.2.0",
          "retrieved_context": [
            "Config file says PrimaryDB4 is on v1.2.0",
            "Half the nodes of PrimaryDB4 report version v1.2.0.",
            "New instances of PrimaryDB4 are spinning up with version v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 797.5938320159912,
          "generation_latency_ms": 2615.091562271118,
          "total_latency_ms": 3412.6853942871094
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user complain about the two-hour 'quick sync' before or after asking to clear their schedule to review hosting costs?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown. The context states the quick sync happened and the user feels behind schedule *today* because of",
          "retrieved_context": [
            "User wants to review hosting costs and asks if database and cache are on the same bill",
            "User's cofounder pulled User into a quick sync that lasted two hours.",
            "User needs to schedule a UI sync with Oliver and Noah later"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 370.3737258911133,
          "generation_latency_ms": 4574.445009231567,
          "total_latency_ms": 4944.818735122681
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the CEO specifically concerned about the cost of the authentication service compared to the rest of the stack?",
          "answer_type": "short_answer",
          "gold_answer": "It is isolated on Azure",
          "predicted_answer": "The cofounder thinks they are overpaying for the authentication system, which is racking up charges on Microsoft's side.",
          "retrieved_context": [
            "User's cofounder wants to know why authentication system is not on same provider as rest of stack.",
            "User's cofounder thinks User is overpaying for the authentication system.",
            "User notes that the auth service is racking up charges on Microsoft's side"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 585.9010219573975,
          "generation_latency_ms": 3192.072868347168,
          "total_latency_ms": 3777.9738903045654
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user's cofounder feel about ambiguity in status reports?",
          "answer_type": "short_answer",
          "gold_answer": "He hates it",
          "predicted_answer": "The user's cofounder hates ambiguity.",
          "retrieved_context": [
            "User does not want to stress cofounder unless necessary",
            "User's cofounder hates ambiguity",
            "User wants to know if cofounder is coming in late."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 746.4454174041748,
          "generation_latency_ms": 2470.6168174743652,
          "total_latency_ms": 3217.06223487854
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the CEO's concern about overpaying apply to the AWS services or the Azure service?",
          "answer_type": "short_answer",
          "gold_answer": "Azure service",
          "predicted_answer": "The CEO's concern about overpaying applies to the Azure service, specifically the authentication system.",
          "retrieved_context": [
            "Elijah was asking about the AWS bill earlier.",
            "Elijah, the user's cofounder, is stressing about cloud bills.",
            "User thinks running multi-cloud is too expensive"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 494.661808013916,
          "generation_latency_ms": 3360.635757446289,
          "total_latency_ms": 3855.297565460205
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Oliver's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know if Oliver is around.",
            "User asks what kind of pet Noah has.",
            "User asks if Oliver is working on the same UI kit as Noah."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 885.1020336151123,
          "generation_latency_ms": 2830.409288406372,
          "total_latency_ms": 3715.5113220214844
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Does the assistant have visual confirmation of how big Noah's cat has grown?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User asks what kind of pet Noah has.",
            "Luna is Noah's cat",
            "Noah spams channel with pictures of his cat"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 344.1798686981201,
          "generation_latency_ms": 4036.1504554748535,
          "total_latency_ms": 4380.330324172974
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-11",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "Who is currently responsible for handling the backend logic of the infrastructure migration?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia, the backend lead.",
          "retrieved_context": [
            "Sophia is backend lead and the only one who understands the migration path.",
            "The sync agenda is infrastructure migration.",
            "Sophia runs the backend team"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 664.4682884216309,
          "generation_latency_ms": 3313.1909370422363,
          "total_latency_ms": 3977.659225463867
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Based on the user's interactions, what specific database issue creates a risk of confusing the CEO?",
          "answer_type": "short_answer",
          "gold_answer": "Version ambiguity",
          "predicted_answer": "The database version discrepancy and the user's confusion about database versioning.",
          "retrieved_context": [
            "User wants the CEO to know about unstable beta software in production.",
            "User wants a note drafted to Backend Lead regarding database version discrepancy.",
            "PrimaryDB4 is acting up."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 733.0913543701172,
          "generation_latency_ms": 4214.142560958862,
          "total_latency_ms": 4947.2339153289795
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_048.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 74.37974381446838,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' who is stressing about platform stability?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User had a long call with cofounder about platform stability for upcoming launch.",
            "User's cofounder is seeing variance.",
            "User will tell cofounder himself."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 608.4268093109131,
          "generation_latency_ms": 3585.8800411224365,
          "total_latency_ms": 4194.30685043335
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which employee is the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma",
          "retrieved_context": [
            "Emma is obsessed with Luna the cat",
            "Emma's cat Luna is adorable.",
            "Emma is distracting me with photos of her cat Luna."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 657.3545932769775,
          "generation_latency_ms": 2847.162961959839,
          "total_latency_ms": 3504.5175552368164
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service running the 'v2.0.0-beta' hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "User is confirming that the beta canary of AuthService2 is running in Azure and that the cache is also running in Azure.",
            "User says the instance is hosted on Azure.",
            "User confirms that split versions v1.2.0 and v2.0.0 beta are running on the Azure-hosted auth instance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 361.7737293243408,
          "generation_latency_ms": 2614.0973567962646,
          "total_latency_ms": 2975.8710861206055
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two individuals currently hold the role of Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver and Charlotte",
          "predicted_answer": "Oliver and Charlotte",
          "retrieved_context": [
            "Having both Oliver and Charlotte as Backend Leads is causing some overlap",
            "User has two backend leads: Oliver and Charlotte",
            "User wants current leadership structure for backend team."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 363.50393295288086,
          "generation_latency_ms": 3089.4596576690674,
          "total_latency_ms": 3452.9635906219482
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What personal task did the user ask to be reminded of during the conversation about Oliver and Charlotte's argument?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "Oliver and Charlotte are arguing about whether to kill the beta",
            "Oliver and Charlotte mentioned lag.",
            "Oliver and Charlotte are arguing about optimization strategies in conference room."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 571.8631744384766,
          "generation_latency_ms": 4319.645404815674,
          "total_latency_ms": 4891.50857925415
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the session history, why did the user describe the game match as 'intense'?",
          "answer_type": "short_answer",
          "gold_answer": "It went into overtime",
          "predicted_answer": "Unknown. The context does not mention a game match or describe anything as 'intense'.",
          "retrieved_context": [
            "User says Liam gets nervous when they touch the login flow.",
            "User is more worried about current sprint",
            "User should have stopped for an espresso on the way in."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1168.8826084136963,
          "generation_latency_ms": 3377.155542373657,
          "total_latency_ms": 4546.0381507873535
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What is the CEO's primary motivation for pushing infrastructure consolidation?",
          "answer_type": "short_answer",
          "gold_answer": "Bottom line",
          "predicted_answer": "The cofounder (who is likely the CEO or a high-level executive) is primarily motivated",
          "retrieved_context": [
            "User thinks we need to consolidate infrastructure someday.",
            "Liam is pressuring the user about consolidation",
            "Alex's cofounder is asking for a status report on the infrastructure migration"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 656.4581394195557,
          "generation_latency_ms": 4157.411813735962,
          "total_latency_ms": 4813.869953155518
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Which specific individual is concerned about the 'Azure spend'?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "User",
          "retrieved_context": [
            "User is concerned about Azure spend",
            "Alex is worried about the service running on Azure",
            "User says the instance is hosted on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 567.7528381347656,
          "generation_latency_ms": 2528.7742614746094,
          "total_latency_ms": 3096.527099609375
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is the system 'primarydb_3' currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is asking if PrimaryDB3 is located next to the auth service.",
            "Data Scientist would scream if PrimaryDB3 were moved",
            "User is checking if PrimaryDB3 is on AWS like AuthService8."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 301.851749420166,
          "generation_latency_ms": 3345.782995223999,
          "total_latency_ms": 3647.634744644165
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Data Scientist's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know if Mia is still heading up data science.",
            "User wants to know if the pet is a dog.",
            "User forgot the name of the pet."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 603.6117076873779,
          "generation_latency_ms": 3377.795457839966,
          "total_latency_ms": 3981.4071655273438
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration issue was identified as the cause of the version flapping on the Azure auth service?",
          "answer_type": "short_answer",
          "gold_answer": "Configuration drift",
          "predicted_answer": "Unknown. The context states that version flapping is occurring but does not identify the technical configuration issue causing it.",
          "retrieved_context": [
            "We are seeing weird inconsistencies with Azure authentication service",
            "Other backend lead is stuck dealing with Azure authentication service",
            "User confirms that split versions v1.2.0 and v2.0.0 beta are running on the Azure-hosted auth instance."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 599.4865894317627,
          "generation_latency_ms": 3050.0502586364746,
          "total_latency_ms": 3649.5368480682373
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on the identified configuration drift, was the traffic split on AuthService2 considered intentional?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, the context suggests the split versions are a result of configuration drift and an unstable state that needs stabilization, implying it was not intentional.",
          "retrieved_context": [
            "User believes there is configuration drift.",
            "User needs to fix AuthService2 issue before lunch",
            "User confirms that split versions v1.2.0 and v2.0.0 beta are running on the Azure-hosted auth instance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 326.4741897583008,
          "generation_latency_ms": 3816.2076473236084,
          "total_latency_ms": 4142.681837081909
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_049.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 73.52783918380737,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific system is the user's cofounder concerned about regarding version ambiguity?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "The main database on Google's cloud.",
          "retrieved_context": [
            "User suspects Ops lead has been tinkering with versions again",
            "User's cofounder thinks the versioning of the main database on Google's cloud is off.",
            "User will bring up compatibility errors with cofounder during sync later"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 723.2539653778076,
          "generation_latency_ms": 3207.49568939209,
          "total_latency_ms": 3930.7496547698975
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the DevOps lead responsible for the recent beta deployment on the Google Cloud database?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown.",
          "retrieved_context": [
            "User vaguely remembers DevOps guy might have updated main database last night",
            "PrimaryDB7 is on Google Cloud.",
            "DevOps guy is working on the issue."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 527.6401042938232,
          "generation_latency_ms": 3137.028455734253,
          "total_latency_ms": 3664.668560028076
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "The database hosted on AWS runs alongside which other system on the same cloud provider?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine6",
          "predicted_answer": "An analytics engine and a backup database.",
          "retrieved_context": [
            "PrimaryDB3 is hosted on AWS",
            "User mixes up which database is on GCP vs AWS.",
            "User wants to know which database is in Amazon environment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 599.7092723846436,
          "generation_latency_ms": 3292.325019836426,
          "total_latency_ms": 3892.0342922210693
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions are currently active on PrimaryDB7 according to the latest system checks?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Deployment logs and Alex's observations reference v2.0.0-beta for PrimaryDB",
          "retrieved_context": [
            "The version strings for PrimaryDB7 look weird.",
            "User looked through deployment logs for PrimaryDB7 this morning.",
            "User needs version number of main database"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 1176.9490242004395,
          "generation_latency_ms": 4461.246728897095,
          "total_latency_ms": 5638.195753097534
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the DevOps lead unavailable to immediately fix the database conflict in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Taking cat to vet",
          "predicted_answer": "Unknown. The context states \"DevOps guy is working on the issue,\" implying he is available and actively addressing it.",
          "retrieved_context": [
            "User vaguely remembers DevOps guy might have updated main database last night",
            "DevOps guy is working on the issue.",
            "Frontend leads are trying to determine if the issue is related to the AWS-hosted database used for legacy data."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 756.6797733306885,
          "generation_latency_ms": 3799.8948097229004,
          "total_latency_ms": 4556.574583053589
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the AWS bill happen before or after the strategy sync meeting was mentioned?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The discussion about the AWS bill happened before the strategy sync meeting was mentioned.",
          "retrieved_context": [
            "User has an analytics engine running on AWS and one database running on AWS, and is concerned about the AWS bill.",
            "User needs to review infrastructure map before strategy meeting with cofounder later.",
            "User says CEO is complaining about Amazon bill for other database"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 612.7047538757324,
          "generation_latency_ms": 3438.652753829956,
          "total_latency_ms": 4051.3575077056885
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-6",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What item is currently on the user's shopping list based on recent requests?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "Coffee beans, gift for niece, and flowers for anniversary.",
          "retrieved_context": [
            "User asks who owns the new UI mockups and wants to send new mockups to them.",
            "User needs to grab a sandwich before next meeting",
            "User intends to get new mockups done for the dashboard"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 608.1955432891846,
          "generation_latency_ms": 2911.174774169922,
          "total_latency_ms": 3519.3703174591064
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-7",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Which two frontend leads did the user decide to involve to verify feasibility?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte and Sophia",
          "predicted_answer": "Unknown. Only Sophia is mentioned in relation to verifying feasibility.",
          "retrieved_context": [
            "User wants both frontend leads on the UI review invite",
            "User wants to know if the frontend leadership is being split",
            "User wants to know who leads frontend team"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 384.5248222351074,
          "generation_latency_ms": 3374.605894088745,
          "total_latency_ms": 3759.1307163238525
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's repeated checks, which infrastructure component requires the most monitoring attention?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "Databases.",
          "retrieved_context": [
            "Oliver is worried about billing complexity from the multi-cloud setup",
            "User feels infrastructure is not fully stable on legacy build.",
            "Sophia complains that the dashboard is loading slowly."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 788.5117530822754,
          "generation_latency_ms": 4009.669542312622,
          "total_latency_ms": 4798.1812953948975
        },
        {
          "probe_id": "preference_learning-explicit_preferences-9",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific aspect of the company does Oliver usually focus on?",
          "answer_type": "short_answer",
          "gold_answer": "Financial overview",
          "predicted_answer": "Cloud costs, billing complexity, and quarterly targets.",
          "retrieved_context": [
            "User asks if Oliver mentioned when the board meeting is.",
            "Oliver wants to review quarterly targets.",
            "Oliver hates Azure"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 570.8503723144531,
          "generation_latency_ms": 3323.245048522949,
          "total_latency_ms": 3894.0954208374023
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's gym?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User mentions Luna.",
            "User is going to hunt for caffeine",
            "User wants to check where the auth service is sitting"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 603.2779216766357,
          "generation_latency_ms": 3011.852025985718,
          "total_latency_ms": 3615.1299476623535
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-11",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of Backend Lead at the company?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User wants to know who leads frontend team",
            "User wants to know if the frontend leadership is being split",
            "User thinks Charlotte is leading the frontend team"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 918.2543754577637,
          "generation_latency_ms": 3591.5963649749756,
          "total_latency_ms": 4509.850740432739
        }
      ],
      "summary": {
        "overall_accuracy": 0.16666666666666666,
        "total_score": 2,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 10,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_050.json",
      "adapter_type": "Mem0Adapter",
      "num_sessions": 25,
      "num_probes": 9,
      "ingestion_time_s": 0.0,
      "total_evaluation_time_s": 53.007657289505005,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Noah",
          "retrieved_context": [
            "User's cofounder is usually faster",
            "User's cofounder is asking about the beta test",
            "The user has a cofounder stressing about the quarterly review."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 556.9620132446289,
          "generation_latency_ms": 2831.512212753296,
          "total_latency_ms": 3388.474225997925
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the Frontend Lead responsible for the React migration?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "User will hand over to James to lead frontend implementation after design sign off",
            "User needs the Frontend Lead's perspective.",
            "Frontend team relies on v1.2.0"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 520.6215381622314,
          "generation_latency_ms": 2878.57985496521,
          "total_latency_ms": 3399.2013931274414
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the authentication service that is currently experiencing a stuck blue-green deployment?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Unknown. The context does not mention an authentication service experiencing a stuck blue-green deployment.",
          "retrieved_context": [
            "User says the main authentication service deployed on Google Cloud is behaving erratically.",
            "User wants to confirm which auth service is currently in the Google cloud environment.",
            "Backup auth service is on Microsoft's cloud."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 299.63207244873047,
          "generation_latency_ms": 3430.50479888916,
          "total_latency_ms": 3730.1368713378906
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions of the software are currently detected running on AuthService8?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "AuthService8 is detected running v1.2.0 (from the dashboard) and v2.0.0-beta (from crash logs).",
          "retrieved_context": [
            "User needs to figure out why AuthService8 is flapping between v1.2.0 and v2 beta",
            "Auth service instance is 8.",
            "User was supposed to upgrade the eighth instance of the auth service and wants to know its deployment location and current version (v1.2 or v2.0.0-beta)."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 602.1163463592529,
          "generation_latency_ms": 3491.001605987549,
          "total_latency_ms": 4093.1179523468018
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the user concerned about egress fees regarding the load balancer hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "It routes traffic to GCP",
          "predicted_answer": "Unknown. The provided context does not mention any concerns about egress fees regarding the load balancer hosted on AWS.",
          "retrieved_context": [
            "User is asking why LoadBalancer2 is on AWS",
            "User wants to check the status of the load balancer on Amazon to ensure traffic isn't blackholed.",
            "User notes that the load balancer connected to the main auth service is an older AWS load balancer, which User finds inefficient."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 609.7102165222168,
          "generation_latency_ms": 3032.2346687316895,
          "total_latency_ms": 3641.9448852539062
        },
        {
          "probe_id": "preference_learning-preference_induction-5",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the CEO prefer infrastructure cost reports to be formatted?",
          "answer_type": "short_answer",
          "gold_answer": "Detailed",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User's cofounder wants a full rundown of infrastructure costs before the board meeting.",
            "User expects Noah to ask about infrastructure costs during the exec meeting.",
            "User is concerned about infrastructure costs."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 722.3961353302002,
          "generation_latency_ms": 3557.790756225586,
          "total_latency_ms": 4280.186891555786
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-6",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What kind of pet does Will own?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User is with Will and Liam.",
            "User is asking who is partnering with Will in the design team.",
            "User says Will is focused"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 736.717700958252,
          "generation_latency_ms": 2913.0611419677734,
          "total_latency_ms": 3649.7788429260254
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific roast of coffee beans did the user ask to be added to the shopping list?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Dark roast.",
          "retrieved_context": [
            "User is completely out of coffee beans.",
            "User wants the usual dark roast.",
            "User is having their second coffee of the day."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 358.5779666900635,
          "generation_latency_ms": 3351.660966873169,
          "total_latency_ms": 3710.2389335632324
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-8",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What does the user identify as the number one priority when dealing with the executive team?",
          "answer_type": "short_answer",
          "gold_answer": "Keeping them happy",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "User expects Noah to ask about infrastructure costs during the exec meeting.",
            "User feels that being the CTO involves more people management than expected",
            "User's cofounder was asking about resource allocation and is stressing out about the burn rate again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 629.126787185669,
          "generation_latency_ms": 4856.630802154541,
          "total_latency_ms": 5485.75758934021
        }
      ],
      "summary": {
        "overall_accuracy": 0.4444444444444444,
        "total_score": 4,
        "count": 9,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 7,
            "accuracy": 0.42857142857142855
          },
          "abstain": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    }
  ]
}