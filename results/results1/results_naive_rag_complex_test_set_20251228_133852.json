{
  "generated_at": "2025-12-28T14:28:34.179360",
  "adapter_type": "NaiveRAGAdapter",
  "num_benchmarks": 50,
  "total_probes": 503,
  "overall_summary": {
    "overall_accuracy": 0.4970178926441352,
    "total_score": 250,
    "count": 503,
    "by_answer_type": {
      "short_answer": {
        "total": 170,
        "count": 408,
        "accuracy": 0.4166666666666667
      },
      "boolean": {
        "total": 9,
        "count": 18,
        "accuracy": 0.5
      },
      "abstain": {
        "total": 71,
        "count": 73,
        "accuracy": 0.9726027397260274
      },
      "verbatim": {
        "total": 0,
        "count": 2,
        "accuracy": 0.0
      },
      "generation": {
        "total": 0,
        "count": 2,
        "accuracy": 0.0
      }
    },
    "by_pillar": {
      "world_modeling": {
        "total": 46,
        "count": 85,
        "accuracy": 0.5411764705882353
      },
      "declarative_reasoning": {
        "total": 59,
        "count": 95,
        "accuracy": 0.6210526315789474
      },
      "temporal_episodic": {
        "total": 12,
        "count": 83,
        "accuracy": 0.14457831325301204
      },
      "preference_learning": {
        "total": 23,
        "count": 75,
        "accuracy": 0.30666666666666664
      },
      "knowledge_boundaries": {
        "total": 85,
        "count": 90,
        "accuracy": 0.9444444444444444
      },
      "procedural_knowledge": {
        "total": 25,
        "count": 75,
        "accuracy": 0.3333333333333333
      }
    }
  },
  "stored_memories": [],
  "benchmark_summaries": [
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_001.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 2.2953224182128906,
      "eval_time_s": 70.75597214698792
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_002.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.7560255527496338,
      "eval_time_s": 71.5946295261383
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_003.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 1.9932494163513184,
      "eval_time_s": 71.78149008750916
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_004.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 1.9027795791625977,
      "eval_time_s": 69.58601784706116
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_005.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.804985523223877,
      "eval_time_s": 71.35511350631714
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_006.json",
      "accuracy": 0.36363636363636365,
      "num_probes": 11,
      "ingestion_time_s": 1.7686684131622314,
      "eval_time_s": 61.6126811504364
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_007.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 1.7248103618621826,
      "eval_time_s": 71.13190007209778
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_008.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.7699000835418701,
      "eval_time_s": 67.50228905677795
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_009.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 1.7870543003082275,
      "eval_time_s": 69.0567135810852
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_010.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 1.7096600532531738,
      "eval_time_s": 65.79268217086792
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_011.json",
      "accuracy": 0.7142857142857143,
      "num_probes": 7,
      "ingestion_time_s": 1.6638333797454834,
      "eval_time_s": 39.29260730743408
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_012.json",
      "accuracy": 1.0,
      "num_probes": 2,
      "ingestion_time_s": 1.8158371448516846,
      "eval_time_s": 9.651219844818115
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_013.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 1.5906548500061035,
      "eval_time_s": 11.576337099075317
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_014.json",
      "accuracy": 1.0,
      "num_probes": 2,
      "ingestion_time_s": 1.720266342163086,
      "eval_time_s": 9.74711298942566
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_015.json",
      "accuracy": 0.5454545454545454,
      "num_probes": 11,
      "ingestion_time_s": 1.8044984340667725,
      "eval_time_s": 65.04884672164917
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_016.json",
      "accuracy": 0.6153846153846154,
      "num_probes": 13,
      "ingestion_time_s": 1.8828856945037842,
      "eval_time_s": 70.51674485206604
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_017.json",
      "accuracy": 1.0,
      "num_probes": 12,
      "ingestion_time_s": 1.8649373054504395,
      "eval_time_s": 65.42112493515015
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_018.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.634047269821167,
      "eval_time_s": 66.04705047607422
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_019.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.736020803451538,
      "eval_time_s": 66.33112907409668
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_020.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.7022826671600342,
      "eval_time_s": 67.09104466438293
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_021.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 1.6844956874847412,
      "eval_time_s": 68.49715113639832
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_022.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 1.784080982208252,
      "eval_time_s": 66.75527453422546
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_023.json",
      "accuracy": 0.375,
      "num_probes": 8,
      "ingestion_time_s": 1.6812989711761475,
      "eval_time_s": 54.39633131027222
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_024.json",
      "accuracy": 0.6,
      "num_probes": 10,
      "ingestion_time_s": 1.852081537246704,
      "eval_time_s": 55.347286224365234
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_025.json",
      "accuracy": 0.6,
      "num_probes": 10,
      "ingestion_time_s": 1.6308026313781738,
      "eval_time_s": 57.93008637428284
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_026.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.7035367488861084,
      "eval_time_s": 66.45756316184998
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_027.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 1.7311162948608398,
      "eval_time_s": 11.539492130279541
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_028.json",
      "accuracy": 0.3333333333333333,
      "num_probes": 12,
      "ingestion_time_s": 1.6589760780334473,
      "eval_time_s": 66.35480618476868
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_029.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.6958434581756592,
      "eval_time_s": 65.84325766563416
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_030.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 1.6421101093292236,
      "eval_time_s": 9.293370962142944
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_031.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 1.6433937549591064,
      "eval_time_s": 11.05293583869934
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_032.json",
      "accuracy": 0.4,
      "num_probes": 10,
      "ingestion_time_s": 1.7211604118347168,
      "eval_time_s": 57.77245616912842
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_033.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 12,
      "ingestion_time_s": 1.6666102409362793,
      "eval_time_s": 68.25260519981384
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_034.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.6376399993896484,
      "eval_time_s": 67.79885244369507
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_035.json",
      "accuracy": 0.7,
      "num_probes": 10,
      "ingestion_time_s": 1.7769815921783447,
      "eval_time_s": 56.22878456115723
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_036.json",
      "accuracy": 0.45454545454545453,
      "num_probes": 11,
      "ingestion_time_s": 1.7032372951507568,
      "eval_time_s": 61.445682764053345
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_037.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 1.682023048400879,
      "eval_time_s": 67.92540168762207
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_038.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 1.624096155166626,
      "eval_time_s": 67.289705991745
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_039.json",
      "accuracy": 0.5,
      "num_probes": 2,
      "ingestion_time_s": 1.7986676692962646,
      "eval_time_s": 11.79163932800293
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_040.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.8345181941986084,
      "eval_time_s": 69.0654866695404
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_041.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.7977628707885742,
      "eval_time_s": 62.40901279449463
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_042.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 9,
      "ingestion_time_s": 1.7504546642303467,
      "eval_time_s": 50.04071640968323
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_043.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 1.7823948860168457,
      "eval_time_s": 63.53350043296814
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_044.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 1.7616662979125977,
      "eval_time_s": 67.97324085235596
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_045.json",
      "accuracy": 0.5,
      "num_probes": 10,
      "ingestion_time_s": 1.7476985454559326,
      "eval_time_s": 54.84619688987732
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_046.json",
      "accuracy": 0.5833333333333334,
      "num_probes": 12,
      "ingestion_time_s": 1.664355754852295,
      "eval_time_s": 67.64169359207153
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_047.json",
      "accuracy": 0.4166666666666667,
      "num_probes": 12,
      "ingestion_time_s": 1.724006175994873,
      "eval_time_s": 71.17117428779602
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_048.json",
      "accuracy": 0.5,
      "num_probes": 12,
      "ingestion_time_s": 1.7510015964508057,
      "eval_time_s": 66.07590246200562
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_049.json",
      "accuracy": 0.25,
      "num_probes": 12,
      "ingestion_time_s": 1.6409649848937988,
      "eval_time_s": 68.0642499923706
    },
    {
      "file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_050.json",
      "accuracy": 0.6666666666666666,
      "num_probes": 9,
      "ingestion_time_s": 1.707235336303711,
      "eval_time_s": 47.32272243499756
    }
  ],
  "detailed_results": [
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_001.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 2.2953224182128906,
      "total_evaluation_time_s": 70.75597214698792,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who deployed the analytics engine situated on Google Cloud Platform?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 9] assistant: Yes, AnalyticsEngine2 is hosted on GCP.",
            "[Session llm-7, Turn 7] assistant: Mostly. AnalyticsEngine9 and AnalyticsEngine5 are on AWS, but AnalyticsEngine2 is actually still sitting over on Google Cloud.",
            "[Session llm-9, Turn 5] assistant: Correct, AnalyticsEngine2 is hosted on GCP. The others, 9 and 5, are on AWS along with the cache layer."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.654045104980469,
          "generation_latency_ms": 3944.5958137512207,
          "total_latency_ms": 3954.249858856201
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Data Scientist",
          "predicted_answer": "Data scientist",
          "retrieved_context": [
            "[Session llm-24, Turn 8] user: Thanks. He's probably distracted by his cat again. What's her name? Luna?",
            "[Session llm-2, Turn 1] assistant: Not yet, he mentioned earlier that Luna knocked over a plant, so he might be running late cleaning up after the cat.",
            "[Session llm-22, Turn 11] assistant: The one of his cat, Luna? Yes, our data scientist seems to be taking a break from the models today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.317419052124023,
          "generation_latency_ms": 4555.697441101074,
          "total_latency_ms": 4568.014860153198
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service that is currently reporting conflicting version numbers?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Microsoft Azure.",
          "retrieved_context": [
            "[Session llm-4, Turn 0] user: Can you check the status of our main database on the Microsoft cloud? I'm seeing some weird version numbers in the logs.",
            "[Session llm-22, Turn 3] assistant: The Azure-hosted node, PrimaryDB7, is signaling mixed versions—it's currently flagging both v1.2.0 and the v2.0.0-beta.",
            "[Session llm-9, Turn 10] user: It's reporting two different versions. I see v1.2.0 on one node and v2.0.0-beta on another."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99815559387207,
          "generation_latency_ms": 3677.600860595703,
          "total_latency_ms": 3686.599016189575
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What specific versions are currently being flagged by the primary database according to the latest logs and config?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "The primary database (PrimaryDB7) is flagging both v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-7, Turn 3] assistant: It looks like PrimaryDB7 is reporting both v1.2.0 and v2.0.0-beta. Since it's hosted on Azure, I can pull the specific deployment logs if you need them.",
            "[Session llm-2, Turn 3] assistant: Looking at PrimaryDB7 now. It's actually showing some conflicting telemetry regarding the version.",
            "[Session llm-13, Turn 0] user: Morning. I need a quick status check on PrimaryDB7. Lucas mentioned some instability during the deployment earlier."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.375810623168945,
          "generation_latency_ms": 4067.6333904266357,
          "total_latency_ms": 4077.0092010498047
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation about consolidating analytics services, what personal appointment was the user reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist appointment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 6] user: Got it. I thought we had one of the analytics services on Google Cloud as well?",
            "[Session llm-3, Turn 10] user: No, I'll talk to him. What about the analytics setup? I feel like we have stuff scattered everywhere.",
            "[Session llm-12, Turn 6] user: Speaking of data, which analytics engine is the odd one out infrastructure-wise? The one not on Amazon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.997699737548828,
          "generation_latency_ms": 3649.472713470459,
          "total_latency_ms": 3661.470413208008
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Was the reminder to call mom set for 5 PM before or after the user discussed the CEO's memo?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 1] assistant: Good morning. Yes, the CEO arrived about twenty minutes ago and went straight into a strategy meeting.",
            "[Session llm-8, Turn 13] assistant: It might be wise to give the CEO a heads-up before she tries to pull a report. Shall I draft a message?",
            "[Session llm-22, Turn 1] assistant: Reminder set for 5 PM. Do you want me to add a specific topic for the call?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.117696762084961,
          "generation_latency_ms": 3641.0844326019287,
          "total_latency_ms": 3652.2021293640137
        },
        {
          "probe_id": "preference_learning-preference_drift-0",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "What is the most recently requested time for the user's reminder to call their mom?",
          "answer_type": "short_answer",
          "gold_answer": "6 PM",
          "predicted_answer": "5 PM",
          "retrieved_context": [
            "[Session llm-22, Turn 0] user: Hey, remind me to call my mom at 5 PM today.",
            "[Session llm-22, Turn 1] assistant: Reminder set for 5 PM. Do you want me to add a specific topic for the call?",
            "[Session llm-24, Turn 7] assistant: Reminder set for 5 PM. By the way, Will sent over those data models you asked for this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.58905029296875,
          "generation_latency_ms": 4094.346046447754,
          "total_latency_ms": 4105.935096740723
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Did the user specify a particular topic to discuss during the call with their mom?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-22, Turn 1] assistant: Reminder set for 5 PM. Do you want me to add a specific topic for the call?",
            "[Session llm-23, Turn 1] assistant: I haven't seen it yet, but that sounds like typical CEO pressure. Is she asking for anything specific?",
            "[Session llm-4, Turn 7] assistant: Understood. Do you want me to draft a summary for Charlotte before the meeting starts?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.021997451782227,
          "generation_latency_ms": 3327.970504760742,
          "total_latency_ms": 3336.9925022125244
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version is the AnalyticsEngine2 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-15, Turn 13] assistant: It is a bit of a mix. AnalyticsEngine2 is over on GCP, but AnalyticsEngine9 and AnalyticsEngine5 are hosted on AWS.",
            "[Session llm-9, Turn 5] assistant: Correct, AnalyticsEngine2 is hosted on GCP. The others, 9 and 5, are on AWS along with the cache layer.",
            "[Session llm-7, Turn 7] assistant: Mostly. AnalyticsEngine9 and AnalyticsEngine5 are on AWS, but AnalyticsEngine2 is actually still sitting over on Google Cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.0321044921875,
          "generation_latency_ms": 3363.274574279785,
          "total_latency_ms": 3374.3066787719727
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Product Designer's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 11] assistant: Noah has his status set to \"Deep Work.\" The Product Designer is likely finalizing the mockups you discussed yesterday.",
            "[Session llm-12, Turn 13] assistant: I haven't seen an upload from the product designer yet.",
            "[Session llm-22, Turn 11] assistant: The one of his cat, Luna? Yes, our data scientist seems to be taking a break from the models today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.116483688354492,
          "generation_latency_ms": 4405.488967895508,
          "total_latency_ms": 4413.605451583862
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant determine the version conflict on the Azure-hosted database?",
          "answer_type": "short_answer",
          "gold_answer": "Comparing logs and config",
          "predicted_answer": "The Azure-hosted node, PrimaryDB7, is signaling mixed versions and reporting",
          "retrieved_context": [
            "[Session llm-10, Turn 13] assistant: Good to hear. I'll keep an eye on the version conflict on the Azure DB.",
            "[Session llm-22, Turn 3] assistant: The Azure-hosted node, PrimaryDB7, is signaling mixed versions—it's currently flagging both v1.2.0 and the v2.0.0-beta.",
            "[Session llm-13, Turn 1] assistant: Good morning, Alex. checking the Azure instances now. It looks like you have both v1.2.0 and the new v2.0.0-beta running simultaneously, which might be causing the conflict."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.021520614624023,
          "generation_latency_ms": 4182.109832763672,
          "total_latency_ms": 4191.131353378296
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What technical task is the DevOps lead currently struggling with?",
          "answer_type": "short_answer",
          "gold_answer": "Azure deployment",
          "predicted_answer": "Stabilizing the deployment.",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: That sounds like a solid plan for your DevOps lead. On a related note, CacheLayer9 is also on AWS, so moving everything there might simplify his workflow.",
            "[Session llm-15, Turn 11] assistant: Good idea, Lucas is the right person for DevOps issues.",
            "[Session llm-22, Turn 5] assistant: I'll flag it for him; our DevOps lead should be able to stabilize the deployment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.098695755004883,
          "generation_latency_ms": 4231.615304946899,
          "total_latency_ms": 4241.714000701904
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_002.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.7560255527496338,
      "total_evaluation_time_s": 71.5946295261383,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the person referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 10] user: That sounds like a botched canary deploy. My cofounder is going to freak out if downtime hits. Who can I pull in to fix this?",
            "[Session llm-23, Turn 13] assistant: Emma is primarily on Frontend. It was likely one of the DevOps engineers, either Ava or Noah.",
            "[Session llm-18, Turn 3] assistant: You can tell your cofounder that AuthService7 is currently hosted on the Google Cloud Platform."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.998580932617188,
          "generation_latency_ms": 3445.171594619751,
          "total_latency_ms": 3458.170175552368
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which DevOps team member is associated with a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 4] user: That explains the error logs. I need to talk to one of the DevOps engineers about this. Who was the one with the cat again? I always forget.",
            "[Session llm-12, Turn 6] user: Thanks. I need to get the DevOps team to kill that beta instance. Can you draft a message to the guy with the cat named Luna? And include his teammate.",
            "[Session llm-23, Turn 13] assistant: Emma is primarily on Frontend. It was likely one of the DevOps engineers, either Ava or Noah."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.966300964355469,
          "generation_latency_ms": 4297.422885894775,
          "total_latency_ms": 4308.389186859131
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service currently reporting a conflict between v1.2.0 and v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "GCP",
          "retrieved_context": [
            "[Session llm-22, Turn 1] assistant: It looks like that service is hosted on GCP, but I'm seeing a conflict. It's reporting both v1.2.0 and v2.0.0-beta running simultaneously.",
            "[Session llm-16, Turn 7] assistant: That seems likely. I'm seeing traffic being served by both v1.2.0 and the v2.0.0-beta build simultaneously.",
            "[Session llm-1, Turn 9] assistant: It looks a bit mixed; I'm seeing signals for both v1.2.0 and the v2.0.0-beta running simultaneously on that service."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998964309692383,
          "generation_latency_ms": 3974.822521209717,
          "total_latency_ms": 3985.821485519409
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "What two specific versions are simultaneously reported as active on AuthService7?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-12, Turn 3] assistant: I see AuthService7 is reporting both v1.2.0 and v2.0.0-beta active simultaneously. Is that intended?",
            "[Session llm-21, Turn 3] assistant: Pulling up the metrics for AuthService7. It looks like there's some mixed signaling here; I'm seeing both v1.2.0 and v2.0.0-beta reporting as active.",
            "[Session llm-15, Turn 1] assistant: Charlotte hasn't flagged anything specific, but looking at AuthService7 on GCP, I am seeing a split in the active versions—it's toggling between v1.2.0 and v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 16.65806770324707,
          "generation_latency_ms": 4072.7550983428955,
          "total_latency_ms": 4089.4131660461426
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event involving a pet did the user speculate might have caused the canary deployment issues?",
          "answer_type": "short_answer",
          "gold_answer": "Luna walking on keyboard",
          "predicted_answer": "Noah's cat, Luna, kept walking on his keyboard.",
          "retrieved_context": [
            "[Session llm-21, Turn 4] user: That's weird. Noah must be messing with a canary deployment. He mentioned his cat, Luna, kept walking on his keyboard this morning, so who knows what he typed.",
            "[Session llm-16, Turn 6] user: Is it a versioning issue? I have a feeling the canary deployment didn't clean up properly.",
            "[Session llm-11, Turn 10] user: That sounds like a botched canary deploy. My cofounder is going to freak out if downtime hits. Who can I pull in to fix this?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.653112411499023,
          "generation_latency_ms": 3720.6170558929443,
          "total_latency_ms": 3733.2701683044434
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "Immediately after mentioning the strategy session with Charlotte, what personal task did the user request a reminder for?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 0] user: Just got out of a long strategy session with Charlotte. My brain is fried.",
            "[Session llm-12, Turn 1] assistant: Did Charlotte give a specific deadline, or is it just the usual \"as soon as possible\"?",
            "[Session llm-5, Turn 12] user: Got it. I have a strategy meeting with Charlotte later, so I need to have these infrastructure details straight."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.006427764892578,
          "generation_latency_ms": 3596.8098640441895,
          "total_latency_ms": 3606.816291809082
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Emma's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 13] assistant: Emma is primarily on Frontend. It was likely one of the DevOps engineers, either Ava or Noah.",
            "[Session llm-4, Turn 13] assistant: Yes, Emma is the Frontend Lead.",
            "[Session llm-1, Turn 5] assistant: Yes, he has a cat named Luna. He shares photos of her quite often."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.01119613647461,
          "generation_latency_ms": 3655.1175117492676,
          "total_latency_ms": 3665.128707885742
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "On which cloud platform is the 'PaymentGateway' service hosted?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: Thanks. I'm reviewing our cloud spread. Which auth service did we put on Azure?",
            "[Session llm-1, Turn 7] assistant: AuthService7 is currently hosted on the Google Cloud Platform.",
            "[Session llm-18, Turn 3] assistant: You can tell your cofounder that AuthService7 is currently hosted on the Google Cloud Platform."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 25.271892547607422,
          "generation_latency_ms": 3631.537914276123,
          "total_latency_ms": 3656.8098068237305
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "At what time does the user repeatedly request to be reminded to pick up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "5 PM",
          "predicted_answer": "5 PM",
          "retrieved_context": [
            "[Session llm-20, Turn 9] assistant: Noted. I have set a reminder for your dry cleaning at 5:00 PM.",
            "[Session llm-19, Turn 7] assistant: considers it done. I've added a reminder to pick up dry cleaning to your daily list.",
            "[Session llm-3, Turn 1] assistant: Noted, I've set a reminder for your dry cleaning at 5 PM today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.809442520141602,
          "generation_latency_ms": 3488.076686859131,
          "total_latency_ms": 3501.8861293792725
        },
        {
          "probe_id": "preference_learning-preference_scope-1",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Has the user expressed a preference for which cloud provider to use for new services?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Thanks. I'm reviewing the architecture diagrams. Is AuthService7 still sitting on the Google Cloud environment?",
            "[Session llm-18, Turn 2] user: Charlotte is stressing about our multi-cloud strategy again. She wants to know where the primary authentication service, number 7, is actually sitting.",
            "[Session llm-0, Turn 6] user: Thanks. I'm reviewing our cloud spread. Which auth service did we put on Azure?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.896610260009766,
          "generation_latency_ms": 4546.841859817505,
          "total_latency_ms": 4558.738470077515
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What administrative action did the assistant take to ensure the version discrepancy was addressed during the user's sync with Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Added a note",
          "predicted_answer": "The assistant added a note for the user's sync with Charlotte regarding the",
          "retrieved_context": [
            "[Session llm-23, Turn 5] assistant: I've added a note for your sync with Charlotte regarding the version discrepancy on the GCP auth service.",
            "[Session llm-15, Turn 1] assistant: Charlotte hasn't flagged anything specific, but looking at AuthService7 on GCP, I am seeing a split in the active versions—it's toggling between v1.2.0 and v2.0.0-beta.",
            "[Session llm-8, Turn 13] assistant: Understood. I'll keep the status report for Charlotte focused on the resolution steps rather than the problem."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.522790908813477,
          "generation_latency_ms": 3678.8835525512695,
          "total_latency_ms": 3693.406343460083
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Which two team members did the assistant suggest contacting to handle an infrastructure rollback?",
          "answer_type": "short_answer",
          "gold_answer": "Ava or Noah",
          "predicted_answer": "Ava and Noah",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: You should probably ping Ava or Noah; either of them can handle the infrastructure rollback.",
            "[Session llm-1, Turn 3] assistant: That would be Noah; he and Ava are both handling the DevOps responsibilities currently.",
            "[Session llm-11, Turn 11] assistant: Since it's an infrastructure issue, you should probably contact Ava or Noah from the DevOps team."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.175798416137695,
          "generation_latency_ms": 3664.7658348083496,
          "total_latency_ms": 3676.9416332244873
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_003.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 1.9932494163513184,
      "total_evaluation_time_s": 71.78149008750916,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 8] user: That's her. Also, we need to sync with the data team. Who do we have besides Sophia?",
            "[Session llm-1, Turn 2] user: A bit chaotic. My cofounder is stressing out about the upcoming board meeting.",
            "[Session llm-14, Turn 0] user: Hey, did my cofounder mention anything about the cloud budget meeting this morning?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.608455657958984,
          "generation_latency_ms": 3430.910110473633,
          "total_latency_ms": 3444.518566131592
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which employee is associated with the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "The Product Designer and Ava.",
          "retrieved_context": [
            "[Session llm-18, Turn 7] assistant: I don't have access to her schedule, but I recall Luna is her cat, correct?",
            "[Session llm-8, Turn 7] assistant: Luna is her cat, right? Hopefully, the cuteness made up for the latency complaints.",
            "[Session llm-23, Turn 7] assistant: Yes, the Product Designer uploaded them an hour ago. She also attached a photo of her cat, Luna, sleeping on the keyboard."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.796215057373047,
          "generation_latency_ms": 4165.234088897705,
          "total_latency_ms": 4174.030303955078
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Is the service currently running the v2.0.0-beta version hosted on the same cloud provider as LoadBalancer2?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 3] assistant: Yes, LoadBalancer2 is currently hosted on GCP, unlike the rest of your core services which are over on Azure.",
            "[Session llm-12, Turn 3] assistant: Correct, LoadBalancer2 is currently hosted on the Google Cloud Platform.",
            "[Session llm-11, Turn 2] user: Mostly the cloud distribution. Are we still running LoadBalancer2 on the Google Cloud Platform?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.536380767822266,
          "generation_latency_ms": 3237.5309467315674,
          "total_latency_ms": 3252.0673274993896
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What exact phrase did the user use to describe their low energy level in Session 23?",
          "answer_type": "verbatim",
          "gold_answer": "I am dragging today",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 6] user: Okay, that's what I thought. What about the load balancer?",
            "[Session llm-23, Turn 5] assistant: I'll add that to the agenda. Both of your Data Scientists have been logging heavy hours lately.",
            "[Session llm-18, Turn 12] user: Right, the multi-cloud experiment. Okay, thanks. I need to grab a coffee."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.77635383605957,
          "generation_latency_ms": 3781.3708782196045,
          "total_latency_ms": 3791.147232055664
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the request to text Ava occur before or after the discussion about the Azure bill?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-14, Turn 5] assistant: Understood. I'll keep an eye on the billing alerts for those Azure resources.",
            "[Session llm-25, Turn 2] user: He's mostly concerned about the Azure bill. It seems high this month.",
            "[Session llm-14, Turn 2] user: No rush. I'm just worried about the bill since we moved most of the heavy lifting to Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.642217636108398,
          "generation_latency_ms": 6482.681035995483,
          "total_latency_ms": 6494.323253631592
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific time slot was secured for the meeting with the CEO regarding the data team?",
          "answer_type": "short_answer",
          "gold_answer": "2 PM",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: Let me check the CEO's schedule... barely. He has a tiny window at 2 PM.",
            "[Session llm-7, Turn 4] user: Yeah, we got through it. Sophia and Will were waiting on the data specs anyway, so the delay didn't hurt their data science work too much.",
            "[Session llm-24, Turn 10] user: Okay. I need to set up a meeting with the data team. That's Will and Sophia, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.964227676391602,
          "generation_latency_ms": 4393.980979919434,
          "total_latency_ms": 4403.945207595825
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which specific service did the user explicitly instruct to exclude from the Azure cost report?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "LoadBalancer2",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: Correct, LoadBalancer2 is hosted on GCP, so I'll exclude it from the Azure report.",
            "[Session llm-25, Turn 2] user: He's mostly concerned about the Azure bill. It seems high this month.",
            "[Session llm-17, Turn 2] user: Yeah, my cofounder was asking about our cloud footprint. Are all our analytics services on Azure?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.120391845703125,
          "generation_latency_ms": 3352.7657985687256,
          "total_latency_ms": 3362.8861904144287
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When asking for system status for the investor call, what level of detail did the user prefer?",
          "answer_type": "short_answer",
          "gold_answer": "High level",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 0] user: Can you pull up the current system status? My cofounder is asking for a stability report before the investor call.",
            "[Session llm-7, Turn 5] assistant: Good to hear Sophia and Will weren't held up. Speaking of data specs, did you need me to look up the status of the analytics services?",
            "[Session llm-15, Turn 13] assistant: I will draft a report for the CEO highlighting the beta testing status on the analytics engine."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.063264846801758,
          "generation_latency_ms": 3196.666479110718,
          "total_latency_ms": 3208.7297439575195
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific version number currently running on AuthService4?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 13] assistant: AuthService4 is running normally on Azure.",
            "[Session llm-6, Turn 9] assistant: AuthService4 is currently hosted on Azure, same as the analytics engines.",
            "[Session llm-11, Turn 8] user: And what version is that one running? Liam thinks we pushed the beta, but I thought we were still on the stable release."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.191129684448242,
          "generation_latency_ms": 3961.4670276641846,
          "total_latency_ms": 3972.658157348633
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current Product Manager at Nebula?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 1] assistant: Good morning, Alex. I can certainly help with that. What specific part of the Nebula Corp infrastructure is the CEO asking about?",
            "[Session llm-1, Turn 1] assistant: Done. I've added that to your schedule. How is everything going over at Nebula today?",
            "[Session llm-12, Turn 0] user: Good morning. Can you pull up the hosting details for our main services at Nebula Corp?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.795759201049805,
          "generation_latency_ms": 3487.8499507904053,
          "total_latency_ms": 3499.645709991455
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What habit of Will's is believed to be the cause of the version conflicts on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Testing in prod",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 13] assistant: Enjoy your coffee. Let me know if you need help resolving the version conflict on the analytics engine.",
            "[Session llm-20, Turn 4] user: Yeah, specifically for the second analytics engine. I feel like we have a version mismatch there.",
            "[Session llm-16, Turn 13] assistant: I have flagged the version conflict on AnalyticsEngine2. Do you want me to notify Will directly?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.589693069458008,
          "generation_latency_ms": 3862.9438877105713,
          "total_latency_ms": 3875.5335807800293
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What logic was applied to exclude LoadBalancer2 from the Azure spend report?",
          "answer_type": "short_answer",
          "gold_answer": "Hosted on GCP",
          "predicted_answer": "LoadBalancer2 was excluded from the Azure spend report because it is hosted on GCP.",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: Correct, LoadBalancer2 is hosted on GCP, so I'll exclude it from the Azure report.",
            "[Session llm-26, Turn 6] user: Actually, yes. Is LoadBalancer2 on Azure as well?",
            "[Session llm-15, Turn 3] assistant: Yes, LoadBalancer2 is currently hosted on GCP, unlike the rest of your core services which are over on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.223958969116211,
          "generation_latency_ms": 3698.58717918396,
          "total_latency_ms": 3710.811138153076
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 8,
            "accuracy": 0.25
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_004.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.9027795791625977,
      "total_evaluation_time_s": 69.58601784706116,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user planning to discuss quarterly goals with?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "[Session llm-21, Turn 4] user: Yeah, exactly. It's a messy mix right now. Also, can you remind me to grab coffee with my cofounder later? Mia wants to chat about the quarterly goals.",
            "[Session llm-3, Turn 7] assistant: I'll add a reminder to get a treat for his cat. By the way, Sophia was asking about the sales projections earlier.",
            "[Session llm-14, Turn 8] user: His cat, Luna, decided to walk right across his keyboard while he was presenting the quarterly targets."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004997253417969,
          "generation_latency_ms": 2602.5173664093018,
          "total_latency_ms": 2612.5223636627197
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which member of the sales team is the owner of Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas",
          "retrieved_context": [
            "[Session llm-14, Turn 9] assistant: Haha, maybe Luna is trying to help out with Sales? She might be a natural closer.",
            "[Session llm-9, Turn 10] user: Right, the sales sync. I hope he doesn't bring up his cat again. I've heard enough stories about Luna for one lifetime.",
            "[Session llm-8, Turn 7] assistant: I haven't heard any updates on Luna lately, but I hope she's doing well! As for Lucas, since he's leading Sales, does he need access to the production dashboard?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.713172912597656,
          "generation_latency_ms": 3405.9410095214844,
          "total_latency_ms": 3415.654182434082
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What cloud provider hosts the load balancer that the DevOps lead was worried about?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud Platform (GCP)",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: Is our DevOps lead worried about LoadBalancer8? I recall that's sitting on GCP.",
            "[Session llm-1, Turn 5] assistant: Our DevOps guy is currently investigating some latency on LoadBalancer8. It looks like it's hosted on the GCP cluster.",
            "[Session llm-12, Turn 2] user: Let's start with the caching. I was talking to our DevOps guy, Liam, and he mentioned some changes to the load balancer on Google's cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999917984008789,
          "generation_latency_ms": 3597.3310470581055,
          "total_latency_ms": 3608.3309650421143
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-0",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the conflicting system logs, which two cloud providers are reportedly hosting CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "GCP and Azure",
          "predicted_answer": "GCP and Azure.",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: That is the strange part. The config logs for CacheLayer3 are conflicting; one tag explicitly says GCP, but another trace implies it is hosted on Azure.",
            "[Session llm-11, Turn 0] user: Morning. I'm trying to figure out what's going on with the infrastructure today. Can you tell me where CacheLayer3 is currently hosted?",
            "[Session llm-3, Turn 4] user: That part is stable. The real confusion is around CacheLayer3. One dashboard says it's on Azure, but the documentation claims Google Cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.784004211425781,
          "generation_latency_ms": 3624.0835189819336,
          "total_latency_ms": 3636.8675231933594
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What ongoing activity explains why CacheLayer3 is reporting two different software versions simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Update migration",
          "predicted_answer": "CacheLayer3 has active instances running on both GCP and Azure, which might be contributing to the version discrepancy.",
          "retrieved_context": [
            "[Session llm-10, Turn 11] assistant: Understood. I will flag the multi-cloud and version discrepancy on CacheLayer3 as a priority for the Sales Leads' demo.",
            "[Session llm-10, Turn 7] assistant: That version mismatch could definitely cause instability. Is CacheLayer3 running on a specific provider?",
            "[Session llm-8, Turn 1] assistant: Good morning to you too! It looks like CacheLayer3 is reporting v1.2.0 in the main logs, though I'm seeing some intermittent signals for v2.0.0-beta as well."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.524894714355469,
          "generation_latency_ms": 3962.566614151001,
          "total_latency_ms": 3971.0915088653564
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the strategy meeting, what specific event interrupted the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Yeah, just got out of a long strategy sync with my cofounder and the sales leads. Intense stuff.",
            "[Session llm-4, Turn 12] user: Thanks. Also, did my cofounder mention when the sales meeting is?",
            "[Session llm-1, Turn 11] assistant: Actually, the Sales Lead is offline. He posted a photo saying his cat, Luna, chewed through his fiber cable."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 47.155141830444336,
          "generation_latency_ms": 4698.158979415894,
          "total_latency_ms": 4745.314121246338
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user frequently consume to cope with work exhaustion?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee/caffeine.",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: Good morning, Alex. I've got the dashboard ready. Hopefully, the caffeine hits soon. What are we looking for?",
            "[Session llm-6, Turn 1] assistant: Added to your list. By the way, Lucas mentioned Luna kept him up all night again, so he might need that caffeine.",
            "[Session llm-5, Turn 5] assistant: I hear you. A caffeine fix sounds like a good idea. While you grab that, should I check the software version running on that instance?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.001110076904297,
          "generation_latency_ms": 3732.09285736084,
          "total_latency_ms": 3743.093967437744
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What item did the user explicitly ask to be added to their shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "coffee beans",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: Morning. Can you add \"buy coffee beans\" to my shopping list? We're completely out at the house.",
            "[Session llm-1, Turn 0] user: Hey, can you add \"buy espresso beans\" to my personal to-do list?",
            "[Session llm-19, Turn 9] assistant: I haven't seen a formal update from the sales lead yet, but he did mention he was a bit distracted because his cat, Luna, had a vet appointment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.510372161865234,
          "generation_latency_ms": 3192.0082569122314,
          "total_latency_ms": 3203.5186290740967
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Backend Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-13, Turn 7] assistant: Yes, Charlotte is the Backend Lead.",
            "[Session llm-1, Turn 11] assistant: Actually, the Sales Lead is offline. He posted a photo saying his cat, Luna, chewed through his fiber cable.",
            "[Session llm-19, Turn 9] assistant: I haven't seen a formal update from the sales lead yet, but he did mention he was a bit distracted because his cat, Luna, had a vet appointment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000322341918945,
          "generation_latency_ms": 3531.437397003174,
          "total_latency_ms": 3543.437719345093
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the 'auth_service'?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 2] user: I'm on my third cup, being the CTO is exhausting this week. What cloud provider is that node actually on?",
            "[Session llm-2, Turn 8] user: Exactly. One monitoring tool places it on GCP, but the billing invoice clearly lists it under Azure.",
            "[Session llm-8, Turn 4] user: Okay, I'll check with Liam since he handles the deployment pipelines. By the way, is that service hosted on GCP or Azure right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000633239746094,
          "generation_latency_ms": 3740.119457244873,
          "total_latency_ms": 3751.120090484619
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What two specific data sources does the assistant cite when analyzing the version conflict on CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "Response headers and heartbeats",
          "predicted_answer": "GCP and Azure.",
          "retrieved_context": [
            "[Session llm-10, Turn 7] assistant: That version mismatch could definitely cause instability. Is CacheLayer3 running on a specific provider?",
            "[Session llm-16, Turn 13] assistant: I'll draft a message to Charlotte and Liam regarding the routing and version conflict on CacheLayer3.",
            "[Session llm-10, Turn 11] assistant: Understood. I will flag the multi-cloud and version discrepancy on CacheLayer3 as a priority for the Sales Leads' demo."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.382341384887695,
          "generation_latency_ms": 4009.5653533935547,
          "total_latency_ms": 4020.9476947784424
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on previous checks, what is the expected stability status of CacheLayer3?",
          "answer_type": "short_answer",
          "gold_answer": "Unstable",
          "predicted_answer": "The version mismatch on CacheLayer3 could definitely cause instability.",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Yeah, Luna. Anyway, back to the mess. Charlotte was asking about the versioning on CacheLayer3. Is it stable?",
            "[Session llm-8, Turn 1] assistant: Good morning to you too! It looks like CacheLayer3 is reporting v1.2.0 in the main logs, though I'm seeing some intermittent signals for v2.0.0-beta as well.",
            "[Session llm-10, Turn 7] assistant: That version mismatch could definitely cause instability. Is CacheLayer3 running on a specific provider?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.155128479003906,
          "generation_latency_ms": 4044.501543045044,
          "total_latency_ms": 4055.656671524048
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_005.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.804985523223877,
      "total_evaluation_time_s": 71.35511350631714,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which executive is repeatedly referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-14, Turn 13] assistant: As CEO, Mia definitely knows which terms resonate with stakeholders.",
            "[Session llm-10, Turn 2] user: Desperately. Anyway, I just got out of a sync with my cofounder. She's worried about the quarterly targets.",
            "[Session llm-14, Turn 12] user: Right, the multi-cloud strategy. My cofounder loves using that buzzword in investor updates."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.789581298828125,
          "generation_latency_ms": 3346.494436264038,
          "total_latency_ms": 3361.284017562866
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific service is co-located on the Google Cloud Platform alongside the main database?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "CacheLayer4",
          "retrieved_context": [
            "[Session llm-11, Turn 7] assistant: Right, PrimaryDB3 is hosted on GCP. The cross-cloud hop between Azure and Google Cloud could definitely introduce lag.",
            "[Session llm-10, Turn 13] assistant: PrimaryDB3 is sitting in Google Cloud Platform alongside that second cache layer.",
            "[Session llm-23, Turn 9] assistant: PrimaryDB3 is hosted on GCP. We also have CacheLayer4 running alongside it on the Google Cloud Platform."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 4349.175930023193,
          "total_latency_ms": 4359.179258346558
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "The Backend Lead set up a specific caching layer. Which cloud platform is that layer hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-10, Turn 11] assistant: Not all of them. The Backend Lead has CacheLayer2 on AWS, but the fourth layer cache is actually over on GCP.",
            "[Session llm-22, Turn 3] assistant: That refers to CacheLayer2, which is indeed on AWS. The Backend Lead set that up last month.",
            "[Session llm-13, Turn 9] assistant: It is distributed. CacheLayer4 is on GCP with the database, but CacheLayer2 is actually running on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.705947875976562,
          "generation_latency_ms": 3613.0199432373047,
          "total_latency_ms": 3623.7258911132812
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two software versions is AuthService7 currently reporting in the logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "AuthService7 is currently reporting v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-15, Turn 5] assistant: AuthService7 is hosted on Azure. It looks like it's reporting two different versions active simultaneously: v1.2.0 and v2.0.0-beta.",
            "[Session llm-16, Turn 1] assistant: Good morning, Alex. I'm looking at AuthService7 now. It seems to be running, but I'm seeing some inconsistent metadata regarding the deployed version.",
            "[Session llm-1, Turn 3] assistant: I see it. It looks like AuthService7 is reporting both v1.2.0 and that new beta, v2.0.0-beta, simultaneously."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.47317886352539,
          "generation_latency_ms": 3422.7116107940674,
          "total_latency_ms": 3434.184789657593
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the most recent interaction, what reason was given for the Sales Lead being inactive or late?",
          "answer_type": "short_answer",
          "gold_answer": "Luna was sick",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 8] user: We really need to fix that latency. Anyway, enough shop talk for a second. Did James send over the invite for the sales kickoff?",
            "[Session llm-15, Turn 2] user: Thanks. Is the sales lead online yet? I need to go over the quarterly numbers before my meeting with Mia.",
            "[Session llm-12, Turn 10] user: Right. James was asking about uptime on the database. He's paranoid about the upcoming sales demo."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.978626251220703,
          "generation_latency_ms": 4314.661979675293,
          "total_latency_ms": 4326.640605926514
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder for concert tickets before or after the reminder for the team lunch pizza?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The user requested a reminder for concert tickets before the reminder for the team lunch pizza. The ticket reminder was set in Session llm-22, and the pizza reminder was requested in Session llm-24.",
          "retrieved_context": [
            "[Session llm-24, Turn 0] user: Morning! Can you set a reminder for me to order pizza for the team lunch at noon?",
            "[Session llm-22, Turn 5] assistant: Reminder set for 12:00 PM to purchase tickets.",
            "[Session llm-2, Turn 3] assistant: Reminder set for pizza at noon. Regarding the service, I see some conflicting signals on the running version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.432979583740234,
          "generation_latency_ms": 4172.458648681641,
          "total_latency_ms": 4185.891628265381
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When offered the option to filter the Sales Lead's distracting messages, what was the user's decision?",
          "answer_type": "short_answer",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 3] assistant: Typical Sales Lead behavior. At least Luna is cute. Do you want me to filter his messages while we work?",
            "[Session llm-10, Turn 4] user: A bit of both. Speaking of sales, James was on a call earlier and I swear I heard meowing in the background.",
            "[Session llm-21, Turn 5] assistant: The Sales Lead usually worries about that. Did he bring his cat onto the video call again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.782646179199219,
          "generation_latency_ms": 4065.512180328369,
          "total_latency_ms": 4077.2948265075684
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-1",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "Why did the user choose not to filter James's messages despite the distractions?",
          "answer_type": "short_answer",
          "gold_answer": "To stay in the loop",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 8] user: Ugh, I totally forgot. Thanks. Speaking of distractions, James won't stop spamming the general channel with photos of his new pet.",
            "[Session llm-20, Turn 2] user: You guessed it. She's worried about the infrastructure costs. Also, James keeps distracting me with photos of his cat, Luna, instead of focusing on the pipeline.",
            "[Session llm-14, Turn 8] user: Typical. James is probably trying to demo features before they're ready. How's his cat doing, by the way?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.583566665649414,
          "generation_latency_ms": 4209.40089225769,
          "total_latency_ms": 4220.98445892334
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the 'PaymentGateway' service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 7] assistant: Looking at AuthService7... it's currently hosted on Azure. However, I'm seeing a mix of versions running right now.",
            "[Session llm-7, Turn 1] assistant: Good morning, Alex. Looking at the infrastructure map, AuthService7 is currently hosted on Azure.",
            "[Session llm-0, Turn 5] assistant: AuthService7 is currently hosted on Azure, if that's the system you are referring to."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.40284538269043,
          "generation_latency_ms": 3183.4635734558105,
          "total_latency_ms": 3194.866418838501
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the name of the Backend Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 5] assistant: That would be Luna. I believe our Sales Lead is working from home today, so his cat probably decided to join the pitch.",
            "[Session llm-14, Turn 5] assistant: Yes, Will is the Backend Lead. By the way, did you want me to add \"buy milk\" to your grocery list?",
            "[Session llm-5, Turn 13] assistant: Well, pets can be good for morale. On a different note, Olivia sent over the new wireframes."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.405778884887695,
          "generation_latency_ms": 3774.009943008423,
          "total_latency_ms": 3784.4157218933105
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What recurring weekly event does the user repeatedly ask the assistant to help prepare for?",
          "answer_type": "short_answer",
          "gold_answer": "Sync with Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: I've made a note to remind you to message the sales lead about his cat. Do you want that reminder set for a specific time?",
            "[Session llm-2, Turn 3] assistant: Reminder set for pizza at noon. Regarding the service, I see some conflicting signals on the running version.",
            "[Session llm-4, Turn 11] assistant: I can schedule a meeting with the Backend Lead if you'd like."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.435104370117188,
          "generation_latency_ms": 4272.602081298828,
          "total_latency_ms": 4283.037185668945
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Is the service managed by the Backend Lead hosted on the same platform as the Analytics Engine?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Yes, the service managed by the Backend Lead (AuthService7) is hosted on",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Okay, thanks. I feel like we have services scattered everywhere. Is the analytics engine on the same provider?",
            "[Session llm-21, Turn 3] assistant: Yes, it is currently hosted on Azure, same as the analytics engine.",
            "[Session llm-20, Turn 5] assistant: Okay, checking the Azure environment. That would cover AuthService7 and AnalyticsEngine9. What did our Backend Lead find?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.47062873840332,
          "generation_latency_ms": 4932.174921035767,
          "total_latency_ms": 4942.64554977417
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 3,
            "accuracy": 0.6666666666666666
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_006.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 11,
      "ingestion_time_s": 1.7686684131622314,
      "total_evaluation_time_s": 61.6126811504364,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'Backend Lead' investigating the database discrepancies?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Is the Backend Lead asking about PrimaryDB8 specifically?",
            "[Session llm-2, Turn 7] assistant: The Backend Lead is currently working on AuthService7, yes. It looks like it's running on AWS.",
            "[Session llm-9, Turn 6] user: Great, another migration ghost. I need to talk to the backend lead about that. Who is that again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.46895980834961,
          "generation_latency_ms": 3908.680200576782,
          "total_latency_ms": 3919.149160385132
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the organizational role of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "The Sales Lead.",
          "retrieved_context": [
            "[Session llm-12, Turn 11] assistant: The ones of Luna? It seems his cat is becoming the unofficial mascot of the office.",
            "[Session llm-9, Turn 9] assistant: If you mean Luna the cat, no, James brings her up quite often.",
            "[Session llm-5, Turn 7] assistant: The Sales Lead hasn't been active yet. He might be dealing with his cat, Luna; didn't he mention a vet appointment?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.516239166259766,
          "generation_latency_ms": 3604.653835296631,
          "total_latency_ms": 3614.1700744628906
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service associated with the CEO's login errors?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 2] user: Right, I'll message him. He's probably busy with his cat, Luna, anyway. Can you check which cloud provider the primary database is on?",
            "[Session llm-21, Turn 5] assistant: If Will is seeing login errors, we should check AuthService7. It looks like we have v1.2.0 stable, but there's also a v2.0.0-beta running. He might be hitting the new version.",
            "[Session llm-15, Turn 5] assistant: I understand. I can generate a report for the CEO if you'd like to clarify the billing sources."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.605573654174805,
          "generation_latency_ms": 3813.2622241973877,
          "total_latency_ms": 3823.8677978515625
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Is the primary database currently confirmed to be hosted in a single environment?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, the documentation and system records indicate contradictory information and lingering tags for Azure,",
          "retrieved_context": [
            "[Session llm-4, Turn 6] user: Right, thanks. Speaking of infrastructure weirdness, where is PrimaryDB8 currently hosted? I thought we moved it.",
            "[Session llm-17, Turn 7] assistant: PrimaryDB8 is currently hosted on AWS, though I see some configuration notes regarding an Azure instance as well. It looks like a multi-cloud setup might be in progress.",
            "[Session llm-14, Turn 14] user: That's confusing. And where is the primary database hosted right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.021137237548828,
          "generation_latency_ms": 4753.1983852386475,
          "total_latency_ms": 4764.219522476196
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific activity caused the user's lack of sleep before the auth service check?",
          "answer_type": "short_answer",
          "gold_answer": "quarterly strategy call",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 8] user: That's annoying. Can you check if the Load Balancer is seeing it? Also, did we deploy the new Auth service?",
            "[Session llm-15, Turn 6] user: Please do. Also, I'm seeing weird logs from the auth service. Is it on the new beta version?",
            "[Session llm-12, Turn 6] user: Okay. I also saw a weird alert regarding the authentication service earlier. Do we know what build is live?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.232231140136719,
          "generation_latency_ms": 4020.479202270508,
          "total_latency_ms": 4033.7114334106445
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What item did the user ask to be reminded to buy while checking the load balancer logs?",
          "answer_type": "short_answer",
          "gold_answer": "coffee beans",
          "predicted_answer": "Coffee beans.",
          "retrieved_context": [
            "[Session llm-22, Turn 0] user: Hey, can you pull up the logs for the load balancer? I need to check the traffic spikes from this morning. Also, remind me to buy coffee beans later.",
            "[Session llm-22, Turn 1] assistant: Checking LoadBalancer5 logs now. I've added coffee beans to your shopping list. Was the traffic spike related to the new campaign?",
            "[Session llm-24, Turn 7] assistant: That aligns with the logs. At least LoadBalancer5 and AnalyticsEngine2 are consistently reporting from AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.531902313232422,
          "generation_latency_ms": 2625.401020050049,
          "total_latency_ms": 2635.9329223632812
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When reviewing the CEO's agendas, which department's representation does the user explicitly prioritize?",
          "answer_type": "short_answer",
          "gold_answer": "engineering",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: Yes, the CEO emailed the agenda an hour ago. It focuses heavily on the upcoming product roadmap.",
            "[Session llm-14, Turn 4] user: Okay, what about the department heads? I always forget who is officially leading what.",
            "[Session llm-15, Turn 5] assistant: I understand. I can generate a report for the CEO if you'd like to clarify the billing sources."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.768009185791016,
          "generation_latency_ms": 4313.040494918823,
          "total_latency_ms": 4322.808504104614
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's mechanic?",
          "answer_type": "short_answer",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 10] user: He's probably distracted. He sent me a picture of his cat, Luna, destroying his couch this morning.",
            "[Session llm-0, Turn 2] user: noted.",
            "[Session llm-0, Turn 4] user: noted."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.018659591674805,
          "generation_latency_ms": 2634.3295574188232,
          "total_latency_ms": 2643.348217010498
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the Analytics Engine currently running?",
          "answer_type": "short_answer",
          "gold_answer": "unknown",
          "predicted_answer": "AnalyticsEngine2",
          "retrieved_context": [
            "[Session llm-23, Turn 14] user: Haha, probably. One last check, is the analytics engine on the same stack?",
            "[Session llm-13, Turn 10] user: Thanks. My cofounder was asking about the analytics engine earlier, is that stable?",
            "[Session llm-13, Turn 11] assistant: Yes, Will can stop worrying. AnalyticsEngine2 is humming along perfectly on AWS."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 2959.423065185547,
          "total_latency_ms": 2969.4230556488037
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-9",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration is suspected to be the cause of the cofounder's login errors?",
          "answer_type": "short_answer",
          "gold_answer": "beta version traffic",
          "predicted_answer": "The cofounder might be hitting the v2.0.0-beta version of AuthService7, while the stable version is v1.2.0.",
          "retrieved_context": [
            "[Session llm-22, Turn 6] user: Please do. And check AuthService7. My cofounder Will is asking why some users are seeing the new login screen and others aren't.",
            "[Session llm-21, Turn 4] user: Okay. My cofounder was having trouble logging in earlier, so maybe it's upstream.",
            "[Session llm-21, Turn 5] assistant: If Will is seeing login errors, we should check AuthService7. It looks like we have v1.2.0 stable, but there's also a v2.0.0-beta running. He might be hitting the new version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.420488357543945,
          "generation_latency_ms": 3821.7456340789795,
          "total_latency_ms": 3833.1661224365234
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What action did the user plan to take regarding the product roadmap agenda sent by the CEO?",
          "answer_type": "generation",
          "gold_answer": "review for engineering representation",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: Yes, the CEO emailed the agenda an hour ago. It focuses heavily on the upcoming product roadmap.",
            "[Session llm-3, Turn 2] user: Just checking the timeline he proposed. He's in full CEO mode today, very demanding. Also, remind me to buy milk later.",
            "[Session llm-15, Turn 5] assistant: I understand. I can generate a report for the CEO if you'd like to clarify the billing sources."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.841991424560547,
          "generation_latency_ms": 3486.705780029297,
          "total_latency_ms": 3495.5477714538574
        }
      ],
      "summary": {
        "overall_accuracy": 0.36363636363636365,
        "total_score": 4,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_007.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.7248103618621826,
      "total_evaluation_time_s": 71.13190007209778,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that the user reviewed the infrastructure bill with?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 0] user: I was reviewing the infrastructure bill with my cofounder this morning, and the cross-cloud costs are getting ridiculous.",
            "[Session llm-16, Turn 2] user: Yeah, my cofounder is asking for a status update on the Azure infrastructure.",
            "[Session llm-0, Turn 9] assistant: I can draft a summary for Will if the CEO needs a breakdown of the infrastructure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.847806930541992,
          "generation_latency_ms": 3283.8757038116455,
          "total_latency_ms": 3294.7235107421875
        },
        {
          "probe_id": "world_modeling-category_membership-2",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Which employees require access to the beta database for their data science models?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas and James",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 4] user: Okay, let's keep it on beta for now unless alerts fire. I need to make sure the data science guys can access it for their models.",
            "[Session llm-12, Turn 8] user: They look clean. Now, I need to make sure the data science team has what they need. Can you confirm if Lucas and James have access to the Azure instances?",
            "[Session llm-0, Turn 12] user: Also, are Lucas and James still assigned to the data science projects?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.999700546264648,
          "generation_latency_ms": 3860.2640628814697,
          "total_latency_ms": 3874.2637634277344
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the database that is running the v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-21, Turn 3] assistant: PrimaryDB9 is hosted on Azure. I'm actually seeing some conflicting signals from it—it's reporting both v1.2.0 and v2.0.0-beta active.",
            "[Session llm-9, Turn 1] assistant: Good morning, Alex. Taking a look at the hosting providers now. PrimaryDB9 is currently on Azure, while PrimaryDB6 is hosted on GCP.",
            "[Session llm-22, Turn 3] assistant: Yes, PrimaryDB9 is currently reporting both v1.2.0 and v2.0.0-beta active. It's hosted on Azure, so checking the update logs there might clarify the version conflict."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.988784790039062,
          "generation_latency_ms": 3501.887083053589,
          "total_latency_ms": 3511.875867843628
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite earlier ambiguous logs, what version was PrimaryDB9 confirmed to be running during the final infrastructure audit?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 2] user: Actually, I'm seeing some weird logs on PrimaryDB9. What version is it supposed to be on?",
            "[Session llm-9, Turn 9] assistant: PrimaryDB9 appears to be running version v1.2.0, though I also see traces of v2.0.0-beta in the logs.",
            "[Session llm-22, Turn 3] assistant: Yes, PrimaryDB9 is currently reporting both v1.2.0 and v2.0.0-beta active. It's hosted on Azure, so checking the update logs there might clarify the version conflict."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.200262069702148,
          "generation_latency_ms": 4126.157283782959,
          "total_latency_ms": 4136.357545852661
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific distraction caused the Product Designer to fail to deliver the UI assets on time?",
          "answer_type": "short_answer",
          "gold_answer": "His cat Luna",
          "predicted_answer": "A cat.",
          "retrieved_context": [
            "[Session llm-20, Turn 5] assistant: Distractions happen, especially with pets involved. Did the Product Designer eventually get the assets over?",
            "[Session llm-7, Turn 12] user: Perfect. By the way, did Oliver upload those new UI assets? He spent twenty minutes showing me photos of his cat this morning instead of working.",
            "[Session llm-23, Turn 12] user: Right. Changing subjects, has Oliver uploaded the new UI assets?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004758834838867,
          "generation_latency_ms": 4177.170038223267,
          "total_latency_ms": 4187.1747970581055
        },
        {
          "probe_id": "temporal_episodic-sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder to buy cat treats before or after the final infrastructure audit?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Typical Product Designer behavior. I assume Luna is his cat?",
            "[Session llm-23, Turn 13] assistant: No updates from the Product Designer yet. He mentioned something about his cat, Luna, causing chaos in his home office today.",
            "[Session llm-20, Turn 5] assistant: Distractions happen, especially with pets involved. Did the Product Designer eventually get the assets over?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.413719177246094,
          "generation_latency_ms": 3101.3708114624023,
          "total_latency_ms": 3110.7845306396484
        },
        {
          "probe_id": "preference_learning-induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What specific beverage does the user consistently consume before starting their morning work?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "[Session llm-6, Turn 1] assistant: That is a terrible way to start the day; I hope you can find some caffeine soon. What is Will stressed about?",
            "[Session llm-1, Turn 1] assistant: Good morning. I can help with that while you caffeine up. What's on your mind?",
            "[Session llm-5, Turn 8] user: That explains the headache. I need a coffee. Remind me to pick up beans later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.174274444580078,
          "generation_latency_ms": 4307.836294174194,
          "total_latency_ms": 4318.010568618774
        },
        {
          "probe_id": "preference_learning-explicit-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to rollback the Azure database to v1.2.0 immediately following the audit?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 2] user: You could say that. My cofounder wants a full infrastructure audit before the board meeting. Can you check which version the main Azure database is running?",
            "[Session llm-24, Turn 3] assistant: Checking PrimaryDB9 on Azure now... It looks like it is currently on the v2.0.0-beta release, though the v1.2.0 rollback is still available.",
            "[Session llm-20, Turn 2] user: Thanks. My cofounder pinged me late last night about the Azure database, number 9. Did we actually update it?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.533809661865234,
          "generation_latency_ms": 3757.431983947754,
          "total_latency_ms": 3767.965793609619
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the current running version of the primary database hosted on GCP?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 13] assistant: Yes, PrimaryDB6 is also hosted on GCP.",
            "[Session llm-14, Turn 14] user: Typical. Hey, weird question, what version is PrimaryDB9 actually running?",
            "[Session llm-9, Turn 1] assistant: Good morning, Alex. Taking a look at the hosting providers now. PrimaryDB9 is currently on Azure, while PrimaryDB6 is hosted on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001659393310547,
          "generation_latency_ms": 3887.6495361328125,
          "total_latency_ms": 3897.651195526123
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 5] assistant: Distractions happen, especially with pets involved. Did the Product Designer eventually get the assets over?",
            "[Session llm-5, Turn 3] assistant: Typical Product Designer behavior. I assume Luna is his cat?",
            "[Session llm-23, Turn 13] assistant: No updates from the Product Designer yet. He mentioned something about his cat, Luna, causing chaos in his home office today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.754419326782227,
          "generation_latency_ms": 3509.3729496002197,
          "total_latency_ms": 3519.127368927002
        },
        {
          "probe_id": "procedural_knowledge-task_tracking-11",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What shopping task did the user ask the assistant to add to their list regarding the Product Designer's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Buy treats for Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: Typical Product Designer behavior. I assume Luna is his cat?",
            "[Session llm-20, Turn 5] assistant: Distractions happen, especially with pets involved. Did the Product Designer eventually get the assets over?",
            "[Session llm-23, Turn 13] assistant: No updates from the Product Designer yet. He mentioned something about his cat, Luna, causing chaos in his home office today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.284828186035156,
          "generation_latency_ms": 3198.317527770996,
          "total_latency_ms": 3209.6023559570312
        },
        {
          "probe_id": "procedural_knowledge-task_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What reminder did the user set for themselves before the standup meeting in the first recorded session?",
          "answer_type": "short_answer",
          "gold_answer": "Grab an extra espresso",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: Reminder set for 2 PM. Do you want me to cross-reference this with the other database clusters while we troubleshoot?",
            "[Session llm-14, Turn 7] assistant: Added to your reminders for 5 PM. By the way, did you see the notes from the sales lead?",
            "[Session llm-9, Turn 5] assistant: Consider it done. I've set a reminder for 12:30 PM for lunch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.339685440063477,
          "generation_latency_ms": 7001.335382461548,
          "total_latency_ms": 7015.675067901611
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 9,
            "accuracy": 0.1111111111111111
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_008.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.7699000835418701,
      "total_evaluation_time_s": 67.50228905677795,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the name of the individual referred to as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 1] assistant: Sure thing. What's on the agenda with your cofounder today?",
            "[Session llm-4, Turn 3] assistant: I can help compile the data for your cofounder. Which part of the system is he concerned about?",
            "[Session llm-17, Turn 10] user: Seriously. Anyway, I need to sync with my cofounder later regarding the quarterly targets."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.603832244873047,
          "generation_latency_ms": 3108.337879180908,
          "total_latency_ms": 3119.9417114257812
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which analytics service is hosted on a different cloud provider than the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine9",
          "predicted_answer": "AnalyticsEngine9 is hosted on a different cloud provider (Azure) than the primary database (AWS).",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: AnalyticsEngine9 is currently hosted on Azure, whereas your other services like AnalyticsEngine6 and the load balancer are on AWS.",
            "[Session llm-15, Turn 7] assistant: Yes, the primary DB is hosted on AWS, same as the load balancer and the cache layer.",
            "[Session llm-13, Turn 5] assistant: That specific instance, AnalyticsEngine9, is hosted on Azure, whereas AnalyticsEngine6 is running on AWS alongside the cache layer."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.205839157104492,
          "generation_latency_ms": 3691.826105117798,
          "total_latency_ms": 3704.0319442749023
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud platform is the database instance that is signalling version v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Amazon cloud",
          "retrieved_context": [
            "[Session llm-10, Turn 4] user: It's the main database. I was sure we pinned it to the stable v1.2.0 release, but I'm seeing logs that suggest the v2.0.0-beta is active.",
            "[Session llm-22, Turn 4] user: Not yet. We were mostly debating the database migration. Are we still sitting on version 1.2.0 for the primary DB, or did the beta go live?",
            "[Session llm-16, Turn 3] assistant: It might be. The primary database is currently straddling v1.2.0 and that v2.0.0-beta release."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998010635375977,
          "generation_latency_ms": 3428.063154220581,
          "total_latency_ms": 3439.061164855957
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What two specific version numbers are currently conflicting in the primary database logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "The two specific version numbers currently conflicting in the primary database logs are 1.2 and 2.0 beta.",
          "retrieved_context": [
            "[Session llm-9, Turn 0] user: I'm staring at the logs for PrimaryDB1 and seeing something weird. My cofounder was asking why we're reporting two different version strings.",
            "[Session llm-0, Turn 2] user: He's mostly confused about the versioning. It looks like PrimaryDB1 is reporting conflicting info.",
            "[Session llm-11, Turn 10] user: Not really. But I'm seeing a weird version flag on PrimaryDB1. Is it on 1.2 or the 2.0 beta?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.13548469543457,
          "generation_latency_ms": 3492.682933807373,
          "total_latency_ms": 3501.8184185028076
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user mention fixing a leaky faucet before or after the discussion about Azure latency?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 11] assistant: Engine 6 is stable on the main cloud provider, but Engine 9—the one over on Azure—is showing slightly higher latency than usual.",
            "[Session llm-23, Turn 4] user: That explains the headache. My cofounder was complaining about the latency there.",
            "[Session llm-8, Turn 8] user: Thanks. Has James or Mia complained about the CRM latency today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999752044677734,
          "generation_latency_ms": 3052.29115486145,
          "total_latency_ms": 3062.290906906128
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What domestic task did the user mention as the reason for their weekend feeling too short?",
          "answer_type": "short_answer",
          "gold_answer": "Fixing a leaky faucet",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 14] user: Do that. Also, what's the weather look like for the weekend?",
            "[Session llm-21, Turn 1] assistant: Good morning, Alex. I can pull up the latest system stats for you. How was your weekend?",
            "[Session llm-9, Turn 2] user: Yeah, typical CEO anxiety. He thinks we're going to crash production during the day. By the way, remind me to pick up treats for Luna later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.070013046264648,
          "generation_latency_ms": 3468.5404300689697,
          "total_latency_ms": 3479.6104431152344
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When requesting a reminder for coffee beans, what specific variety did the user specify?",
          "answer_type": "short_answer",
          "gold_answer": "Dark roast",
          "predicted_answer": "Dark roast.",
          "retrieved_context": [
            "[Session llm-5, Turn 7] assistant: Added \"buy dark roast coffee beans\" to your shopping list.",
            "[Session llm-24, Turn 5] assistant: Noted. I've added coffee beans to your shopping list. Anything else for the list?",
            "[Session llm-5, Turn 6] user: Okay, remind me to buy more coffee beans later. The dark roast."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.522697448730469,
          "generation_latency_ms": 3319.6232318878174,
          "total_latency_ms": 3332.145929336548
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user characterize the billing situation resulting from the multi-cloud setup?",
          "answer_type": "short_answer",
          "gold_answer": "Fragmented",
          "predicted_answer": "The user characterizes the billing situation as \"fragmented\" and refers to the multi-cloud setup",
          "retrieved_context": [
            "[Session llm-5, Turn 8] user: Thanks. Also, looking at our cloud bill. Why is one of the analytics engines billed separately from Amazon?",
            "[Session llm-2, Turn 9] assistant: That would be AnalyticsEngine9 on Azure. It is currently active. Elijah might notice that AnalyticsEngine6 is on AWS, which splits the billing.",
            "[Session llm-13, Turn 3] assistant: Noted, I've set a reminder for your coffee run. Regarding the infrastructure spend, do you want a breakdown by cloud provider since we are multi-cloud?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.015487670898438,
          "generation_latency_ms": 4973.228216171265,
          "total_latency_ms": 4983.243703842163
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 7] assistant: Noted. I'll flag that the Frontend Lead requires backend stability during the UI updates.",
            "[Session llm-21, Turn 13] assistant: I'll check the repository for the Frontend Lead's latest commits.",
            "[Session llm-18, Turn 11] assistant: Liam is currently the Frontend Lead."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.99903678894043,
          "generation_latency_ms": 3191.7946338653564,
          "total_latency_ms": 3201.793670654297
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is the Load Balancer currently reporting?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 7] assistant: LoadBalancer1 looks stable on the Amazon cloud. No alerts there.",
            "[Session llm-24, Turn 1] assistant: Good morning, Alex. You are partially correct. Most of the stack, including the cache layer and the first load balancer, is on AWS.",
            "[Session llm-6, Turn 11] assistant: CacheLayer7 and LoadBalancer1 are both reporting healthy status on the AWS side."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.175373077392578,
          "generation_latency_ms": 3430.0785064697266,
          "total_latency_ms": 3438.253879547119
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What structural factor did the assistant identify as the cause of the latency on the Azure instance?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-region calls",
          "predicted_answer": "Cross-region calls.",
          "retrieved_context": [
            "[Session llm-8, Turn 11] assistant: Engine 6 is stable on the main cloud provider, but Engine 9—the one over on Azure—is showing slightly higher latency than usual.",
            "[Session llm-10, Turn 7] assistant: Is the lag happening between the AWS services and that one analytics engine hosted on Azure?",
            "[Session llm-4, Turn 13] assistant: Noted. AnalyticsEngine9 on Azure might have a latency issue connecting back to the application."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.940696716308594,
          "generation_latency_ms": 4080.806016921997,
          "total_latency_ms": 4089.7467136383057
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What action did the assistant suggest to address the latency issues observed by the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Draft migration plan",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 4] user: That explains the headache. My cofounder was complaining about the latency there.",
            "[Session llm-8, Turn 8] user: Thanks. Has James or Mia complained about the CRM latency today?",
            "[Session llm-8, Turn 11] assistant: Engine 6 is stable on the main cloud provider, but Engine 9—the one over on Azure—is showing slightly higher latency than usual."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.749101638793945,
          "generation_latency_ms": 4217.174053192139,
          "total_latency_ms": 4227.923154830933
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_009.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 1.7870543003082275,
      "total_evaluation_time_s": 69.0567135810852,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to by the user as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 2] user: Weird. I thought I had a sync with my cofounder. He's been stressing about the quarterly board meeting since he's the CEO.",
            "[Session llm-7, Turn 1] assistant: Good morning, Alex. CEOs tend to focus on the budget. Is my cofounder worried about a specific provider?",
            "[Session llm-5, Turn 14] user: Right, we really are all over the place. My cofounder is going to freak out when he sees the consolidated cloud bill."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 14.07003402709961,
          "generation_latency_ms": 3556.333065032959,
          "total_latency_ms": 3570.4030990600586
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the database service that the Backend Lead named Oliver is concerned about?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-14, Turn 4] user: Mostly the database sprawl. Can you remind me which cloud provider is hosting the eighth primary db instance?",
            "[Session llm-15, Turn 11] assistant: Correct, both PrimaryDB7 and AuthService1 are on Azure. It's good that Oliver is handling them.",
            "[Session llm-3, Turn 9] assistant: Yes, AuthService1 is hosted on Azure. It's keeping PrimaryDB7 company over there."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.919498443603516,
          "generation_latency_ms": 3671.5288162231445,
          "total_latency_ms": 3683.448314666748
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Identify the team member who holds the same job title as the owner of the cat named Luna.",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 7] assistant: Yes, that would be Liam's cat. He seems as dedicated to Luna as he is to his responsibilities as Backend Lead.",
            "[Session llm-16, Turn 6] user: Thanks. Speaking of the team, I saw a picture of a cat named Luna on the general Slack channel earlier. Was that Liam's?",
            "[Session llm-24, Turn 5] assistant: Did Liam get a new cat to keep Luna company, or is he just talking about Luna again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.99903678894043,
          "generation_latency_ms": 4075.3772258758545,
          "total_latency_ms": 4085.376262664795
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-2",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two specific version numbers are currently detected running simultaneously on the AWS database?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta",
          "retrieved_context": [
            "[Session llm-22, Turn 2] user: The database status. Specifically the one we have sitting in AWS. I'm seeing some weird versioning flags.",
            "[Session llm-7, Turn 8] user: Mostly. They're arguing about the versioning on the AWS database though.",
            "[Session llm-16, Turn 8] user: Haha, true. Okay, back to tech. What's the status of the database hosted on AWS? I feel like the versioning was getting a bit messy there."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.024309158325195,
          "generation_latency_ms": 2857.4867248535156,
          "total_latency_ms": 2867.511034011841
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Based on the user's schedule, which task occurs earlier in the day: approving the budget report or picking up dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Approving budget report",
          "predicted_answer": "Approving the budget report occurs earlier in the day, as it is due by noon, while picking up dry cleaning is scheduled for 5 PM or 6 PM.",
          "retrieved_context": [
            "[Session llm-26, Turn 1] assistant: Good morning, Alex. You're clear for the next hour, but you have a reminder to approve the budget report by noon.",
            "[Session llm-9, Turn 0] user: Hey, can you set a reminder for me to pick up the dry cleaning at 6 PM? My cofounder just walked in looking frantic, so I might get distracted.",
            "[Session llm-8, Turn 0] user: Morning. Can you set a reminder for me to pick up the dry cleaning at 5 PM?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.211706161499023,
          "generation_latency_ms": 3575.045108795166,
          "total_latency_ms": 3585.256814956665
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-2",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What technical issue is identified as the cause of the latency on the PrimaryDB8 instance?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting versions",
          "predicted_answer": "The latency on PrimaryDB8 is attributed to an attempted upgrade to the v2.0.",
          "retrieved_context": [
            "[Session llm-2, Turn 4] user: The main database cluster. We're seeing some lag on the Amazon-hosted instance, specifically PrimaryDB8.",
            "[Session llm-0, Turn 4] user: Mostly the database fragmentation. Having PrimaryDB8 on Amazon's cloud while the old PrimaryDB7 is still sitting on Azure is inefficient.",
            "[Session llm-23, Turn 14] user: Got it. I'll let James know, he was asking about latency issues for his data models."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.528802871704102,
          "generation_latency_ms": 4489.591121673584,
          "total_latency_ms": 4500.119924545288
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the beta push, do they currently prioritize rapid deployment or production stability?",
          "answer_type": "short_answer",
          "gold_answer": "Production stability",
          "predicted_answer": "Based on the user's concern about \"latency on that main database\" and asking \"Is it",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: As the Backend Lead, it is natural for him to worry about stability. Should I flag the beta version anomalies?",
            "[Session llm-11, Turn 12] user: Doubt it. She usually sticks to the UI work. I'll just ask my cofounder; Lucas should know if we authorized a beta deployment.",
            "[Session llm-21, Turn 4] user: We're seeing some latency on that main database we have over on Amazon. Is it the beta version acting up?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.662555694580078,
          "generation_latency_ms": 4498.238563537598,
          "total_latency_ms": 4508.901119232178
        },
        {
          "probe_id": "preference_learning-explicit_preferences-2",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What beverage does the user explicitly rely on to 'wake up properly' in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "[Session llm-23, Turn 0] user: Good morning. I'm just settling in with a coffee, trying to wake up properly.",
            "[Session llm-26, Turn 11] assistant: Coffee sounds like a solid plan. Do you want me to remind you to drink water too?",
            "[Session llm-2, Turn 0] user: Hey, hope you're awake. I need another espresso before tackling this pile of emails."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.913755416870117,
          "generation_latency_ms": 2674.1976737976074,
          "total_latency_ms": 2683.1114292144775
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the specific running version number of the database hosted on Azure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 8] user: The Azure-hosted stuff seems fine, both the older database and the auth service.",
            "[Session llm-13, Turn 7] assistant: It seems so. PrimaryDB8 is currently hosted on AWS, while PrimaryDB7 remains on Azure.",
            "[Session llm-8, Turn 12] user: Okay, I'll cover for him. On another note, James is asking for access to the Azure-hosted database for his models."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.130167007446289,
          "generation_latency_ms": 3802.309513092041,
          "total_latency_ms": 3812.4396800994873
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Product Designer's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 11] assistant: Understood. Let me know if the Product Designer needs any backend data to support those new sketches.",
            "[Session llm-16, Turn 6] user: Thanks. Speaking of the team, I saw a picture of a cat named Luna on the general Slack channel earlier. Was that Liam's?",
            "[Session llm-16, Turn 7] assistant: Yes, that would be Liam's cat. He seems as dedicated to Luna as he is to his responsibilities as Backend Lead."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.2620849609375,
          "generation_latency_ms": 3924.1786003112793,
          "total_latency_ms": 3933.440685272217
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific data does the user typically request to prepare for a strategy meeting with the CEO?",
          "answer_type": "short_answer",
          "gold_answer": "Latest metrics",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 1] assistant: Always a pleasure when the CEO gets involved in the technical weeds. What specifically do you need to see?",
            "[Session llm-16, Turn 2] user: Let's start with leadership. Just double-checking who is currently handling the chief executive duties and who is owning product design.",
            "[Session llm-14, Turn 3] assistant: CEOs do tend to worry about the bottom line. What specific part of the infrastructure is your cofounder concerned about?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.525226593017578,
          "generation_latency_ms": 3921.5519428253174,
          "total_latency_ms": 3932.077169418335
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-2",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What was the user's explicit instruction regarding the deployment of the beta version to production?",
          "answer_type": "short_answer",
          "gold_answer": "Do not push",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 12] user: Doubt it. She usually sticks to the UI work. I'll just ask my cofounder; Lucas should know if we authorized a beta deployment.",
            "[Session llm-8, Turn 8] user: Yeah, it's reporting v1.2.0 in the monitoring tool, but the deployment script says v2.0.0-beta. It's a mess.",
            "[Session llm-3, Turn 2] user: Right, the v2.0.0-beta rollout. Liam is managing that, I hope he knows what he's doing."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.220932006835938,
          "generation_latency_ms": 3995.009183883667,
          "total_latency_ms": 4006.230115890503
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 10,
            "accuracy": 0.2
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_010.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 1.7096600532531738,
      "total_evaluation_time_s": 65.79268217086792,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service managed by the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-0, Turn 5] assistant: Yes, the Backend Lead is overseeing it. AuthService5 is currently deployed on AWS under his watch.",
            "[Session llm-9, Turn 1] assistant: I can certainly take a look. Is the backend lead concerned about a specific service, or just general system health?",
            "[Session llm-14, Turn 8] user: Thanks. While we're looking at infrastructure, remind me which cloud provider we put that specific cache node on? We're split between two right now."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999679565429688,
          "generation_latency_ms": 2905.8616161346436,
          "total_latency_ms": 2916.8612957000732
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the cofounder concerned about the recent spike in cloud costs?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 9] assistant: I can prepare a cost analysis report for Charlotte if that helps explain the multi-cloud strategy.",
            "[Session llm-25, Turn 8] user: We really need to consolidate this. My cofounder was asking why our cloud bill is so fragmented.",
            "[Session llm-23, Turn 2] user: I've already had two espressos, so let's dive in. My cofounder is asking why our Azure bill spiked this month."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.404586791992188,
          "generation_latency_ms": 4094.1619873046875,
          "total_latency_ms": 4104.56657409668
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: Reminder set: buy cat food. Speaking of cats, did Elijah mention if Luna recovered from that vet visit?",
            "[Session llm-0, Turn 6] user: Okay. And did he ever stop talking about that cat of his? What was it, Luna?",
            "[Session llm-19, Turn 7] assistant: Noted. Are you thinking of getting him something for his cat, Luna?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.75942611694336,
          "generation_latency_ms": 3892.5600051879883,
          "total_latency_ms": 3902.3194313049316
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-1",
          "pillar": "declarative_reasoning",
          "subpillar": "task_state_tracking",
          "question": "Which two version numbers are simultaneously reported by the sixth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-25, Turn 0] user: I'm looking at the dashboard for CacheLayer6, and the version numbers look weird. Is it running the stable release or the beta?",
            "[Session llm-20, Turn 5] assistant: I'm detecting a versioning conflict on CacheLayer6. It is reporting both v1.2.0 and v2.0.0-beta.",
            "[Session llm-3, Turn 8] user: That makes sense. What doesn't make sense is the versioning. The logs for CacheLayer6 show v1.2.0 active, but I keep seeing v2.0.0-beta headers popping up."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.27879524230957,
          "generation_latency_ms": 3813.7853145599365,
          "total_latency_ms": 3826.064109802246
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific activity by the Data Scientist was identified as the cause of the Azure bill spike?",
          "answer_type": "short_answer",
          "gold_answer": "Running heavy models",
          "predicted_answer": "Running queries against the Azure instance.",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: I've already had two espressos, so let's dive in. My cofounder is asking why our Azure bill spiked this month.",
            "[Session llm-24, Turn 13] assistant: Yes, our Data Scientist is currently running queries against that Azure instance.",
            "[Session llm-23, Turn 5] assistant: Checking the logs. Yes, the data scientist's activity correlates with the spike. On a related note, don't forget you have a dentist appointment tomorrow at 2 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.963274002075195,
          "generation_latency_ms": 2919.31414604187,
          "total_latency_ms": 2929.2774200439453
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user requested a ticket be created regarding the deployment manifest, who was the intended recipient?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-25, Turn 2] user: That's annoying. I'll have to get Elijah to look at the deployment manifest.",
            "[Session llm-21, Turn 4] user: Let me guess, someone deployed the beta?",
            "[Session llm-4, Turn 2] user: Yeah, I was talking to our backend lead earlier, and he's convinced we have a deployment mismatch. Can you check the cache nodes?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.684728622436523,
          "generation_latency_ms": 2616.931438446045,
          "total_latency_ms": 2627.6161670684814
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user consistently request before morning meetings?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "[Session llm-15, Turn 10] user: No, wait until he gets in. By the way, remind me to pick up a sandwich before my 1:00 PM meeting.",
            "[Session llm-7, Turn 1] assistant: Good morning, Alex. Sorry to hear about the coffee situation. How can I help you get started despite the caffeine deficit?",
            "[Session llm-1, Turn 1] assistant: I've set a timer for your coffee run. Looking at the map, which specific services do you need location data for?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.037660598754883,
          "generation_latency_ms": 4673.460245132446,
          "total_latency_ms": 4683.497905731201
        },
        {
          "probe_id": "preference_learning-belief_revision-1",
          "pillar": "preference_learning",
          "subpillar": "belief_revision",
          "question": "What is the final confirmed time for the dentist appointment, overriding the initially suggested time?",
          "answer_type": "short_answer",
          "gold_answer": "3 PM",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: Done. I've added the dentist appointment to your schedule for 3 PM.",
            "[Session llm-24, Turn 0] user: Morning. Can you set a reminder for my dentist appointment at 3 PM?",
            "[Session llm-15, Turn 10] user: No, wait until he gets in. By the way, remind me to pick up a sandwich before my 1:00 PM meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.994029998779297,
          "generation_latency_ms": 3408.5934162139893,
          "total_latency_ms": 3418.5874462127686
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is CacheLayer3 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 7] assistant: CacheLayer6? It's running, but I'm seeing conflicting signals regarding the software version.",
            "[Session llm-16, Turn 3] assistant: That would be CacheLayer6. It is currently reporting version v2.0.0-beta, though there is a lingering signal for v1.2.0 as well.",
            "[Session llm-25, Turn 0] user: I'm looking at the dashboard for CacheLayer6, and the version numbers look weird. Is it running the stable release or the beta?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.507013320922852,
          "generation_latency_ms": 2842.632055282593,
          "total_latency_ms": 2851.1390686035156
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current Project Manager for the Nebula organization?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 9] assistant: Good idea to check with Charlotte. Since she's the CEO, she might have fast-tracked it.",
            "[Session llm-4, Turn 3] assistant: Sure thing. I'm assuming you mean the infrastructure Elijah manages? I see a few different clusters.",
            "[Session llm-19, Turn 13] assistant: I haven't seen a commit from the Data Scientist today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.00411605834961,
          "generation_latency_ms": 3032.130718231201,
          "total_latency_ms": 3041.134834289551
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Azure bill spike occur before or after the user first noticed the version conflict on CacheLayer6?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 11] assistant: CacheLayer1 on AWS looks stable. CacheLayer3, which is also on Azure, is showing normal behavior without the version conflict.",
            "[Session llm-12, Turn 13] assistant: CacheLayer1 is on AWS and running normally. It seems the issue is isolated to the version conflict on the Azure cache node.",
            "[Session llm-16, Turn 2] user: Let's check the new caching layer we deployed to Azure. The one running the beta version."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003494262695312,
          "generation_latency_ms": 4386.65509223938,
          "total_latency_ms": 4397.658586502075
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What action by the Backend Lead likely caused the split versioning on the Azure cache instance?",
          "answer_type": "short_answer",
          "gold_answer": "Leaving canary deployment active",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 1] assistant: I'm looking at the Production environment now. It definitely looks split. You've got CacheLayer3 and CacheLayer6 sitting over on Azure.",
            "[Session llm-12, Turn 13] assistant: CacheLayer1 is on AWS and running normally. It seems the issue is isolated to the version conflict on the Azure cache node.",
            "[Session llm-20, Turn 11] assistant: CacheLayer1 on AWS looks stable. CacheLayer3, which is also on Azure, is showing normal behavior without the version conflict."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.19232177734375,
          "generation_latency_ms": 4223.376035690308,
          "total_latency_ms": 4234.568357467651
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 10,
            "accuracy": 0.2
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_011.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 7,
      "ingestion_time_s": 1.6638333797454834,
      "total_evaluation_time_s": 39.29260730743408,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-27, Turn 4] user: Good. My cofounder is panicking about potential downtime during the investor demo later today.",
            "[Session llm-24, Turn 8] user: Yeah, I'll tell her. My cofounder is stressing out about it because he has a board meeting tomorrow and needs the platform stable.",
            "[Session llm-25, Turn 10] user: Thanks. My cofounder is breathing down my neck about reliability today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.023687362670898,
          "generation_latency_ms": 3003.420114517212,
          "total_latency_ms": 3015.443801879883
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the engineer currently debugging the Amazon infrastructure?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 0] user: I'm just getting back from a lunch break, had a pretty terrible sandwich. Anyway, Mia is currently debugging the Amazon infrastructure.",
            "[Session llm-5, Turn 6] user: While she's out, I need to audit our infrastructure split. Which services are we currently running on the Amazon cloud?",
            "[Session llm-24, Turn 1] assistant: Sorry to hear about the lunch disaster. Hopefully, the infrastructure news is better. Which services is she looking at over on AWS?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.009527206420898,
          "generation_latency_ms": 2815.119504928589,
          "total_latency_ms": 2825.1290321350098
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-2",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific missing asset is causing the Sales Lead to panic about the TechGlobal deal?",
          "answer_type": "short_answer",
          "gold_answer": "New dashboard",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 13] assistant: Yes, the Product Designer uploaded the assets an hour ago. Looks like everything is coming together for Nebula Corp.",
            "[Session llm-6, Turn 11] assistant: It makes sense that the CEO wants a solid platform before the sales lead brings in new traffic.",
            "[Session llm-24, Turn 12] user: You know how our sales lead gets. He's worried, but Ava is keeping spirits up; she's just focused on the new UI assets."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.253190994262695,
          "generation_latency_ms": 4047.7359294891357,
          "total_latency_ms": 4057.9891204833984
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-3",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific task is the Data Scientist, Sophia, currently working on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 13] assistant: Glad the product designer is staying zen. One last thing, does Sophia need access to that GCP database for her analysis?",
            "[Session llm-17, Turn 13] assistant: Not yet, but our Data Scientist usually waits until the nightly batch finishes before requesting more resources.",
            "[Session llm-1, Turn 14] user: Perfect. One last check—did Sophia finish the model training?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.445760726928711,
          "generation_latency_ms": 4129.049777984619,
          "total_latency_ms": 4140.495538711548
        },
        {
          "probe_id": "preference_learning-explicit_preferences-4",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What beverage does the user repeatedly request in the morning sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "[Session llm-5, Turn 8] user: Okay. I need to grab a coffee before I fall asleep. Remind me to buy beans later.",
            "[Session llm-25, Turn 0] user: Morning. I need another coffee, this weather is putting me to sleep.",
            "[Session llm-13, Turn 1] assistant: Good morning, Alex. Coffee is essential. What sort of weirdness are we looking at today?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.564016342163086,
          "generation_latency_ms": 3388.167142868042,
          "total_latency_ms": 3399.731159210205
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-5",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions are currently conflicting on LoadBalancer8 according to the latest status check?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Not down, but LoadBalancer8 is reporting conflicting status information. It claims to be running v1.2.0 and v2.0.0-beta simultaneously.",
            "[Session llm-20, Turn 9] assistant: It's still ambiguous. LoadBalancer8 shows active traces of v1.2.0, but there's also a v2.0.0-beta running simultaneously.",
            "[Session llm-16, Turn 8] user: Thanks. Also, my cofounder keeps pestering me about the new beta. Is LoadBalancer8 fully upgraded yet?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.104511260986328,
          "generation_latency_ms": 4540.377616882324,
          "total_latency_ms": 4552.482128143311
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-6",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific metric did the assistant check to verify the connection between the AWS auth service and the Google database?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Latency.",
          "retrieved_context": [
            "[Session llm-10, Turn 11] assistant: The metrics for AuthService4 on AWS look nominal, so Liam might be worrying unnecessarily. However, I'll keep monitoring it.",
            "[Session llm-27, Turn 3] assistant: Checking the route from the auth service on Amazon to PrimaryDB2 on Google... latency is within normal limits, under 20ms.",
            "[Session llm-3, Turn 11] assistant: Yes, AuthService4 is also located on AWS, keeping the latency low between them."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.707450866699219,
          "generation_latency_ms": 3461.329698562622,
          "total_latency_ms": 3471.0371494293213
        }
      ],
      "summary": {
        "overall_accuracy": 0.7142857142857143,
        "total_score": 5,
        "count": 7,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 6,
            "accuracy": 0.6666666666666666
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_012.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 2,
      "ingestion_time_s": 1.8158371448516846,
      "total_evaluation_time_s": 9.651219844818115,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:cachelayer_8 hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "CacheLayer8 is hosted on GCP.",
          "retrieved_context": [
            "[Session llm-23, Turn 5] assistant: It is a bit split. CacheLayer8 is hosted on GCP, but interestingly, CacheLayer7 is still running over on AWS.",
            "[Session llm-9, Turn 9] assistant: Mostly. CacheLayer8 is on GCP, but CacheLayer7 is still hosted on AWS.",
            "[Session llm-24, Turn 4] user: Is that the same environment where we put CacheLayer8?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.389017105102539,
          "generation_latency_ms": 2881.41131401062,
          "total_latency_ms": 2892.8003311157227
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 5] assistant: Added milk to your shopping list. Shall I schedule a lunch reminder with Noah as well?",
            "[Session llm-27, Turn 1] assistant: Reminder set for 4 PM. What specific topics does your cofounder want to cover in the board meeting?",
            "[Session llm-19, Turn 2] user: No, let him handle it. He probably just wants to test the new routing before the weekend. Speaking of which, did I confirm my dinner reservations?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.006355285644531,
          "generation_latency_ms": 2853.748083114624,
          "total_latency_ms": 2864.7544384002686
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 2,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_013.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 2,
      "ingestion_time_s": 1.5906548500061035,
      "total_evaluation_time_s": 11.576337099075317,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:lucas has role?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: He handles it well. Speaking of Lucas, he sent a picture of Luna earlier; she looks adorable.",
            "[Session llm-13, Turn 6] user: Great, a split brain. I bet Lucas was messing with it. Speaking of him, did he say how his cat, Luna, is doing?",
            "[Session llm-1, Turn 3] assistant: Lucas isn't showing as active yet, but your cofounder James is online. The CEO has been logged in since 6 AM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.297868728637695,
          "generation_latency_ms": 3911.7822647094727,
          "total_latency_ms": 3924.0801334381104
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 6] user: Just the usual dark roast. Back to work—did Olivia say anything about the new mockups?",
            "[Session llm-24, Turn 3] assistant: Reminder set for 5 PM. Anything else on the agenda?",
            "[Session llm-15, Turn 12] user: Good to hear. Oh, and did Noah and Olivia finalize the new UI mockups?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.985685348510742,
          "generation_latency_ms": 3194.4751739501953,
          "total_latency_ms": 3204.460859298706
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_014.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 2,
      "ingestion_time_s": 1.720266342163086,
      "total_evaluation_time_s": 9.74711298942566,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:charlotte has role?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Chief Executive Officer.",
          "retrieved_context": [
            "[Session llm-13, Turn 1] assistant: That would be Charlotte; she's currently serving as the Chief Executive Officer.",
            "[Session llm-12, Turn 10] user: Oh right, Charlotte wants to go over the quarterly goals. She's intense about those.",
            "[Session llm-9, Turn 0] user: I just finished a long strategy session with Charlotte. She's pushing hard on the roadmap for the next quarter."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.99650764465332,
          "generation_latency_ms": 2698.3585357666016,
          "total_latency_ms": 2710.355043411255
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 12] user: You guessed it. Distracted everyone during the sprint planning. Speaking of planning, does Liam have the new mockups ready?",
            "[Session llm-20, Turn 1] assistant: Rain can be good for deep work. What is on the agenda for today?",
            "[Session llm-16, Turn 0] user: Hey, did my cofounder mention when she wants that strategy deck?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.14366340637207,
          "generation_latency_ms": 3112.3502254486084,
          "total_latency_ms": 3120.4938888549805
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 2,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_015.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 11,
      "ingestion_time_s": 1.8044984340667725,
      "total_evaluation_time_s": 65.04884672164917,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the team member responsible for the 'messy' deployment scripts mentioned by the user?",
          "answer_type": "short_answer",
          "gold_answer": "Lucas",
          "predicted_answer": "Lucas",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Thanks for the reminder. What version is that thing actually running? Lucas was messing with the deployment scripts earlier.",
            "[Session llm-24, Turn 2] user: Inconsistent how? Did Lucas mess up the deployment tags again?",
            "[Session llm-3, Turn 3] assistant: Typical DevOps worries. Did he specify which part of the stack is acting up?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.655330657958984,
          "generation_latency_ms": 4709.327220916748,
          "total_latency_ms": 4720.982551574707
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the database service that is NOT co-located with the Auth service?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-23, Turn 9] assistant: PrimaryDB2 is with the auth service on GCP, but PrimaryDB7 is hosted over on Azure.",
            "[Session llm-25, Turn 3] assistant: That is correct, AuthService6 is hosted on GCP, along with PrimaryDB2.",
            "[Session llm-12, Turn 7] assistant: Yes, AuthService6 and PrimaryDB2 are both co-located with the load balancer on the Google Cloud environment."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.879350662231445,
          "generation_latency_ms": 3964.033842086792,
          "total_latency_ms": 3973.9131927490234
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Based on the user's diagnosis, which specific database service is likely contributing to the latency reported by the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "PrimaryDB7 on Azure.",
          "retrieved_context": [
            "[Session llm-24, Turn 6] user: Right, thanks. Back to the infrastructure—is AuthService6 affected by the latency?",
            "[Session llm-15, Turn 7] assistant: PrimaryDB2 on GCP is healthy. However, PrimaryDB7, which is over on Azure, is showing slightly higher latency.",
            "[Session llm-17, Turn 7] assistant: That might be related to the multi-cloud topology. Do you want me to verify the latency between the services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.566473007202148,
          "generation_latency_ms": 4097.512483596802,
          "total_latency_ms": 4108.078956604004
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What is the confirmed operational strategy for LoadBalancer9's versioning, clarifying the initial ambiguity?",
          "answer_type": "short_answer",
          "gold_answer": "Canary testing",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 4] user: I'm looking at the logs for LoadBalancer9 and the versioning looks inconsistent. What's it running?",
            "[Session llm-15, Turn 1] assistant: Good morning. Taking a look at LoadBalancer9 now. It appears to be reporting two active versions simultaneously: v1.2.0 and v2.0.0-beta.",
            "[Session llm-4, Turn 3] assistant: It is a bit unusual. I'm detecting a version conflict on LoadBalancer9. It seems to be reporting both v1.2.0 and the v2.0.0-beta simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.891437530517578,
          "generation_latency_ms": 4216.858148574829,
          "total_latency_ms": 4227.749586105347
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the hybrid cloud latency occur before or after the user asked to ping the DevOps lead regarding the alerts?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 5] assistant: Is the latency coming from the cross-cloud communication again?",
            "[Session llm-4, Turn 13] assistant: The Sales Lead is usually the first to notice performance drops during client presentations. Should I draft a ticket to investigate the cross-cloud latency?",
            "[Session llm-10, Turn 4] user: Classic DevOps move, testing in prod. Speaking of production, did we ever consolidate our cloud providers?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.50910758972168,
          "generation_latency_ms": 4201.025485992432,
          "total_latency_ms": 4213.534593582153
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific architectural factor did the user identify as the cause for the dashboard latency around 9 AM?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid cloud setup",
          "predicted_answer": "The user identified cross-cloud latency hitting the cache as the probable cause for the",
          "retrieved_context": [
            "[Session llm-12, Turn 10] user: Right, legacy stuff. Emma keeps asking why the sales dashboard is slow. Probably the cross-cloud latency hitting the cache.",
            "[Session llm-24, Turn 6] user: Right, thanks. Back to the infrastructure—is AuthService6 affected by the latency?",
            "[Session llm-25, Turn 0] user: Hey, just grabbed a coffee. Sophia was asking about the dashboard latency earlier this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.285615921020508,
          "generation_latency_ms": 4159.436225891113,
          "total_latency_ms": 4169.721841812134
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the CacheLayer4 service currently running?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 11] assistant: CacheLayer4 appears to be operational on Azure, though the connection to the GCP services is slower than usual.",
            "[Session llm-15, Turn 9] assistant: Yes, CacheLayer4 is also hosted on Azure.",
            "[Session llm-1, Turn 9] assistant: Yes, CacheLayer4 is running on Azure. Do you want me to check the database on that side as well?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.290145874023438,
          "generation_latency_ms": 3137.7458572387695,
          "total_latency_ms": 3148.036003112793
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 9] assistant: Haha, is Luna his cat? I suppose a few pet photos are inevitable when the frontend lead is working from home.",
            "[Session llm-11, Turn 9] assistant: Yes, the Frontend Lead seems very fond of his cat today.",
            "[Session llm-10, Turn 11] assistant: The Sales Lead is currently in a meeting with Sophia."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.021209716796875,
          "generation_latency_ms": 3759.7944736480713,
          "total_latency_ms": 3769.815683364868
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-9",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration has been identified as problematic for latency when running the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hosting",
          "predicted_answer": "Running PrimaryDB7 and CacheLayer4 on a separate provider (or multi-cloud",
          "retrieved_context": [
            "[Session llm-19, Turn 6] user: That's the part causing headaches. We stuck PrimaryDB7 and the fourth cache layer over there. Lucas is stressing out about the latency between clouds.",
            "[Session llm-15, Turn 7] assistant: PrimaryDB2 on GCP is healthy. However, PrimaryDB7, which is over on Azure, is showing slightly higher latency.",
            "[Session llm-25, Turn 5] assistant: Running PrimaryDB7 and CacheLayer4 on a separate provider can definitely introduce latency overhead. Do you want me to run a ping test between the regions?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.706829071044922,
          "generation_latency_ms": 4452.860593795776,
          "total_latency_ms": 4464.567422866821
        },
        {
          "probe_id": "preference_learning-preference_drift-10",
          "pillar": "preference_learning",
          "subpillar": "preference_drift",
          "question": "At what time does the user currently want to be reminded about their dry cleaning, considering their most recent request?",
          "answer_type": "short_answer",
          "gold_answer": "5:30 PM",
          "predicted_answer": "5 PM",
          "retrieved_context": [
            "[Session llm-14, Turn 4] user: Okay. Totally unrelated, but can you remind me to pick up dry cleaning at 5?",
            "[Session llm-24, Turn 4] user: Ugh, classic. By the way, remind me to pick up my dry cleaning at 5 PM.",
            "[Session llm-0, Turn 3] assistant: I've added the dry cleaning reminder to your evening list."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.701562881469727,
          "generation_latency_ms": 3443.826198577881,
          "total_latency_ms": 3452.5277614593506
        },
        {
          "probe_id": "preference_learning-explicit_preferences-11",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user prefer the assistant to handle communication with the DevOps lead when he is offline?",
          "answer_type": "short_answer",
          "gold_answer": "Ping when active",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 1] assistant: I haven't seen our DevOps guy log in this morning. Do you want me to ping him when he's active?",
            "[Session llm-27, Turn 13] assistant: Understood. I've set an alert threshold for the beta version. I'll let the DevOps team know if anything goes red.",
            "[Session llm-24, Turn 3] assistant: It's possible. Our DevOps lead usually keeps things tidy, but I'm seeing both v1.2.0 and v2.0.0-beta active simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.890483856201172,
          "generation_latency_ms": 3691.9078826904297,
          "total_latency_ms": 3702.798366546631
        }
      ],
      "summary": {
        "overall_accuracy": 0.5454545454545454,
        "total_score": 6,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 11,
            "accuracy": 0.5454545454545454
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_016.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 13,
      "ingestion_time_s": 1.8828856945037842,
      "total_evaluation_time_s": 70.51674485206604,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific system is the user referring to as the 'traffic manager in our Google environment'?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "LoadBalancer2",
          "retrieved_context": [
            "[Session llm-22, Turn 2] user: It's the traffic manager in our Google environment, LoadBalancer2. It's behaving weirdly.",
            "[Session llm-1, Turn 10] user: No worries. Back to the tech stack—Elijah and James were discussing the load balancers earlier. Which one is over on Google Cloud?",
            "[Session llm-7, Turn 3] assistant: You mean LoadBalancer2? I'm pulling up the metrics for that Google Cloud instance now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.35399055480957,
          "generation_latency_ms": 2506.391763687134,
          "total_latency_ms": 2520.7457542419434
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who are the two individuals identified as arguing over the button push for the beta release?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah and James",
          "predicted_answer": "Elijah and James.",
          "retrieved_context": [
            "[Session llm-24, Turn 4] user: Exactly. It looks like it's toggling between v1.2.0 and that v2.0.0-beta we were supposed to canary. Elijah and James are currently arguing over who pushed the button.",
            "[Session llm-22, Turn 5] assistant: That is definitely a conflict. Running a beta release alongside a stable version in the same environment often causes instability.",
            "[Session llm-12, Turn 6] user: Yeah, send a ticket to Elijah and James. They are the DevOps guys, they can figure out who pushed the button."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001420974731445,
          "generation_latency_ms": 2677.1891117095947,
          "total_latency_ms": 2687.190532684326
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who kept the leadership team late going over quarterly strategy?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 2] user: You could say that. Noah kept the leadership team late last night going over the quarterly strategy. Plus, I think our GCP ingress is having an identity crisis.",
            "[Session llm-26, Turn 0] user: Hey, did you see the email from my cofounder about the quarterly targets? I'm trying to focus, but my head is spinning.",
            "[Session llm-17, Turn 5] assistant: That sounds like a productive strategy. Since the Sales Lead is relying on it, I will keep a closer watch on the analytics metrics."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.090755462646484,
          "generation_latency_ms": 2643.542766571045,
          "total_latency_ms": 2654.6335220336914
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "On which cloud provider is the service hosted that the user described as the 'outlier on Microsoft's cloud'?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-14, Turn 10] user: That's the outlier. AuthService3 is sitting on Azure. Oh, did I tell you Mia derailed the meeting earlier?",
            "[Session llm-25, Turn 4] user: Right, okay. And the authentication service? That's the outlier on Microsoft's cloud, isn't it?",
            "[Session llm-3, Turn 0] user: Hey, can you quickly check where our authentication service is currently hosted? My cofounder keeps asking about our cloud spread."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.485649108886719,
          "generation_latency_ms": 3619.974136352539,
          "total_latency_ms": 3630.459785461426
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-5",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two specific software versions are currently conflicting on the GCP load balancer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-23, Turn 7] assistant: I'm looking at `loadbalancer2` on GCP now. It seems to be reporting two active versions simultaneously: v1.2.0 and v2.0.0-beta.",
            "[Session llm-10, Turn 7] assistant: LoadBalancer2 is active on GCP. However, I'm seeing mixed version reports—some instances are on v1.2.0 while others are showing v2.0.0-beta.",
            "[Session llm-20, Turn 5] assistant: Correct, LoadBalancer2 is hosted on GCP. However, I'm seeing some inconsistent data about whether it's on v1.2.0 or the v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.334968566894531,
          "generation_latency_ms": 3323.7268924713135,
          "total_latency_ms": 3334.061861038208
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the infrastructure review discussion, why is latency expected between the primary database and the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "Hosted on different clouds",
          "predicted_answer": "Latency is expected because the primary database (`primarydb3`) is hosted on AWS,",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: That makes sense. `primarydb3` is hosted on AWS, while `authservice3` is over on Azure, so some latency is expected between them.",
            "[Session llm-23, Turn 2] user: Mostly the cross-cloud latency issues. We're worried about the main database connecting to the authentication service.",
            "[Session llm-2, Turn 10] user: Right, that saves us some latency. But the auth service is the odd one out, right? It's on Microsoft's platform?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.622978210449219,
          "generation_latency_ms": 3876.5876293182373,
          "total_latency_ms": 3887.2106075286865
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-7",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What personal task did the user ask to be reminded of during the conversation about cross-cloud latency?",
          "answer_type": "short_answer",
          "gold_answer": "Call dentist",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 12] user: You can say that again. Noah thinks multi-cloud makes us robust, but he doesn't have to manage the latency.",
            "[Session llm-11, Turn 1] assistant: sounds like a typical Tuesday. Is it the multi-cloud latency acting up again or something else?",
            "[Session llm-13, Turn 11] assistant: Cross-cloud latency between GCP and AWS is a likely culprit. What about the main database?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.464191436767578,
          "generation_latency_ms": 3378.5159587860107,
          "total_latency_ms": 3388.9801502227783
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the assistant's preparation for the meeting, what type of infrastructure breakdown does the cofounder usually prefer?",
          "answer_type": "short_answer",
          "gold_answer": "High-level summary",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 1] assistant: Is your cofounder pushing for another infrastructure overhaul?",
            "[Session llm-14, Turn 1] assistant: I can certainly help you visualize the current setup. What specific area is your cofounder focused on today?",
            "[Session llm-13, Turn 1] assistant: I can help with that. Which specific part of the infrastructure is causing concern?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.022401809692383,
          "generation_latency_ms": 4450.06799697876,
          "total_latency_ms": 4460.090398788452
        },
        {
          "probe_id": "preference_learning-preference_scope-9",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user have a stated preference for which cloud provider to review first during infrastructure checks?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No.",
          "retrieved_context": [
            "[Session llm-6, Turn 13] assistant: That is a common strategic view for a CEO. Do you want me to check the latency between the Azure instance and the AWS backend?",
            "[Session llm-10, Turn 4] user: Okay. My cofounder was asking about our multi-cloud footprint during the standup. He's worried about the billing complexity.",
            "[Session llm-6, Turn 12] user: You can say that again. Noah thinks multi-cloud makes us robust, but he doesn't have to manage the latency."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.661840438842773,
          "generation_latency_ms": 4126.260280609131,
          "total_latency_ms": 4136.922121047974
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the current running version of the authentication service on Azure?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 12] user: What about the authentication service? That one is on Azure, right?",
            "[Session llm-25, Turn 5] assistant: Yes, AuthService3 is running on Azure.",
            "[Session llm-16, Turn 13] assistant: Yes, AuthService3 is the one hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.064695358276367,
          "generation_latency_ms": 3620.4941272735596,
          "total_latency_ms": 3632.558822631836
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-11",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the team lead for the Analytics Engine?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 2] user: Let's look at the AWS stack. I know the main database is there, but is the analytics engine there too?",
            "[Session llm-8, Turn 8] user: Got it. And the analytics engine?",
            "[Session llm-3, Turn 13] assistant: AnalyticsEngine7 is hosted on AWS, same as the primary database."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.914470672607422,
          "generation_latency_ms": 2608.962297439575,
          "total_latency_ms": 2617.8767681121826
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What specific action did the assistant propose to help resolve the confusion while the DevOps team argued?",
          "answer_type": "short_answer",
          "gold_answer": "Flag the discrepancy",
          "predicted_answer": "The assistant proposed to flag the discrepancy in the deployment logs.",
          "retrieved_context": [
            "[Session llm-3, Turn 9] assistant: It seems the DevOps team might be in the middle of a canary release or a rollback.",
            "[Session llm-12, Turn 7] assistant: Ticket drafted for the DevOps team regarding the version mismatch.",
            "[Session llm-13, Turn 5] assistant: That sounds like a potential conflict. Do you want me to notify the DevOps team about the risk?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.208845138549805,
          "generation_latency_ms": 4139.480829238892,
          "total_latency_ms": 4149.689674377441
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-13",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the user's routine for preparing for meetings with the cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Review infrastructure overview",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-14, Turn 1] assistant: I can certainly help you visualize the current setup. What specific area is your cofounder focused on today?",
            "[Session llm-11, Turn 7] assistant: Your cofounder does tend to get intense around quarter-end. Do you want me to draft a status update for him?",
            "[Session llm-22, Turn 7] assistant: I don't eat, but I can certainly organize the RSVPs if you'd like. Is your cofounder worried about uptime specifically?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.998632431030273,
          "generation_latency_ms": 3897.660255432129,
          "total_latency_ms": 3906.658887863159
        }
      ],
      "summary": {
        "overall_accuracy": 0.6153846153846154,
        "total_score": 8,
        "count": 13,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 12,
            "accuracy": 0.5833333333333334
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 3,
            "accuracy": 0.6666666666666666
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_017.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 34,
      "num_probes": 12,
      "ingestion_time_s": 1.8649373054504395,
      "total_evaluation_time_s": 65.42112493515015,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as 'my cofounder' by the user?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia",
          "retrieved_context": [
            "[Session llm-1, Turn 2] user: The production one. My cofounder wants to see the latest user metrics before her lunch meeting.",
            "[Session llm-17, Turn 8] user: Yeah, I might. Back to the infrastructure map—James is one of the backend leads, but who is the other one? The guy with the cat named Luna.",
            "[Session llm-23, Turn 8] user: What a mess. I need to talk to my cofounder about consolidating this. Is Olivia free after lunch?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.443304061889648,
          "generation_latency_ms": 2990.710735321045,
          "total_latency_ms": 3003.1540393829346
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who shares the Backend Lead responsibilities with the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "[Session llm-17, Turn 8] user: Yeah, I might. Back to the infrastructure map—James is one of the backend leads, but who is the other one? The guy with the cat named Luna.",
            "[Session llm-31, Turn 7] assistant: I haven't seen his schedule, but I recall he has a cat named Luna if that's who you're referring to.",
            "[Session llm-30, Turn 11] assistant: That must have been Luna. I recall Will mentioning her. I can add a reminder for the cat treat as well if you like."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.302947998046875,
          "generation_latency_ms": 3676.5003204345703,
          "total_latency_ms": 3687.803268432617
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that is currently reporting conflicting versions v1.2.0 and v2.0.0-beta?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-31, Turn 1] assistant: I'm looking at AnalyticsEngine3 right now. It's reporting status, but I'm seeing conflicting version info—some logs say v1.2.0 and others indicate v2.0.0-beta.",
            "[Session llm-0, Turn 7] assistant: It is currently hosted on AWS. However, I'm seeing some noise in the logs; it appears to be reporting both v1.2.0 and v2.0.0-beta simultaneously.",
            "[Session llm-28, Turn 13] assistant: It appears to be running both v1.2.0 and v2.0.0-beta simultaneously."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.569406509399414,
          "generation_latency_ms": 3354.440450668335,
          "total_latency_ms": 3364.0098571777344
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which cloud provider hosts the primary database mentioned during the infrastructure map review?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-23, Turn 4] user: That must be it. Okay, regarding our infrastructure—I feel like our hosting is all over the place. Where is the primary database living these days?",
            "[Session llm-6, Turn 3] assistant: PrimaryDB5 is hosted on Azure. It definitely complicates things compared to the rest of the stack.",
            "[Session llm-6, Turn 2] user: Okay, that explains the noise. It's just a pain managing credentials when our infrastructure is scattered across every cloud provider in existence. Where is the primary database sitting again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.81705093383789,
          "generation_latency_ms": 3648.8378047943115,
          "total_latency_ms": 3659.6548557281494
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the user, what was the specific cause of Will's distraction during their earlier call?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumping on desk",
          "predicted_answer": "According to the user, Will was distracted during their earlier call because Luna kept jumping on his desk.",
          "retrieved_context": [
            "[Session llm-28, Turn 2] user: Yeah, trying to. I was on a call with Will earlier, but he was distracted because Luna kept jumping on his desk.",
            "[Session llm-24, Turn 9] assistant: I haven't seen a completion ticket, but knowing Will, he might be distracted. How is his cat, Luna, doing?",
            "[Session llm-20, Turn 6] user: Ah, my mistake. Speaking of Will, did he ever stop talking about his cat?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.311271667480469,
          "generation_latency_ms": 3307.744026184082,
          "total_latency_ms": 3316.0552978515625
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user requested a 'sanity check' on the infrastructure map, where did the assistant locate AuthService6 and AuthService7?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AuthService6 and AuthService7 were located on AWS.",
          "retrieved_context": [
            "[Session llm-29, Turn 13] assistant: They aren't with the first one. Both AuthService6 and AuthService7 are hosted on AWS.",
            "[Session llm-18, Turn 9] assistant: That's correct. AuthService6 and AuthService7 are both hosted on AWS.",
            "[Session llm-21, Turn 9] assistant: It is a mix. AuthService1 is on GCP, while numbers 6 and 7 are hosted on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.979341506958008,
          "generation_latency_ms": 2995.014190673828,
          "total_latency_ms": 3006.993532180786
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user imply they need before dealing with infrastructure latency or complex messes?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee",
          "retrieved_context": [
            "[Session llm-31, Turn 12] user: No wonder James is complaining about latency. Being a backend lead here must be a headache.",
            "[Session llm-32, Turn 2] user: I knew it. And let me guess, the primary database is somewhere else entirely? I really need to grab a coffee before dealing with this latency.",
            "[Session llm-5, Turn 10] user: We really need to consolidate. My cofounder was asking why our infrastructure bill is coming from three different places."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.09511947631836,
          "generation_latency_ms": 3449.824810028076,
          "total_latency_ms": 3459.9199295043945
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user explicitly state a preference for using GCP over AWS for authentication services?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. The provided documents do not contain any statement from the user explicitly stating a preference for using GCP over AWS for authentication services. The documents only state where various AuthService instances are currently",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Ah, right, AuthService1 on GCP. Networking between that and the AWS infrastructure can be tricky.",
            "[Session llm-1, Turn 11] assistant: AuthService1 is over on GCP. However, AuthService6 and AuthService7 are both hosted on AWS.",
            "[Session llm-19, Turn 7] assistant: Yes, AuthService1 is on GCP, but the other two, 6 and 7, are running on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.388757705688477,
          "generation_latency_ms": 3651.456117630005,
          "total_latency_ms": 3659.8448753356934
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the final score of the game the user missed last night?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-30, Turn 0] user: Hey, did you happen to catch the game last night? I fell asleep before the end.",
            "[Session llm-28, Turn 0] user: Hey, did you catch the game last night?",
            "[Session llm-6, Turn 14] user: Also, did the Giants win last night?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00666618347168,
          "generation_latency_ms": 3785.170793533325,
          "total_latency_ms": 3795.177459716797
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the development environment for the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 7] assistant: Right, the Google Cloud instance. Moving that might require some coordination with the backend team.",
            "[Session llm-16, Turn 10] user: That makes sense. The Frontend Lead is probably testing the beta integration today.",
            "[Session llm-6, Turn 3] assistant: PrimaryDB5 is hosted on Azure. It definitely complicates things compared to the rest of the stack."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 14.359235763549805,
          "generation_latency_ms": 3908.9953899383545,
          "total_latency_ms": 3923.3546257019043
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What general observation did the assistant make about the difficulty of working from home when discussing Will's situation?",
          "answer_type": "short_answer",
          "gold_answer": "Challenging with a cat",
          "predicted_answer": "Working from home with a cat can be challenging.",
          "retrieved_context": [
            "[Session llm-18, Turn 11] assistant: Based on the chat logs, Will is working from home today to take care of Luna.",
            "[Session llm-6, Turn 5] assistant: I don't see any meetings on Will's calendar for the next hour. He might be distracted by Luna, though, if he's working from home.",
            "[Session llm-28, Turn 3] assistant: Working from home with a cat can be challenging. Is he still handling the backend leadership duties effectively?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.328292846679688,
          "generation_latency_ms": 4277.583360671997,
          "total_latency_ms": 4287.911653518677
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What is the documented procedure for migrating AuthService1 from GCP to AWS?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 9] assistant: Ah, right, AuthService1 on GCP. Networking between that and the AWS infrastructure can be tricky.",
            "[Session llm-33, Turn 9] assistant: Not completely. AuthService1 is still lingering on GCP, while instances 6 and 7 are over on AWS.",
            "[Session llm-25, Turn 1] assistant: Not currently. It looks like AuthService1 is still sitting on GCP, while instances 6 and 7 are running on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.985519409179688,
          "generation_latency_ms": 3046.5166568756104,
          "total_latency_ms": 3055.50217628479
        }
      ],
      "summary": {
        "overall_accuracy": 1.0,
        "total_score": 12,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 8,
            "count": 8,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "abstain": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_018.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.634047269821167,
      "total_evaluation_time_s": 66.04705047607422,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user's cofounder that was concerned about the AWS credits?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 4] user: Right, the caches are on Amazon, correct? My cofounder was asking about the AWS credits.",
            "[Session llm-9, Turn 14] user: No, I'll dig into the AWS console myself. Just wanted to sanity check that I wasn't seeing things.",
            "[Session llm-3, Turn 13] assistant: Pulling the logs now. It definitely looks like a partial deployment of the beta build on that specific AWS instance."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.73598861694336,
          "generation_latency_ms": 2783.5116386413574,
          "total_latency_ms": 2794.247627258301
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the Frontend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: Typical behavior for a cat owner. Hopefully, our frontend lead manages to get some work done despite the feline supervision.",
            "[Session llm-13, Turn 9] assistant: She's a very talented cat. I'll make a note that the Frontend Lead has some feline assistance.",
            "[Session llm-12, Turn 7] assistant: Is the frontend lead busy with the new UI rollout? Or is this about his cat again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.353876113891602,
          "generation_latency_ms": 3746.669054031372,
          "total_latency_ms": 3756.0229301452637
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Does the service currently running v2.0.0-beta reside on the same cloud provider as PrimaryDB5?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 5] assistant: PrimaryDB5 is still provisioned within the Google Cloud Platform environment, separate from the cache clusters.",
            "[Session llm-7, Turn 9] assistant: No, actually. PrimaryDB5 and PrimaryDB8 are both hosted on GCP, distinct from the cache layers.",
            "[Session llm-27, Turn 7] assistant: Yes, both PrimaryDB5 and PrimaryDB8 are hosted on GCP."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.020515441894531,
          "generation_latency_ms": 3797.9178428649902,
          "total_latency_ms": 3810.9383583068848
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the latest diagnostics, is CacheLayer7 running a single stable version?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, CacheLayer7 is reporting two different versions simultaneously: v1.2.0",
          "retrieved_context": [
            "[Session llm-18, Turn 11] assistant: Charlotte might have spotted that CacheLayer7 is reporting both v1.2.0 and v2.0.0-beta. As CEO, she keeps a close eye on stability.",
            "[Session llm-13, Turn 12] user: Right. I'm worried about the stability of that one node, CacheLayer7. What version is it reporting?",
            "[Session llm-2, Turn 11] assistant: Checking CacheLayer7... it appears to be reporting two different versions simultaneously: v1.2.0 and v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.07046890258789,
          "generation_latency_ms": 3987.6761436462402,
          "total_latency_ms": 3995.746612548828
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the Frontend Lead running late in the session where the user verified the database providers?",
          "answer_type": "short_answer",
          "gold_answer": "Cat knocked over monitor",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 6] user: That explains the latency. I bet the frontend lead was testing something and forgot to rollback.",
            "[Session llm-14, Turn 0] user: Morning. I need to prep for a sync with the frontend lead in a bit, but first, can we look at the infrastructure logs?",
            "[Session llm-17, Turn 4] user: Okay. I need to sync with the frontend lead about how it's handling the requests. Is he available?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.803533554077148,
          "generation_latency_ms": 4037.5685691833496,
          "total_latency_ms": 4047.3721027374268
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user identify the conflicting versions on CacheLayer7 before or after learning about Lucas's monitor accident?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 11] assistant: Checking CacheLayer7... it appears to be reporting two different versions simultaneously: v1.2.0 and v2.0.0-beta.",
            "[Session llm-18, Turn 11] assistant: Charlotte might have spotted that CacheLayer7 is reporting both v1.2.0 and v2.0.0-beta. As CEO, she keeps a close eye on stability.",
            "[Session llm-16, Turn 5] assistant: It does look unusual. I'm detecting two different version signatures active for CacheLayer7: v1.2.0 and the v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.711359024047852,
          "generation_latency_ms": 3932.905673980713,
          "total_latency_ms": 3944.6170330047607
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine requests, what beverage do they frequently consume?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee/espresso.",
          "retrieved_context": [
            "[Session llm-19, Turn 1] assistant: Good morning, Alex. That sounds intense, but I won't judge your caffeine intake. Do you have a busy schedule today?",
            "[Session llm-13, Turn 1] assistant: Good morning, Alex. I can help organize your notes while the caffeine does its work. What's the priority?",
            "[Session llm-19, Turn 0] user: Morning. I'm barely awake; I think I need a third espresso before I can function."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.287046432495117,
          "generation_latency_ms": 3260.0202560424805,
          "total_latency_ms": 3270.3073024749756
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Has the user explicitly requested to move the caching layers to GCP?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. The user asked \"did we move the cache layers over there too?\" and \"And we kept the caching layers on Amazon?\", which are questions about their current or past state, not an explicit request to move them.",
          "retrieved_context": [
            "[Session llm-16, Turn 1] assistant: I'm looking at the topology now. It looks like you've got the caching layers, specifically 6 and 7, running on AWS, but the primary databases, like 5 and 8, are hosted over on GCP.",
            "[Session llm-25, Turn 3] assistant: Yes, it looks like PrimaryDB5 and PrimaryDB8 are both residing on GCP, while the caching layers are elsewhere.",
            "[Session llm-5, Turn 5] assistant: Connectivity between CacheLayer7 and PrimaryDB5 on GCP looks stable with no dropped packets."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.170843124389648,
          "generation_latency_ms": 4270.36452293396,
          "total_latency_ms": 4278.53536605835
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts PrimaryDB9?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 0] user: Morning. I need to check our cloud spend. Can you remind me which provider is hosting the primary databases right now? My brain is foggy before coffee.",
            "[Session llm-26, Turn 5] assistant: PrimaryDB5 is still provisioned within the Google Cloud Platform environment, separate from the cache clusters.",
            "[Session llm-20, Turn 9] assistant: Correct, PrimaryDB5 and PrimaryDB8 are hosted over on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.000467300415039,
          "generation_latency_ms": 3255.27286529541,
          "total_latency_ms": 3265.273332595825
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of VP of Marketing?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Sophia is a Product Designer. If you need the Frontend Lead, that would be Lucas.",
            "[Session llm-2, Turn 9] assistant: Yes, Liam is also a Product Designer.",
            "[Session llm-7, Turn 14] user: Also, who is handling product design these days? I need to forward some specs."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.012626647949219,
          "generation_latency_ms": 2947.755813598633,
          "total_latency_ms": 2957.768440246582
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What action does the assistant consistently take when the user mentions buying items like coffee beans or cat food?",
          "answer_type": "short_answer",
          "gold_answer": "Adds to shopping list",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: Good morning, Alex. I've added coffee beans to your shopping list. Which part of the infrastructure are we looking at?",
            "[Session llm-12, Turn 9] assistant: Sounds messy. Did the product designers, maybe Sophia or Liam, have anything to say about the UI before the coffee incident?",
            "[Session llm-19, Turn 3] assistant: Typical behavior for a cat owner. Hopefully, our frontend lead manages to get some work done despite the feline supervision."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.99903678894043,
          "generation_latency_ms": 3634.6683502197266,
          "total_latency_ms": 3644.667387008667
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What is the recurring cause of interruptions during the Frontend Lead's standups?",
          "answer_type": "short_answer",
          "gold_answer": "His cat Luna",
          "predicted_answer": "His cat.",
          "retrieved_context": [
            "[Session llm-27, Turn 4] user: Yeah, she interrupted our last standup. Since he's the Frontend Lead, he's always on camera, so she makes frequent appearances.",
            "[Session llm-12, Turn 7] assistant: Is the frontend lead busy with the new UI rollout? Or is this about his cat again?",
            "[Session llm-25, Turn 1] assistant: That sounds expensive. Hopefully your Frontend Lead can get back online soon without further equipment damage."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.210990905761719,
          "generation_latency_ms": 3785.3338718414307,
          "total_latency_ms": 3795.5448627471924
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 7,
            "accuracy": 0.14285714285714285
          },
          "boolean": {
            "total": 2,
            "count": 3,
            "accuracy": 0.6666666666666666
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_019.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.736020803451538,
      "total_evaluation_time_s": 66.33112907409668,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: Good to hear. My cofounder is breathing down my neck about uptime for the investor demo next week.",
            "[Session llm-2, Turn 0] user: Hey, just checking in. Has my cofounder landed yet?",
            "[Session llm-14, Turn 8] user: Right, the multi-cloud headache. Did my cofounder say anything about the board meeting?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.434316635131836,
          "generation_latency_ms": 3579.3025493621826,
          "total_latency_ms": 3590.7368659973145
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud platform hosts the database instance specifically used for legacy enterprise clients?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-24, Turn 5] assistant: That instance is on Azure to support the legacy enterprise clients. It is separate from PrimaryDB1 and PrimaryDB9, which are both running on AWS.",
            "[Session llm-23, Turn 1] assistant: Sure, looking at the infrastructure, PrimaryDB1 and PrimaryDB9 are both deployed on AWS, while PrimaryDB3 is hosted on Azure.",
            "[Session llm-26, Turn 6] user: Got it. I feel like our infrastructure is all over the place. The third primary DB is on Microsoft's cloud, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.173891067504883,
          "generation_latency_ms": 2463.2697105407715,
          "total_latency_ms": 2475.4436016082764
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person responsible for the sales strategy?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "[Session llm-23, Turn 4] user: Thanks. I was looking at the org chart earlier—who is taking point on the sales strategy?",
            "[Session llm-11, Turn 2] user: I know Emma runs the company, but who is leading sales these days?",
            "[Session llm-6, Turn 10] user: Thanks. I need to coordinate with Sales on that one. Who is leading that team right now? My brain is fried."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.578083038330078,
          "generation_latency_ms": 2882.7719688415527,
          "total_latency_ms": 2894.350051879883
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-0",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Does the user consider the conflicting version flags on PrimaryDB1 to be an error or an expected state?",
          "answer_type": "short_answer",
          "gold_answer": "Expected state",
          "predicted_answer": "The user considers the conflicting version flags on PrimaryDB1 to be an error or an unexpected state,",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: PrimaryDB1 is running, but I'm seeing some mixed signals on the versioning. It's flagging both v1.2.0 and the v2.0.0-beta.",
            "[Session llm-11, Turn 12] user: What version is PrimaryDB1 running right now?",
            "[Session llm-26, Turn 11] assistant: PrimaryDB1 is mainly reporting v1.2.0, but there are indicators of v2.0.0-beta running as well."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.013103485107422,
          "generation_latency_ms": 4056.1652183532715,
          "total_latency_ms": 4066.178321838379
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user explain the reason for the version conflict on PrimaryDB1 before or after adding coffee beans to their shopping list?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: PrimaryDB1 is running, but I'm seeing some mixed signals on the versioning. It's flagging both v1.2.0 and the v2.0.0-beta.",
            "[Session llm-11, Turn 12] user: What version is PrimaryDB1 running right now?",
            "[Session llm-4, Turn 5] assistant: It looks like PrimaryDB1 is running v1.2.0, though I'm seeing some references to a v2.0.0-beta being tested."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.00213623046875,
          "generation_latency_ms": 3727.1504402160645,
          "total_latency_ms": 3737.152576446533
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific errand did the user ask to be reminded about for 5:30 PM?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 13] assistant: I've set a timer for one hour to remind you about lunch.",
            "[Session llm-19, Turn 4] user: Maybe later. Right now, I just need to get through this morning. Did I have any reminders set for 10 AM?",
            "[Session llm-8, Turn 0] user: Can you set a reminder for me to pick up the dry cleaning at 5 PM?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.379863739013672,
          "generation_latency_ms": 3308.530569076538,
          "total_latency_ms": 3317.9104328155518
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What communication style does the user imply is preferred during early morning interactions before they have had caffeine?",
          "answer_type": "short_answer",
          "gold_answer": "Brief",
          "predicted_answer": "The user implies a preference for brief communication.",
          "retrieved_context": [
            "[Session llm-22, Turn 1] assistant: Good morning, Alex. Sounds like a rough start. I'll keep things brief until the caffeine hits.",
            "[Session llm-16, Turn 1] assistant: Good morning, Alex. Coffee is probably in order then. What kept you up?",
            "[Session llm-2, Turn 13] assistant: Enjoy the caffeine boost, Alex. I'll keep monitoring the systems while you're away."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.590003967285156,
          "generation_latency_ms": 4710.01410484314,
          "total_latency_ms": 4721.604108810425
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What beverage does the user imply is necessary for them to function effectively in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "[Session llm-16, Turn 1] assistant: Good morning, Alex. Coffee is probably in order then. What kept you up?",
            "[Session llm-15, Turn 0] user: Morning. I'm barely awake, need a second cup of coffee. Can you check the status of our main database instance?",
            "[Session llm-26, Turn 1] assistant: Good morning, Alex. I've added coffee beans to the list. Anything else you need help with to start the day?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.002613067626953,
          "generation_latency_ms": 3764.3256187438965,
          "total_latency_ms": 3774.3282318115234
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the current VP of Engineering?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 5] assistant: Oliver is the Data Scientist, and Sophia is the Frontend Lead.",
            "[Session llm-14, Turn 5] assistant: Likely. As the Data Scientist, he usually wants the latest features for his models.",
            "[Session llm-23, Turn 7] assistant: Correct, Oliver is the Data Scientist on the team."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.635448455810547,
          "generation_latency_ms": 3393.4874534606934,
          "total_latency_ms": 3403.122901916504
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts PrimaryDB2?",
          "answer_type": "abstain",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 8] user: Yeah, specifically the hosting providers. Where is PrimaryDB1 hosted again?",
            "[Session llm-1, Turn 7] assistant: Yes, PrimaryDB3 is fully operational on that cloud provider.",
            "[Session llm-27, Turn 13] assistant: PrimaryDB3 is currently hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.302543640136719,
          "generation_latency_ms": 3105.311155319214,
          "total_latency_ms": 3115.6136989593506
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical process explains why a database might report two different versions simultaneously according to the user?",
          "answer_type": "short_answer",
          "gold_answer": "Blue-green deployment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 12] user: Not yet. I'm also confused about the versioning on that first primary database. What's running there?",
            "[Session llm-3, Turn 2] user: Yeah, specifically about PrimaryDB1. I'm seeing some weird telemetry. What version is it actually reporting?",
            "[Session llm-25, Turn 2] user: Mostly the main database node we have on Amazon's cloud. PrimaryDB1. I feel like the version reporting was acting up yesterday."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.404991149902344,
          "generation_latency_ms": 3855.1011085510254,
          "total_latency_ms": 3866.5060997009277
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Who does the user identify as the point of contact for UI issues caused by API changes?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia",
          "retrieved_context": [
            "[Session llm-22, Turn 4] user: Weird. I thought we held off on the 2.0.0-beta push. I'll have to ping Sophia about that later, she might notice issues on the frontend if the API changed.",
            "[Session llm-16, Turn 13] assistant: That is a significant discrepancy. Does the frontend team know about the potential instability?",
            "[Session llm-24, Turn 12] user: Haha, typical. Luna is basically the mascot of the design team at this point. Did the Product Designer actually include the UI specs though?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.482311248779297,
          "generation_latency_ms": 3895.73335647583,
          "total_latency_ms": 3906.2156677246094
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_020.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.7022826671600342,
      "total_evaluation_time_s": 67.09104466438293,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who did the user plan to meet for coffee prior to the board meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "The user planned to meet their cofounder for coffee.",
          "retrieved_context": [
            "[Session llm-23, Turn 0] user: Hey, can you set a reminder for 2 PM? I need to grab coffee with my cofounder before the board meeting.",
            "[Session llm-13, Turn 4] user: Ugh, I need more coffee before dealing with version mismatches. Did James send you the meeting invite for later?",
            "[Session llm-5, Turn 1] assistant: Good morning, Alex. I hope the coffee machine is cooperating. The dashboard shows green across the board, though I see a few alerts from earlier."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.872934341430664,
          "generation_latency_ms": 2679.6560287475586,
          "total_latency_ms": 2692.5289630889893
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the cache instance identified as causing the cross-cloud hop latency?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-2, Turn 7] assistant: Right, CacheLayer6. Cross-cloud latency can be tricky. Is the other cache layer behaving normally?",
            "[Session llm-23, Turn 4] user: I am. We have that weird hybrid setup where the sixth cache layer is sitting over on Azure, and I think it's causing latency.",
            "[Session llm-23, Turn 5] assistant: That cross-cloud hop to CacheLayer6 is definitely a potential bottleneck. Is the other cache instance performing any better?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.023521423339844,
          "generation_latency_ms": 3338.6032581329346,
          "total_latency_ms": 3349.6267795562744
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "[Session llm-16, Turn 9] assistant: He hasn't confirmed, but Luna is usually the star of the show when she visits.",
            "[Session llm-18, Turn 5] assistant: The Sales Lead should be fine, though he did mention he might be late because of his cat, Luna.",
            "[Session llm-6, Turn 7] assistant: He's currently on a call, likely trying to close that new account. He did mention he has to leave early to take Luna to the vet."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.675575256347656,
          "generation_latency_ms": 3907.6552391052246,
          "total_latency_ms": 3916.3308143615723
        },
        {
          "probe_id": "declarative_reasoning-state_tracking-3",
          "pillar": "declarative_reasoning",
          "subpillar": "task_state_tracking",
          "question": "Which two software versions were detected running simultaneously on the AWS cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta",
          "retrieved_context": [
            "[Session llm-23, Turn 13] assistant: Understood. I'll draft an alert regarding the version conflict on the AWS cache node and send it out.",
            "[Session llm-22, Turn 8] user: Thanks. Back to the infra, what version is the AWS cache running right now?",
            "[Session llm-21, Turn 7] assistant: You mean CacheLayer7? It's deployed on AWS, but are we sure about the versioning?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.570671081542969,
          "generation_latency_ms": 3746.891975402832,
          "total_latency_ms": 3755.462646484375
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What architectural configuration did the user identify as the primary cause of backend latency?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hop",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 8] user: That explains the latency spikes. Wait, did I order lunch?",
            "[Session llm-10, Turn 8] user: Perfect. My cofounder was asking about latency between those two earlier this morning.",
            "[Session llm-5, Turn 4] user: I'm seeing some weird latency. I could have sworn we pinned it to the stable v1.2.0 release, but the headers look... experimental."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.564565658569336,
          "generation_latency_ms": 3956.058979034424,
          "total_latency_ms": 3966.623544692993
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user notice the deployment glitch on CacheLayer7 before or after the discussion about the board meeting?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 9] assistant: It looks like CacheLayer7 is flagging as version 2.0.0-beta in the logs, though the configuration manifests still reference v1.2.0. There might be a drift there.",
            "[Session llm-23, Turn 7] assistant: That's problematic. Is CacheLayer7 running a mix of the stable v1.2.0 and that new v2.0.0-beta you were testing?",
            "[Session llm-20, Turn 6] user: I suspect so. We're looking at CacheLayer7. It seems like the deployment to Amazon's cloud got messy."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.45029067993164,
          "generation_latency_ms": 3810.3315830230713,
          "total_latency_ms": 3821.781873703003
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about latency, which infrastructure setup do they appear to dislike?",
          "answer_type": "short_answer",
          "gold_answer": "Hybrid setup",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 8] user: Perfect. My cofounder was asking about latency between those two earlier this morning.",
            "[Session llm-22, Turn 12] user: Okay, I'll ask our data scientist to look into the latency logs later.",
            "[Session llm-27, Turn 7] assistant: CacheLayer6 on Azure is reporting healthy metrics with standard latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.838342666625977,
          "generation_latency_ms": 4218.586444854736,
          "total_latency_ms": 4228.424787521362
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Why did the user explicitly state they wanted to buy a toy for Luna?",
          "answer_type": "short_answer",
          "gold_answer": "To please James",
          "predicted_answer": "The user wanted to buy a toy for Luna because James won't stop talking about his cat, and the user wants to get on James's good side.",
          "retrieved_context": [
            "[Session llm-11, Turn 7] assistant: Will do. Should I ask him about the sales figures or how Luna is doing?",
            "[Session llm-13, Turn 10] user: Right, good point. James just slacked me a picture of Luna instead of the quarterly figures.",
            "[Session llm-24, Turn 0] user: Hey, remind me to pick up a toy for Luna later. James won't stop talking about his cat, so I might as well get on his good side."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.53221321105957,
          "generation_latency_ms": 2880.547046661377,
          "total_latency_ms": 2890.0792598724365
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Data Scientist's pet?",
          "answer_type": "abstain",
          "gold_answer": "Not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 13] assistant: I'll make a note to ask about his cat. Did Charlotte say anything about the data anomalies?",
            "[Session llm-11, Turn 8] user: Ask about the cat, he's stressed enough about the numbers. Has Charlotte complained about data access lately?",
            "[Session llm-19, Turn 13] assistant: The data science team hasn't marked the task as complete yet, but Charlotte usually updates the repository around noon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.59865951538086,
          "generation_latency_ms": 4085.8938694000244,
          "total_latency_ms": 4096.492528915405
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the database layer?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 1] assistant: Good morning, Alex. CacheLayer7 is currently hosted on AWS, while CacheLayer6 is running on Azure. Also, reminder to buy milk on your way home.",
            "[Session llm-10, Turn 1] assistant: That specific service, CacheLayer6, is actually hosted on Azure, which separates it from the rest of your primary stack.",
            "[Session llm-19, Turn 5] assistant: It is a bit fragmented. CacheLayer6 is currently stable over on the Microsoft cloud, while the newer layer 7 setup is running within the Amazon ecosystem alongside the main load balancer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.132955551147461,
          "generation_latency_ms": 3334.9106311798096,
          "total_latency_ms": 3346.043586730957
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What risk did the assistant identify regarding the concurrent execution of two versions on the same instance?",
          "answer_type": "short_answer",
          "gold_answer": "Data inconsistency",
          "predicted_answer": "The assistant identified that running two versions concurrently on the same instance could cause data inconsistency.",
          "retrieved_context": [
            "[Session llm-24, Turn 5] assistant: That is definitely a problem. Running two versions concurrently on the same instance could cause data inconsistency.",
            "[Session llm-9, Turn 7] assistant: Is this related to the versioning discrepancy you mentioned yesterday?",
            "[Session llm-0, Turn 11] assistant: It's reporting a conflict. I see v1.2.0 active, but there are also heartbeats from v2.0.0-beta on the same instance."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.544921875,
          "generation_latency_ms": 3568.1464672088623,
          "total_latency_ms": 3576.6913890838623
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant verify the status of the traffic distributor on Amazon?",
          "answer_type": "short_answer",
          "gold_answer": "Pulled up metrics",
          "predicted_answer": "The assistant stated they were \"Pulling up the metrics now\" and then asked for confirmation that it was the service running on AWS.",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Yeah, actually. Can you pull up the metrics for our main traffic distributor? The one hosted on Amazon.",
            "[Session llm-5, Turn 7] assistant: I missed the standup. Did the Sales Lead have connection issues again?",
            "[Session llm-24, Turn 3] assistant: Pulling up the metrics now. That is the service running on AWS, correct?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.001970291137695,
          "generation_latency_ms": 4004.7316551208496,
          "total_latency_ms": 4013.7336254119873
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_021.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.6844956874847412,
      "total_evaluation_time_s": 68.49715113639832,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that requested a status update on the production environment?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 0] user: My cofounder is asking for a status update on the production environment again.",
            "[Session llm-16, Turn 12] user: Okay, well, my cofounder is asking for a status update on the platform.",
            "[Session llm-9, Turn 8] user: Did my cofounder send over the updated quarterly goals yet?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.3912353515625,
          "generation_latency_ms": 2965.714931488037,
          "total_latency_ms": 2976.1061668395996
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Backend Lead",
          "predicted_answer": "Backend Lead",
          "retrieved_context": [
            "[Session llm-26, Turn 6] user: Yeah, he probably let Luna walk on the enter key. Remind me to buy cat treats for her, by the way.",
            "[Session llm-23, Turn 8] user: Yeah, he had to take Luna to the vet. Typical cat drama. Anyway, since our backend lead is out, I might need to ask Charlotte to look into the API gateway.",
            "[Session llm-9, Turn 3] assistant: Will do. Is this regarding the backend infrastructure he oversees, or did you just want to ask how his cat Luna is doing?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.994745254516602,
          "generation_latency_ms": 3726.1645793914795,
          "total_latency_ms": 3736.159324645996
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service managed by the Data Scientist hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-13, Turn 5] assistant: Yes, I can confirm that the analytics engine is hosted on Azure.",
            "[Session llm-1, Turn 5] assistant: No, the analytics engine is currently hosted on Azure.",
            "[Session llm-1, Turn 4] user: Interesting. Sophia must be testing something new for her data models. Is that service running on Google's cloud?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.723995208740234,
          "generation_latency_ms": 3950.573444366455,
          "total_latency_ms": 3962.2974395751953
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Which two versions of the analytics engine are currently reporting conflicting signals in the logs?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-25, Turn 1] assistant: Good morning, Alex. I'm checking AnalyticsEngine1. I'm seeing conflicting reports: some logs show v1.2.0, but others are flagging v2.0.0-beta.",
            "[Session llm-13, Turn 3] assistant: It looks a bit ambiguous at the moment. I'm seeing signals for both v1.2.0 and v2.0.0-beta running on AnalyticsEngine1.",
            "[Session llm-10, Turn 0] user: Morning. I'm seeing some weird logs from the analytics engine. Which version are we actually serving right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.994434356689453,
          "generation_latency_ms": 3211.0748291015625,
          "total_latency_ms": 3222.069263458252
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the Backend Lead exhausted during the conversation by the coffee machine?",
          "answer_type": "short_answer",
          "gold_answer": "His cat kept him awake",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 12] user: Is the frontend lead blaming the load balancer again?",
            "[Session llm-7, Turn 11] assistant: Done. \"Buy coffee beans\" has been added to your evening tasks. Is there anything else?",
            "[Session llm-2, Turn 11] assistant: Don't drink too much caffeine. While you're grabbing that, Olivia sent a message about the dashboard load times."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999752044677734,
          "generation_latency_ms": 4246.9282150268555,
          "total_latency_ms": 4256.927967071533
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the same conversation where the user asked for a reminder to call their mom, what was the status of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Mixed signals",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-3, Turn 9] assistant: Reminder set for 5 PM. Back to the infrastructure—what's the status of the analytics engine?",
            "[Session llm-23, Turn 2] user: Alright, I'm back. Listen, my cofounder is breathing down my neck about the quarterly numbers. Can we check the status of the analytics engine?",
            "[Session llm-18, Turn 0] user: Hey, can you pull up the current status for the analytics engine? I'm seeing some weird logs coming through."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.952234268188477,
          "generation_latency_ms": 3320.4245567321777,
          "total_latency_ms": 3331.376791000366
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the logs, which version of the analytics engine do they prefer to be active in production?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "The users were debating whether to stick with v1.2.0.",
          "retrieved_context": [
            "[Session llm-2, Turn 0] user: Can you check which version of the analytics engine we're currently serving? My cofounder is asking about the stability of the new release.",
            "[Session llm-12, Turn 4] user: Yeah, he mentioned the analytics engine deployment is acting up. Which version is live right now?",
            "[Session llm-13, Turn 2] user: Start with the analytics engine. My cofounder was asking about the feature set, so I need to know exactly which version is live right now."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.775566101074219,
          "generation_latency_ms": 4276.391983032227,
          "total_latency_ms": 4287.167549133301
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Has the user explicitly requested to filter the Auth Service logs to a single cloud provider?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No. The assistant asked the user if they wanted to filter for one specifically, but the user",
          "retrieved_context": [
            "[Session llm-12, Turn 8] user: Perfect for a hike. Back to the logs—is the auth service impacted? I know we have that split across providers.",
            "[Session llm-25, Turn 5] assistant: AuthService2 is up, but the hosting environment is reporting as both Azure and GCP. Do you want me to filter for one specifically?",
            "[Session llm-13, Turn 8] user: Thanks. Now, regarding the auth service... are we still splitting that across clouds?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.73598861694336,
          "generation_latency_ms": 4484.529495239258,
          "total_latency_ms": 4495.265483856201
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is the Load Balancer currently running?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: Correct, LoadBalancer4 is hosted on GCP.",
            "[Session llm-12, Turn 14] user: No, I'll do it later. Just check where LoadBalancer4 is hosted again?",
            "[Session llm-26, Turn 11] assistant: Yes, LoadBalancer4 is running entirely on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.990453720092773,
          "generation_latency_ms": 3006.283760070801,
          "total_latency_ms": 3016.2742137908936
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the Auth Service currently hosted exclusively on Azure?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, not entirely. AuthService2 appears to be distributed across both Azure and GCP.",
          "retrieved_context": [
            "[Session llm-27, Turn 4] user: Thanks. I need to figure out where our money is going. Is the auth service strictly on Azure right now?",
            "[Session llm-10, Turn 3] assistant: Yes, that service is currently hosted exclusively on Azure.",
            "[Session llm-1, Turn 8] user: Hopefully not. We have enough issues with the auth service. Is that still fully on Azure?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.998632431030273,
          "generation_latency_ms": 4863.666772842407,
          "total_latency_ms": 4872.6654052734375
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Who should be contacted regarding discrepancies in the data science model deployments?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "Sophia",
          "retrieved_context": [
            "[Session llm-18, Turn 11] assistant: I haven't seen any recent commits from her regarding the data science models.",
            "[Session llm-11, Turn 4] user: I bet Sophia deployed that beta. She's always trying to get the latest models running for the data science team.",
            "[Session llm-27, Turn 3] assistant: Reminder set to hydrate. Yes, Sophia is the Data Scientist in charge of those deployments."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999824523925781,
          "generation_latency_ms": 4198.653221130371,
          "total_latency_ms": 4207.653045654297
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What routine action does the user typically take before starting the daily triage?",
          "answer_type": "short_answer",
          "gold_answer": "Grab coffee",
          "predicted_answer": "The user typically grabs a quick coffee.",
          "retrieved_context": [
            "[Session llm-9, Turn 7] assistant: Note taken. Dry cleaning reminder set for 5 PM.",
            "[Session llm-14, Turn 3] assistant: Reminder set for the dentist at 2 PM. What specific infrastructure components are you looking at?",
            "[Session llm-23, Turn 0] user: Hey, hope you're having a good morning. I need to grab a quick coffee before we start the daily triage."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.772611618041992,
          "generation_latency_ms": 2397.2713947296143,
          "total_latency_ms": 2406.0440063476562
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "boolean": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_022.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 30,
      "num_probes": 12,
      "ingestion_time_s": 1.784080982208252,
      "total_evaluation_time_s": 66.75527453422546,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-24, Turn 14] user: Alright, I'll ping my cofounder about the beta tag leak. Elijah needs to know before the board meeting.",
            "[Session llm-1, Turn 2] user: Just the meeting with my cofounder, Elijah. We need to go over the quarterly targets with the sales leads.",
            "[Session llm-5, Turn 4] user: That's messy. I bet my cofounder Elijah pushed that beta tag without telling me. He gets a bit hands-on for a CEO sometimes."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.766101837158203,
          "generation_latency_ms": 3691.3464069366455,
          "total_latency_ms": 3701.1125087738037
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the Sales Lead who is known for watching the dashboards?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 10] user: Probably a cat toy. Are the other Sales Leads, Ava and Sophia, in the office today?",
            "[Session llm-14, Turn 12] user: Yeah, he's always moving fast. Did the other sales leads, Ava or Sophia, mention any issues with the dashboard?",
            "[Session llm-7, Turn 3] assistant: Will do. Is this regarding his Sales Lead targets or are you just checking in on his cat, Luna?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.972738265991211,
          "generation_latency_ms": 4085.1171016693115,
          "total_latency_ms": 4096.089839935303
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service running conflicting versions v1.2.0 and v2.0.0-beta hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-21, Turn 3] assistant: Pulling up the metrics for LoadBalancer3. It's online, but I'm detecting a version mismatch. It reports v1.2.0 active, but the beta tag v2.0.0 is also present in the metadata.",
            "[Session llm-27, Turn 3] assistant: It is hosted on Azure, yes. Right now, it seems to be running a split configuration with both v1.2.0 and the v2.0.0-beta active.",
            "[Session llm-20, Turn 2] user: I'm looking at the dashboard for our Microsoft-hosted environment. It looks like LoadBalancer3 is reporting two different versions running at once, v1.2.0 and the v2.0.0-beta. That seems messy."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.020038604736328,
          "generation_latency_ms": 3252.4116039276123,
          "total_latency_ms": 3265.4316425323486
        },
        {
          "probe_id": "declarative_reasoning-composition-4",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which specific service is hosted on the same cloud platform as 'AnalyticsEngine9'?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer3",
          "predicted_answer": "The third load balancer.",
          "retrieved_context": [
            "[Session llm-0, Turn 7] assistant: Classic Oliver. To answer your question, AnalyticsEngine9 is currently hosted on Azure.",
            "[Session llm-28, Turn 11] assistant: AnalyticsEngine9 is hosted on Azure, same as the third load balancer.",
            "[Session llm-24, Turn 13] assistant: Correct, AnalyticsEngine9 is hosted on Azure, right alongside the troubled load balancer."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.836124420166016,
          "generation_latency_ms": 3125.919818878174,
          "total_latency_ms": 3136.75594329834
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What item did the user ask to be reminded to buy immediately after discussing Elijah's confusion about the load balancer's location?",
          "answer_type": "short_answer",
          "gold_answer": "Milk",
          "predicted_answer": "Milk",
          "retrieved_context": [
            "[Session llm-25, Turn 4] user: I knew it. Elijah was convinced it was on Google. Remind me to buy milk on the way home, by the way.",
            "[Session llm-25, Turn 2] user: That's the one. Okay, focus time. My cofounder is asking about our cloud spread. Where exactly is LoadBalancer3 sitting?",
            "[Session llm-11, Turn 5] assistant: Elijah hasn't shared the deck yet, but he did send a reminder about the strategy sync at noon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.994911193847656,
          "generation_latency_ms": 4079.197406768799,
          "total_latency_ms": 4090.1923179626465
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to be reminded about 'dry cleaning' before or after they asked to be reminded about 'milk'?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "The user asked to be reminded about 'dry cleaning' before they asked to be reminded about '",
          "retrieved_context": [
            "[Session llm-25, Turn 4] user: I knew it. Elijah was convinced it was on Google. Remind me to buy milk on the way home, by the way.",
            "[Session llm-21, Turn 0] user: Morning. Can you add a reminder to my list to pick up the dry cleaning at 5?",
            "[Session llm-1, Turn 0] user: Morning. Can you put a reminder for 2 PM to pick up the dry cleaning?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.16745376586914,
          "generation_latency_ms": 3658.6055755615234,
          "total_latency_ms": 3670.7730293273926
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What consumable item does the user explicitly state is their 'first priority' before they can function?",
          "answer_type": "short_answer",
          "gold_answer": "Caffeine",
          "predicted_answer": "Caffeine",
          "retrieved_context": [
            "[Session llm-27, Turn 1] assistant: Good morning, Alex. I’ll make a note that caffeine is the first priority.",
            "[Session llm-1, Turn 1] assistant: Done. Anything else on the agenda before your big sync with the executive team?",
            "[Session llm-16, Turn 10] user: Hah, true. Is he the only one leading the sales push on this new feature?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 3512.882947921753,
          "total_latency_ms": 3522.886276245117
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Regarding the conflicting versions on LoadBalancer3, does the user prefer to keep the beta percentage high or low?",
          "answer_type": "short_answer",
          "gold_answer": "Low",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 3] assistant: Pulling up the metrics for LoadBalancer3. It's online, but I'm detecting a version mismatch. It reports v1.2.0 active, but the beta tag v2.0.0 is also present in the metadata.",
            "[Session llm-20, Turn 2] user: I'm looking at the dashboard for our Microsoft-hosted environment. It looks like LoadBalancer3 is reporting two different versions running at once, v1.2.0 and the v2.0.0-beta. That seems messy.",
            "[Session llm-12, Turn 1] assistant: I'm looking at LoadBalancer3 now. It's running, but the telemetry is confusing—I'm seeing signals for both v1.2.0 and the v2.0.0-beta simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.342287063598633,
          "generation_latency_ms": 3515.5484676361084,
          "total_latency_ms": 3526.890754699707
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is 'CacheLayer9' hosted on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 11] assistant: No, both CacheLayer5 and CacheLayer7 are hosted on the Google Cloud Platform.",
            "[Session llm-3, Turn 11] assistant: Correct. CacheLayer5 and CacheLayer7 are both hosted on GCP, creating cross-cloud latency.",
            "[Session llm-9, Turn 11] assistant: No, CacheLayer5 is hosted on GCP, so it's separate from the analytics engine on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999917984008789,
          "generation_latency_ms": 2852.5664806365967,
          "total_latency_ms": 2863.5663986206055
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What specific version number is System:LoadBalancer_7 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-14, Turn 8] user: That's what I thought. I'm seeing some weird logs for LoadBalancer3. What version is it actually running?",
            "[Session llm-9, Turn 6] user: I guess so. Hey, looking at that third load balancer... what version is it actually running?",
            "[Session llm-16, Turn 4] user: Not yet. I'm actually more worried about a version mismatch I saw earlier. What is LoadBalancer3 actually running?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.359432220458984,
          "generation_latency_ms": 3437.757968902588,
          "total_latency_ms": 3446.117401123047
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "What specific action did the user decide to take to resolve the jitter caused by the split configuration on LoadBalancer3?",
          "answer_type": "short_answer",
          "gold_answer": "Manually kill the beta instance",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 2] user: That explains the jitter. What about the other load balancer, the one on Google's side?",
            "[Session llm-16, Turn 4] user: Not yet. I'm actually more worried about a version mismatch I saw earlier. What is LoadBalancer3 actually running?",
            "[Session llm-28, Turn 1] assistant: Looking at LoadBalancer3 now. It's behaving strangely; I'm seeing active flags for version 1.2.0, but also traces of v2.0.0-beta running simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.870933532714844,
          "generation_latency_ms": 3990.429162979126,
          "total_latency_ms": 4001.300096511841
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What is the current versioning state of the load balancer hosted on Azure?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting versions",
          "predicted_answer": "There is a version conflict showing for LoadBalancer3 on Azure; it is tagging as v1.2.0 but also reporting v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: Taking a look. That would be LoadBalancer3. It is indeed hosted on Azure. I am seeing some mixed signals on the versioning.",
            "[Session llm-5, Turn 3] assistant: Pulling up the details for LoadBalancer3 on Azure. I should note there's a bit of a version conflict showing; it's tagging as v1.2.0 but also reporting v2.0.0-beta.",
            "[Session llm-27, Turn 2] user: While I'm waiting for the machine, what's the status on LoadBalancer3? Is it fully migrated to Azure yet?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.093378067016602,
          "generation_latency_ms": 3725.0425815582275,
          "total_latency_ms": 3736.135959625244
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_023.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 8,
      "ingestion_time_s": 1.6812989711761475,
      "total_evaluation_time_s": 54.39633131027222,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' the user intends to sync with regarding the cloud footprint?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 2] user: Yeah, let's get into it. My cofounder wants a breakdown of our current cloud footprint before noon.",
            "[Session llm-1, Turn 0] user: Morning. I'm trying to map out our current cloud footprint for the board meeting with my cofounder later.",
            "[Session llm-16, Turn 1] assistant: I've added the dry cleaning reminder. What specifically is Mia concerned about regarding the cloud bill?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.605812072753906,
          "generation_latency_ms": 3979.3405532836914,
          "total_latency_ms": 3989.9463653564453
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member is associated with the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Charlotte",
          "retrieved_context": [
            "[Session llm-22, Turn 7] assistant: I've added that to your list. By the way, Charlotte shared a photo of her cat, Luna, in the random channel earlier; it seems the training is taking a backseat to pet appreciation this morning.",
            "[Session llm-18, Turn 5] assistant: Yes, she brought Luna in today. Having a cat in the data science corner definitely adds some personality to the room.",
            "[Session llm-6, Turn 7] assistant: I don't have a status update on Luna, but I can certainly note down that you want to discuss the migration dependencies with Charlotte."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.775161743164062,
          "generation_latency_ms": 3857.032060623169,
          "total_latency_ms": 3866.807222366333
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the database that the user described as an 'outlier'?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-2, Turn 9] assistant: PrimaryDB3 is the outlier; it's hosted on Azure.",
            "[Session llm-18, Turn 9] assistant: No, that is the outlier. PrimaryDB3 is on Azure, while PrimaryDB6 and the analytics services are all on GCP.",
            "[Session llm-6, Turn 3] assistant: We are. While the majority of the stack, including the second and third analytics engines, is on GCP, we still have that one outlier database, PrimaryDB3, sitting on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.814521789550781,
          "generation_latency_ms": 2882.6167583465576,
          "total_latency_ms": 2895.4312801361084
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What two conflicting versions is AnalyticsEngine3 currently reporting?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: AnalyticsEngine3 is on GCP as well. However, I'm noticing some conflicting signals regarding its version—it's reporting both v1.2.0 and v2.0.0-beta.",
            "[Session llm-24, Turn 13] assistant: AnalyticsEngine3 is simultaneously reporting v1.2.0 and v2.0.0-beta. It looks like the old version wasn't drained properly.",
            "[Session llm-17, Turn 8] user: Thanks. I'm also seeing some version conflicts on that third analytics engine. What's it reporting?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.681940078735352,
          "generation_latency_ms": 2972.499370574951,
          "total_latency_ms": 2982.1813106536865
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What external factor did the Data Scientist blame for the model latency issues?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat",
          "predicted_answer": "Her cat walking on the keyboard.",
          "retrieved_context": [
            "[Session llm-21, Turn 4] user: Right. It complicates things for Charlotte. She was complaining about the latency for her models yesterday, though she blamed it on her cat walking on the keyboard.",
            "[Session llm-24, Turn 6] user: Ugh, right. Where did we stick DB3? Elijah was complaining about latency on the sales dashboards yesterday, and that might be why.",
            "[Session llm-25, Turn 6] user: Ugh, I need to fix that before Elijah tries to demo it to a client. Has he complained about latency?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.045694351196289,
          "generation_latency_ms": 5704.675197601318,
          "total_latency_ms": 5715.720891952515
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific item did the user request a reminder to purchase on the way home?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 2] user: Seriously. She wants to know why we have a random bill from Azure when we're mostly a Google shop.",
            "[Session llm-4, Turn 7] assistant: I've added a reminder to contact Elijah about the sales targets.",
            "[Session llm-19, Turn 5] assistant: Consider it done. I've set a reminder for 12:30 PM. Do you want me to order flowers too?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.703802108764648,
          "generation_latency_ms": 3998.0106353759766,
          "total_latency_ms": 4008.714437484741
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the Sales Lead's infrastructure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-24, Turn 7] assistant: PrimaryDB3 is currently hosted on Azure. The cross-cloud connection could definitely explain what the Sales Lead is experiencing.",
            "[Session llm-5, Turn 3] assistant: It is a bit distributed. For the main storage, PrimaryDB3 is hosted on Azure, but PrimaryDB6 is over on GCP.",
            "[Session llm-6, Turn 2] user: Yeah, I'm looking at our cloud architecture diagrams. Mia is pushing for a cost audit. Are we still split across providers?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.316226959228516,
          "generation_latency_ms": 3758.6159706115723,
          "total_latency_ms": 3770.932197570801
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-1",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who did the assistant plan to contact regarding the UI glitches caused by the engine version conflict?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia and Ava",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 8] user: Thanks. I'm also seeing some version conflicts on that third analytics engine. What's it reporting?",
            "[Session llm-25, Turn 5] assistant: That is unusual; while v1.2.0 is the expected version, system diagnostics are reporting it as v2.0.0-beta.",
            "[Session llm-21, Turn 8] user: Yeah, actually. Did we upgrade the third engine yet? Elijah needs the new dashboard features for his sales demos."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.543418884277344,
          "generation_latency_ms": 10736.04941368103,
          "total_latency_ms": 10745.592832565308
        }
      ],
      "summary": {
        "overall_accuracy": 0.375,
        "total_score": 3,
        "count": 8,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 7,
            "accuracy": 0.42857142857142855
          },
          "abstain": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_024.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 29,
      "num_probes": 10,
      "ingestion_time_s": 1.852081537246704,
      "total_evaluation_time_s": 55.347286224365234,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is identified as the user's cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 14] user: One last thing, did my cofounder sign off on the new budget?",
            "[Session llm-28, Turn 4] user: Yeah, her cat is adorable. Anyway, back to business. Who is currently leading the frontend team? I need to assign a ticket.",
            "[Session llm-7, Turn 6] user: Got it. I need to sync with my cofounder later. Elijah wants a full breakdown of the tech stack."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.207746505737305,
          "generation_latency_ms": 3492.4874305725098,
          "total_latency_ms": 3504.695177078247
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who shares the Sales Lead role with Oliver?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-23, Turn 13] assistant: Yes, Oliver is also designated as a Sales Lead, so he might be handling some clients too.",
            "[Session llm-26, Turn 7] assistant: Correct, both Oliver and Mia are currently the Sales Leads.",
            "[Session llm-7, Turn 11] assistant: Yes, Mia is a Sales Lead, and Oliver holds that title as well."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.012626647949219,
          "generation_latency_ms": 3694.6356296539307,
          "total_latency_ms": 3704.64825630188
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that the user described as 'sitting all alone'?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 1] assistant: Managing costs is tough, Alex. Is Elijah concerned about a specific cloud provider, given you're using a few different ones?",
            "[Session llm-4, Turn 13] assistant: That service is currently hosted on the Google Cloud Platform.",
            "[Session llm-27, Turn 4] user: Haha, yeah, Luna. Anyway, my cofounder Elijah is worried about our cloud fragmentation. Can you summarize where our main data stores are sitting?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.566804885864258,
          "generation_latency_ms": 3095.583200454712,
          "total_latency_ms": 3108.150005340576
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which specific database resource is hosted on the platform provided by Microsoft?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "PrimaryDB7",
          "retrieved_context": [
            "[Session llm-28, Turn 1] assistant: Looking at the current configuration, PrimaryDB7 is hosted on the Microsoft cloud platform.",
            "[Session llm-13, Turn 3] assistant: That database is currently hosted on Azure.",
            "[Session llm-28, Turn 0] user: Hey, I need a quick sanity check on our infrastructure map. Where exactly is the seventh primary database hosted right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.522603988647461,
          "generation_latency_ms": 3852.452039718628,
          "total_latency_ms": 3862.9746437072754
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is currently designated as the Backend Lead?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-28, Turn 5] assistant: It looks like both Olivia and Noah are designated as Frontend Leads right now.",
            "[Session llm-16, Turn 5] assistant: You've got both Olivia and Noah listed as Frontend Leads. I can draft an email to them if you want.",
            "[Session llm-28, Turn 4] user: Yeah, her cat is adorable. Anyway, back to business. Who is currently leading the frontend team? I need to assign a ticket."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.150815963745117,
          "generation_latency_ms": 3186.354875564575,
          "total_latency_ms": 3194.5056915283203
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is currently running on AuthService1?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 9] assistant: Not yet, AuthService1 is still hosted on the Google Cloud Platform.",
            "[Session llm-18, Turn 7] assistant: Yes, AuthService1 is currently hosted on GCP.",
            "[Session llm-22, Turn 9] assistant: AuthService1 is currently deployed on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.631084442138672,
          "generation_latency_ms": 3991.4913177490234,
          "total_latency_ms": 4002.122402191162
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation about the 'multi-cloud mess', what item did the user ask to be reminded to buy?",
          "answer_type": "short_answer",
          "gold_answer": "Milk",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 6] user: Yeah, the multi-cloud bill is giving me a headache. Remind me what's racking up costs on Amazon.",
            "[Session llm-9, Turn 11] assistant: Tell your cofounder it's the price of multi-cloud resilience. Or just blame the network.",
            "[Session llm-22, Turn 0] user: Morning. I need a quick summary of our multi-cloud mess before my coffee kicks in."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.460304260253906,
          "generation_latency_ms": 3554.4369220733643,
          "total_latency_ms": 3565.897226333618
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request a reminder to call their mom before or after requesting a reminder for espresso pods?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-28, Turn 3] assistant: I've added a reminder for the espresso pods. Speaking of office chat, did you see the photos of Luna that Mia shared earlier?",
            "[Session llm-28, Turn 2] user: Okay, thanks. Remind me to order more espresso pods for the office kitchen later; we're running low.",
            "[Session llm-6, Turn 7] assistant: Consider it done. I've set a reminder for 11:30 to order lunch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.790275573730469,
          "generation_latency_ms": 3394.9134349823,
          "total_latency_ms": 3406.7037105560303
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's greetings in the session logs, at what time of day do they typically discuss infrastructure status?",
          "answer_type": "short_answer",
          "gold_answer": "Morning",
          "predicted_answer": "Based on the user's greetings, they typically discuss infrastructure status in the morning.",
          "retrieved_context": [
            "[Session llm-11, Turn 3] assistant: Reminder set for 5 PM. Regarding the infrastructure, where should we start?",
            "[Session llm-6, Turn 0] user: Good morning. I need to get a handle on our infrastructure spread before the weekly sync.",
            "[Session llm-4, Turn 0] user: Hey, hope you're having a good morning. I need a quick sanity check on our infrastructure map before my coffee kicks in."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.999368667602539,
          "generation_latency_ms": 3168.958902359009,
          "total_latency_ms": 3180.9582710266113
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Which two cloud providers does the user repeatedly confuse regarding service locations?",
          "answer_type": "short_answer",
          "gold_answer": "Azure and GCP",
          "predicted_answer": "Google Cloud and Azure.",
          "retrieved_context": [
            "[Session llm-25, Turn 1] assistant: Managing costs is tough, Alex. Is Elijah concerned about a specific cloud provider, given you're using a few different ones?",
            "[Session llm-19, Turn 8] user: Right, Google Cloud. That explains the latency Olivia was complaining about.",
            "[Session llm-26, Turn 3] assistant: PrimaryDB7 is hosted on Azure. The service running on GCP is actually AuthService1."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.03353500366211,
          "generation_latency_ms": 4261.96813583374,
          "total_latency_ms": 4273.001670837402
        }
      ],
      "summary": {
        "overall_accuracy": 0.6,
        "total_score": 6,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_025.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 10,
      "ingestion_time_s": 1.6308026313781738,
      "total_evaluation_time_s": 57.93008637428284,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is identified as the user's cofounder who worries about stability?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-21, Turn 4] user: That might be why my cofounder was asking about stability earlier.",
            "[Session llm-21, Turn 5] assistant: Liam tends to worry about stability when betas hit production, especially in his role as CEO.",
            "[Session llm-19, Turn 2] user: Yeah, my cofounder was pinging me earlier about the current state of our authentication service. He's worried about the stability."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.77325439453125,
          "generation_latency_ms": 3752.720355987549,
          "total_latency_ms": 3762.49361038208
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the DevOps engineer who works alongside Lucas?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 9] assistant: Yes, Lucas is on the DevOps rotation. I'll let him know about the version conflict.",
            "[Session llm-9, Turn 3] assistant: You'll want to talk to Oliver or Lucas then; they are your primary DevOps engineers.",
            "[Session llm-4, Turn 13] assistant: I hope Luna is back home safe. Since Lucas is on DevOps too, hopefully he can cover the deployment tasks in the meantime."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.070396423339844,
          "generation_latency_ms": 4595.028400421143,
          "total_latency_ms": 4604.098796844482
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service hosted that is currently experiencing a version mismatch with a beta release?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud Platform (GCP)",
          "retrieved_context": [
            "[Session llm-4, Turn 2] user: Right, the beta. Is that the one on Google's cloud or are we still testing that locally?",
            "[Session llm-22, Turn 7] assistant: It is hosted on GCP, but I'm detecting a version conflict. You have v1.2.0 running, but v2.0.0-beta is also active in the same environment.",
            "[Session llm-18, Turn 2] user: Just the incident report. I noticed AuthService7 is reporting two different versions in the logs, v1.2.0 and the v2.0.0-beta. It's hosted on GCP, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.693166732788086,
          "generation_latency_ms": 3736.335277557373,
          "total_latency_ms": 3749.028444290161
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which analytics engine is deployed on the same cloud platform as the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine3",
          "predicted_answer": "AnalyticsEngine3",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: AnalyticsEngine3 is hosted on GCP, similar to the auth service, while AnalyticsEngine5 is running on Azure.",
            "[Session llm-17, Turn 3] assistant: They are indeed distributed across providers. AnalyticsEngine6 is currently on AWS, while Engine 3 is on GCP and Engine 5 is running on Azure.",
            "[Session llm-3, Turn 5] assistant: It is quite the spread. AnalyticsEngine6 is over on AWS, but AnalyticsEngine5 is hosted on Azure. Then you have AnalyticsEngine3 sitting on GCP alongside the auth service."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.150432586669922,
          "generation_latency_ms": 3576.3487815856934,
          "total_latency_ms": 3586.4992141723633
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was Oliver distracted during the standup meeting mentioned in the session where the user discussed quarterly goals?",
          "answer_type": "short_answer",
          "gold_answer": "Luna knocked over water",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 4] user: What a mess. Oh, did Oliver mention if he's bringing Luna to the office today?",
            "[Session llm-16, Turn 4] user: I bet Oliver messed that up last night. He mentioned his cat, Luna, was keeping him awake while he was on call.",
            "[Session llm-6, Turn 4] user: What a mess. Did Oliver manage to consolidate anything yet? I heard his cat walked across his keyboard during the last migration attempt."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.123252868652344,
          "generation_latency_ms": 3738.1043434143066,
          "total_latency_ms": 3748.227596282959
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-1",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "In the conversation where the user complained about paying invoices to every cloud provider, where was AnalyticsEngine5 stated to be hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "AnalyticsEngine5 is stated to be hosted on Azure.",
          "retrieved_context": [
            "[Session llm-22, Turn 0] user: Can you pull up the infrastructure map for our analytics engines? I feel like we're paying invoices to every cloud provider in existence.",
            "[Session llm-17, Turn 3] assistant: They are indeed distributed across providers. AnalyticsEngine6 is currently on AWS, while Engine 3 is on GCP and Engine 5 is running on Azure.",
            "[Session llm-9, Turn 9] assistant: Yes, it appears so. AnalyticsEngine6 is hosted on AWS, while number 5 is over on Azure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999679565429688,
          "generation_latency_ms": 4866.685628890991,
          "total_latency_ms": 4877.685308456421
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's comments about the infrastructure map, how do they feel about the current multi-cloud setup?",
          "answer_type": "short_answer",
          "gold_answer": "dislikes it",
          "predicted_answer": "The user feels the current multi-cloud setup is a \"nightmare\" and a",
          "retrieved_context": [
            "[Session llm-6, Turn 0] user: Hey, can you pull up the current infrastructure map? I feel like we're paying for every cloud provider in existence at this point. Also, remind me to call my mom at 5 PM.",
            "[Session llm-18, Turn 10] user: Right, the multi-cloud nightmare. Noah is going to have a fit trying to design a cohesive UI for the admin panel with that kind of latency variance.",
            "[Session llm-22, Turn 0] user: Can you pull up the infrastructure map for our analytics engines? I feel like we're paying invoices to every cloud provider in existence."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.788917541503906,
          "generation_latency_ms": 4422.940492630005,
          "total_latency_ms": 4433.729410171509
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is AnalyticsEngine6 currently running?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 1] assistant: It is a bit of a multi-cloud situation. AnalyticsEngine6 is running on AWS, while AnalyticsEngine5 is deployed over on Azure.",
            "[Session llm-21, Turn 9] assistant: For the data science team's reference: AnalyticsEngine6 is on AWS, number 5 is on Azure, and AnalyticsEngine3 is on GCP.",
            "[Session llm-9, Turn 9] assistant: Yes, it appears so. AnalyticsEngine6 is hosted on AWS, while number 5 is over on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.524511337280273,
          "generation_latency_ms": 2915.4553413391113,
          "total_latency_ms": 2925.9798526763916
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of CFO at Nebula Corp?",
          "answer_type": "abstain",
          "gold_answer": "unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: That one is sitting over on Azure. Managing the credentials across all three for Nebula Corp must be fun.",
            "[Session llm-17, Turn 9] assistant: I am not sure if the CEO has been briefed on that specific version split yet.",
            "[Session llm-2, Turn 5] assistant: Reminder set for lunch with Noah. Is he still leading the product design team?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.290695190429688,
          "generation_latency_ms": 3068.371295928955,
          "total_latency_ms": 3077.6619911193848
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific action did the assistant perform when the user requested to be prompted about calling the dentist?",
          "answer_type": "short_answer",
          "gold_answer": "set a reminder",
          "predicted_answer": "The assistant set a reminder to call the dentist.",
          "retrieved_context": [
            "[Session llm-0, Turn 5] assistant: Reminder set for 2 PM to call the dentist. Regarding the auth service, having both the stable and beta versions running simultaneously could definitely cause issues.",
            "[Session llm-13, Turn 3] assistant: Liam tends to worry, but the CEO has to keep eyes on the big picture. By the way, don't forget you have a dentist appointment at 3 PM.",
            "[Session llm-21, Turn 0] user: Hey, can you set a reminder for me to call the dentist at 2 PM? Also, I'm looking at the deployment logs for AuthService7."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.764360427856445,
          "generation_latency_ms": 3762.8324031829834,
          "total_latency_ms": 3773.59676361084
        }
      ],
      "summary": {
        "overall_accuracy": 0.6,
        "total_score": 6,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 8,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_026.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 1.7035367488861084,
      "total_evaluation_time_s": 66.45756316184998,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Our DevOps lead is the owner of the cat named Luna.",
          "retrieved_context": [
            "[Session llm-30, Turn 1] assistant: I haven't seen him log on yet, but he mentioned yesterday he was taking Luna to the vet. Hope the cat is okay.",
            "[Session llm-7, Turn 7] assistant: He might be, though our DevOps lead mentioned he needed to leave early to take his cat, Luna, to the vet.",
            "[Session llm-24, Turn 7] assistant: He's online, but he just posted a picture of Luna sleeping on his keyboard, so he might be a bit occupied with his cat at the moment."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.510517120361328,
          "generation_latency_ms": 3574.5434761047363,
          "total_latency_ms": 3584.0539932250977
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which cloud provider hosts the load balancer that is currently running a beta version?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 2] user: Definitely. Speaking of deployments, I need to check our Google Cloud footprint. Which load balancers are we running over there again?",
            "[Session llm-7, Turn 2] user: Yeah, I'm reviewing the infrastructure. Which load balancers are we keeping on Google Cloud?",
            "[Session llm-6, Turn 2] user: Great. We need to double-check the infrastructure hosted on Google's cloud. Which load balancers are sitting there again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.622594833374023,
          "generation_latency_ms": 3797.475576400757,
          "total_latency_ms": 3810.098171234131
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who had to take their pet to the vet?",
          "answer_type": "short_answer",
          "gold_answer": "DevOps",
          "predicted_answer": "DevOps lead",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: He might be, though our DevOps lead mentioned he needed to leave early to take his cat, Luna, to the vet.",
            "[Session llm-30, Turn 1] assistant: I haven't seen him log on yet, but he mentioned yesterday he was taking Luna to the vet. Hope the cat is okay.",
            "[Session llm-16, Turn 1] assistant: Sounds like a classic distraction. I hope the cat is doing well, at least. Did Will eventually get back to his DevOps duties?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.003255844116211,
          "generation_latency_ms": 3235.886335372925,
          "total_latency_ms": 3246.889591217041
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which specific service is hosted on the Azure platform?",
          "answer_type": "short_answer",
          "gold_answer": "AuthService9",
          "predicted_answer": "AuthService9",
          "retrieved_context": [
            "[Session llm-14, Turn 9] assistant: Yes, AuthService9 is currently hosted on Azure.",
            "[Session llm-4, Turn 11] assistant: Yes, AuthService9 is currently hosted on Azure.",
            "[Session llm-8, Turn 9] assistant: Yes, AuthService9 is currently the only major component hosted over on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.054422378540039,
          "generation_latency_ms": 2842.817783355713,
          "total_latency_ms": 2851.872205734253
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What event caused the DevOps lead to be delayed in starting his work day?",
          "answer_type": "short_answer",
          "gold_answer": "Vet visit",
          "predicted_answer": "His cat, Luna, knocked over a plant, and he was dealing with the cleanup.",
          "retrieved_context": [
            "[Session llm-27, Turn 3] assistant: The DevOps queue shows progress, though he mentioned a delay.",
            "[Session llm-23, Turn 1] assistant: I haven't seen a commit from our DevOps lead this morning, he might be running late.",
            "[Session llm-13, Turn 3] assistant: I don't see the logs from our DevOps lead yet, but yes, his cat seems to be the priority on the general channel this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.121583938598633,
          "generation_latency_ms": 3571.406841278076,
          "total_latency_ms": 3581.528425216675
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the most recent infrastructure review, which load balancer was confirmed to still be running on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer8",
          "predicted_answer": "LoadBalancer8",
          "retrieved_context": [
            "[Session llm-30, Turn 3] assistant: Yes. It looks like LoadBalancer8 is still running on AWS, while number 4 and number 6 are both over on GCP.",
            "[Session llm-0, Turn 9] assistant: Yes, LoadBalancer8 is the one currently hosted on AWS.",
            "[Session llm-6, Turn 13] assistant: Correct, LoadBalancer8 is on AWS. It is currently reporting healthy status with no version conflicts."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.978460311889648,
          "generation_latency_ms": 4443.301200866699,
          "total_latency_ms": 4454.279661178589
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific beverage does the user require before they can focus on infrastructure or meetings?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "A double espresso.",
          "retrieved_context": [
            "[Session llm-12, Turn 13] assistant: Added milk to your list. Anything else regarding the infrastructure?",
            "[Session llm-3, Turn 1] assistant: I can help you document the system layout. What are you planning to grab for lunch?",
            "[Session llm-19, Turn 7] assistant: Consider it done. Reminder set for buying milk."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.182453155517578,
          "generation_latency_ms": 3725.6224155426025,
          "total_latency_ms": 3734.80486869812
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's initial request in Session 25, which cloud provider's resources do they prioritize reviewing first?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "[Session llm-7, Turn 2] user: Yeah, I'm reviewing the infrastructure. Which load balancers are we keeping on Google Cloud?",
            "[Session llm-13, Turn 4] user: Classic Will. Anyway, let's look at the Google Cloud setup. Which load balancers are sitting there right now?",
            "[Session llm-0, Turn 10] user: Okay. My cofounder is asking about the authentication service latency. Which cloud is that one on again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.406898498535156,
          "generation_latency_ms": 2856.6811084747314,
          "total_latency_ms": 2868.0880069732666
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is currently running on LoadBalancer8?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 14] user: Sounds like her. Hey, I'm seeing some conflicting reports on LoadBalancer4 in production. What version is it actually running?",
            "[Session llm-16, Turn 7] assistant: Number 6 is stable, but LoadBalancer4 is reporting conflicting version information. It says it's running v1.2.0, but also signals v2.0.0-beta.",
            "[Session llm-11, Turn 6] user: Yeah, that's the one. While he's out, can you check the version tag on LoadBalancer4?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001182556152344,
          "generation_latency_ms": 3521.501064300537,
          "total_latency_ms": 3531.5022468566895
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Lucas's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 11] assistant: You mean Lucas and Noah?",
            "[Session llm-8, Turn 14] user: Yeah, actually. What have Lucas and Noah been working on?",
            "[Session llm-22, Turn 6] user: Haha, at least she's cute. I need to set up a meeting with the data science guys later. Are Lucas and Noah free around 3 PM?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.999824523925781,
          "generation_latency_ms": 3531.599998474121,
          "total_latency_ms": 3540.599822998047
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "If a load balancer flags two different versions simultaneously, what type of deployment does the user identify this as?",
          "answer_type": "short_answer",
          "gold_answer": "Canary deployment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 6] user: Yeah, that's the one. While he's out, can you check the version tag on LoadBalancer4?",
            "[Session llm-2, Turn 3] assistant: Taking a look at LoadBalancer4. I see it's flagging v2.0.0-beta, but there are still traces of v1.2.0 active.",
            "[Session llm-16, Turn 9] assistant: Reminder set for 12:00 PM. Do you want me to flag that version conflict on LoadBalancer4 for the engineering team?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.915517807006836,
          "generation_latency_ms": 4121.987819671631,
          "total_latency_ms": 4132.903337478638
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What specific state configuration indicated to the assistant that a canary deployment was active on LoadBalancer4?",
          "answer_type": "short_answer",
          "gold_answer": "Two different versions",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 5] assistant: It seems LoadBalancer4 is reporting both v1.2.0 and v2.0.0-beta simultaneously. It looks like a blue-green deployment might have stalled.",
            "[Session llm-8, Turn 5] assistant: I've set a timer for lunch. As for LoadBalancer4, one check reports it's on v1.2.0, but another signal suggests it's running the v2.0.0-beta.",
            "[Session llm-19, Turn 14] user: Sounds like her. Hey, I'm seeing some conflicting reports on LoadBalancer4 in production. What version is it actually running?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.996746063232422,
          "generation_latency_ms": 3789.1273498535156,
          "total_latency_ms": 3801.124095916748
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_027.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 2,
      "ingestion_time_s": 1.7311162948608398,
      "total_evaluation_time_s": 11.539492130279541,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:loadbalancer_2 running version?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "It is reporting both v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: That would be LoadBalancer2. It's currently reporting a version conflict, showing both v1.2.0 and v2.0.0-beta active.",
            "[Session llm-17, Turn 5] assistant: It looks a bit ambiguous. I'm seeing the standard v1.2.0 signature for LoadBalancer2, but there are also heartbeats reporting as the v2.0.0-beta.",
            "[Session llm-1, Turn 13] assistant: For LoadBalancer2, the data is a bit conflicting. I see reports of it running v1.2.0, but also traces of v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.177207946777344,
          "generation_latency_ms": 4069.2553520202637,
          "total_latency_ms": 4078.432559967041
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-20, Turn 2] user: Yeah, Oliver is going to grill me on the infrastructure costs. He's stressing about the bills for our Azure services.",
            "[Session llm-8, Turn 8] user: Dark roast, definitely. Speaking of the team, is Liam running those heavy queries on the Azure box today?",
            "[Session llm-24, Turn 11] assistant: Yes, I see activity from the Product Designers. Ava and Olivia are currently collaborating on the new mockups."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.06207275390625,
          "generation_latency_ms": 3470.6759452819824,
          "total_latency_ms": 3482.7380180358887
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_028.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.6589760780334473,
      "total_evaluation_time_s": 66.35480618476868,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service where the Product Designer requires write access?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: I'll flag that for review. Since that auth service is hosted on AWS, I'll check the IAM roles there. Do they need the same access level as the Product Designer?",
            "[Session llm-22, Turn 11] assistant: I see. The cross-cloud permissions between Google and Amazon might need a review.",
            "[Session llm-16, Turn 2] user: Yeah, I'm reviewing our infrastructure costs. Remind me again which cloud provider is hosting the load balancer and the analytics engine?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.530471801757812,
          "generation_latency_ms": 4108.2587242126465,
          "total_latency_ms": 4118.789196014404
        },
        {
          "probe_id": "world_modeling-category_membership-1",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Which specific infrastructure services are currently confirmed to be running on the Google Cloud Platform?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer4 and AnalyticsEngine1",
          "predicted_answer": "The load balancer and the analytics engine are currently confirmed to be running on the Google Cloud",
          "retrieved_context": [
            "[Session llm-7, Turn 6] user: No rush. I'm going to grab a sandwich before diving into these logs. Remind me, which services are we keeping on Google's cloud right now?",
            "[Session llm-16, Turn 2] user: Yeah, I'm reviewing our infrastructure costs. Remind me again which cloud provider is hosting the load balancer and the analytics engine?",
            "[Session llm-19, Turn 3] assistant: Currently, the primary databases and the authentication service are hosted on AWS, while the load balancer and the analytics engine are running on GCP."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.415243148803711,
          "generation_latency_ms": 3315.509796142578,
          "total_latency_ms": 3326.925039291382
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the professional role of the individual who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "The individual who owns a cat named Luna is involved in sales.",
          "retrieved_context": [
            "[Session llm-24, Turn 7] assistant: Noted. I'll remind you to message Mia. By the way, Charlotte sent a picture of Luna earlier; apparently, the cat is helping with \"sales strategy.\"",
            "[Session llm-20, Turn 7] assistant: Is that for Charlotte's cat, Luna? or did you get a pet recently?",
            "[Session llm-19, Turn 11] assistant: Her cat seems to be the unofficial mascot of the sales team."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.349750518798828,
          "generation_latency_ms": 3644.5937156677246,
          "total_latency_ms": 3654.9434661865234
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the latest system logs discussed, what is the version status of the second primary database?",
          "answer_type": "short_answer",
          "gold_answer": "Split between v1.2.0 and v2.0.0-beta",
          "predicted_answer": "According to the latest system logs discussed, PrimaryDB2 (the second primary database) is flagging",
          "retrieved_context": [
            "[Session llm-21, Turn 10] user: Definitely. Back to the tech stack—what version is PrimaryDB2 running? I heard we might have auto-updated.",
            "[Session llm-16, Turn 10] user: Ugh, I almost forgot. Thanks. Hey, speaking of the databases, what version is PrimaryDB2 running right now?",
            "[Session llm-19, Turn 6] user: Speaking of PrimaryDB2, are we still on the old version?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.804819107055664,
          "generation_latency_ms": 3435.5814456939697,
          "total_latency_ms": 3447.3862648010254
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific technical issue is causing the user's cofounder to worry about the platform's stability?",
          "answer_type": "short_answer",
          "gold_answer": "Version mismatch on PrimaryDB2",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 4] user: Okay, that split sounds right. My cofounder was asking about reliability earlier; she's worried about the upcoming launch.",
            "[Session llm-10, Turn 11] assistant: Hah, Luna might be the real culprit. How is Oliver handling the product design changes amidst these infrastructure talks?",
            "[Session llm-22, Turn 1] assistant: Sounds like a busy morning. Did she have any specific concerns, or just the general stability of the platform?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.829521179199219,
          "generation_latency_ms": 3853.6479473114014,
          "total_latency_ms": 3863.4774684906006
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the conversation regarding the stuck migration, did the user complain about the coffee before or after requesting permission updates for Liam and Charlotte?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The user complained about the coffee before requesting permission updates for Liam and Charlotte.",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Coffee was weak, unfortunately. Need to fix the machine. Okay, so the migration is stuck. Can you make sure Liam and Charlotte have their permissions updated on AuthService9?",
            "[Session llm-12, Turn 1] assistant: Good morning, Alex. I hope the coffee is still drinkable despite the mechanical protest. Are you ready to dive into the system logs, or do you need a moment?",
            "[Session llm-4, Turn 3] assistant: Will do. I'll set a reminder for your coffee break. Do you want me to flag the database inconsistency for the engineering team?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.9405517578125,
          "generation_latency_ms": 2975.54874420166,
          "total_latency_ms": 2986.4892959594727
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's reaction to the office machine, what is their implied preference regarding coffee strength?",
          "answer_type": "short_answer",
          "gold_answer": "Strong coffee",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: Good morning, Alex. I can imagine. Is the caffeine for general survival or a specific crisis?",
            "[Session llm-9, Turn 1] assistant: Good morning, Alex. I can imagine. Shall I hold off on the morning briefing until the caffeine kicks in?",
            "[Session llm-12, Turn 1] assistant: Good morning, Alex. I hope the coffee is still drinkable despite the mechanical protest. Are you ready to dive into the system logs, or do you need a moment?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.41412353515625,
          "generation_latency_ms": 3759.9637508392334,
          "total_latency_ms": 3770.3778743743896
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-7",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "What specific access level constraint did the user define for the Sales Leads regarding the authentication service?",
          "answer_type": "short_answer",
          "gold_answer": "Read access only",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: I'll flag that for review. Since that auth service is hosted on AWS, I'll check the IAM roles there. Do they need the same access level as the Product Designer?",
            "[Session llm-24, Turn 8] user: Hah, she needs all the help she can get closing those deals. What about the auth service? Is that stable?",
            "[Session llm-21, Turn 14] user: True. Oh, and is AuthService9 on Google or Amazon?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998964309692383,
          "generation_latency_ms": 4047.872304916382,
          "total_latency_ms": 4058.871269226074
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is the company's email server hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 2] user: Yeah, I'm reviewing our infrastructure costs. Remind me again which cloud provider is hosting the load balancer and the analytics engine?",
            "[Session llm-19, Turn 3] assistant: Currently, the primary databases and the authentication service are hosted on AWS, while the load balancer and the analytics engine are running on GCP.",
            "[Session llm-0, Turn 3] assistant: Not exclusively. While the authentication service and both primary databases—numbers 2 and 7—are on AWS, the fourth load balancer and the first analytics engine are actually hosted on Google Cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.955406188964844,
          "generation_latency_ms": 3234.3220710754395,
          "total_latency_ms": 3244.2774772644043
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the Sales Lead named Liam?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 11] assistant: Her cat seems to be the unofficial mascot of the sales team.",
            "[Session llm-17, Turn 8] user: Okay. I have a meeting with the sales leads later. Liam and... who's the other one?",
            "[Session llm-4, Turn 10] user: Hope her cat is okay. What about Liam? Is he waiting on those numbers?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.369850158691406,
          "generation_latency_ms": 3691.434144973755,
          "total_latency_ms": 3700.8039951324463
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-10",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What follow-up action did the assistant commit to regarding the Product Designer's mockups?",
          "answer_type": "short_answer",
          "gold_answer": "Ping him",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 13] assistant: The product designer hasn't updated the repository since yesterday. I'll keep an eye out for his commit.",
            "[Session llm-3, Turn 5] assistant: I've added a reminder for your lunch with the product designer. Hopefully, the mockups are looking good.",
            "[Session llm-10, Turn 11] assistant: Hah, Luna might be the real culprit. How is Oliver handling the product design changes amidst these infrastructure talks?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.574174880981445,
          "generation_latency_ms": 3895.491123199463,
          "total_latency_ms": 3905.0652980804443
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-11",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What specific system mechanism does the assistant use to verify permissions for services hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "IAM roles",
          "predicted_answer": "The assistant checks IAM roles.",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: I'll flag that for review. Since that auth service is hosted on AWS, I'll check the IAM roles there. Do they need the same access level as the Product Designer?",
            "[Session llm-7, Turn 9] assistant: Correct. PrimaryDB2, PrimaryDB7, and the ninth AuthService are all hosted on AWS.",
            "[Session llm-16, Turn 5] assistant: Yes, AuthService9, PrimaryDB2, and PrimaryDB7 are all running on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.678363800048828,
          "generation_latency_ms": 3072.563886642456,
          "total_latency_ms": 3082.242250442505
        }
      ],
      "summary": {
        "overall_accuracy": 0.3333333333333333,
        "total_score": 4,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 12,
            "accuracy": 0.3333333333333333
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_029.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 1.6958434581756592,
      "total_evaluation_time_s": 65.84325766563416,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the 'cofounder' that the user mentions is paranoid about vendor lock-in?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: No, I'll power through. My cofounder was asking about our cloud redundancy earlier. She's paranoid about vendor lock-in.",
            "[Session llm-2, Turn 2] user: Yeah, my cofounder is pushing for that audit. She's worried about the multi-cloud costs.",
            "[Session llm-4, Turn 14] user: Thanks. Is my cofounder free right now? I need to get a budget approval."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.673139572143555,
          "generation_latency_ms": 2953.8116455078125,
          "total_latency_ms": 2966.484785079956
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider's infrastructure are the databases PrimaryDB6 and PrimaryDB7 hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-19, Turn 5] assistant: Not entirely. While PrimaryDB7 and PrimaryDB6 are running on Azure, PrimaryDB2 is still hosted on GCP.",
            "[Session llm-16, Turn 5] assistant: Correct, both PrimaryDB7 and PrimaryDB6 are hosted on Azure.",
            "[Session llm-23, Turn 3] assistant: Mia has a point. Currently, PrimaryDB6 and PrimaryDB7 are both on the Microsoft cloud, but PrimaryDB2 is hosted on Google's infrastructure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.467456817626953,
          "generation_latency_ms": 3230.0069332122803,
          "total_latency_ms": 3241.474390029907
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which specific service hosted on AWS is currently reporting conflicting version headers?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "The AWS cache layer/service.",
          "retrieved_context": [
            "[Session llm-0, Turn 10] user: No lunch yet. I'm seeing a weird version conflict on that AWS cache layer. What does the system report?",
            "[Session llm-16, Turn 9] assistant: Yes, that service is on AWS. The issue might be versioning; I'm detecting both v1.2.0 and v2.0.0-beta active simultaneously.",
            "[Session llm-20, Turn 3] assistant: I don't think it's the scaling. The instance hosted on AWS is reporting a version mismatch in the headers."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.640144348144531,
          "generation_latency_ms": 3982.170343399048,
          "total_latency_ms": 3992.8104877471924
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "The user implies a hard constraint against running which software version in the production environment?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "The user implies a hard constraint against running the beta version in the production environment.",
          "retrieved_context": [
            "[Session llm-23, Turn 9] assistant: I recall the DevOps lead prefers stability. Should I alert her about the version conflict?",
            "[Session llm-20, Turn 4] user: Seriously? We should be locked on v1.2.0. Please tell me we aren't running the beta in prod.",
            "[Session llm-2, Turn 12] user: She thinks the versioning is messed up. She was trying to explain it while feeding Luna."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.999202728271484,
          "generation_latency_ms": 2969.744920730591,
          "total_latency_ms": 2980.7441234588623
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the session immediately following the discussion about the failed blue-green deployment, what personal task did the user ask to be reminded of?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 4] user: Ugh, that sounds like a failed blue-green deployment. I bet our DevOps lead is already looking at it, or she's distracted by her cat.",
            "[Session llm-24, Turn 8] user: That sounds like a split-brain issue. Who is currently managing that deployment?",
            "[Session llm-17, Turn 4] user: That's messy. The beta wasn't supposed to hit the production environment yet. Who was the last to touch the deployment pipeline?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.647369384765625,
          "generation_latency_ms": 3374.6347427368164,
          "total_latency_ms": 3384.282112121582
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What deployment strategy did the user suspect caused the split-brain situation on the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Blue-green deployment",
          "predicted_answer": "The user suspected a partial rollout caused the split-brain situation on CacheLayer4.",
          "retrieved_context": [
            "[Session llm-19, Turn 3] assistant: I haven't seen a check-in from her yet. However, the system is reporting two different versions for CacheLayer4 right now—v1.2.0 and v2.0.0-beta. It looks like a mid-deployment state.",
            "[Session llm-9, Turn 3] assistant: It looks like the cat might have caused a partial rollout. I'm seeing instances running v1.2.0, but there are also traces of v2.0.0-beta active on CacheLayer4.",
            "[Session llm-3, Turn 13] assistant: High stakes then. I'll start analyzing the CacheLayer4 logs to see if the beta version is the culprit."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.422727584838867,
          "generation_latency_ms": 3474.0519523620605,
          "total_latency_ms": 3487.4746799468994
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's repeated requests across sessions, what specific beverage is essential to their morning routine?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "[Session llm-8, Turn 1] assistant: Good morning, Alex. Is the caffeine for general survival or is there a specific crisis?",
            "[Session llm-8, Turn 14] user: constantly. Anyway, remind me to grab a sandwich at 12:30, or I'll forget to eat.",
            "[Session llm-8, Turn 0] user: Morning. I definitely need another coffee before tackling the board slides."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.772300720214844,
          "generation_latency_ms": 3911.1084938049316,
          "total_latency_ms": 3920.8807945251465
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user approve of running the beta version of the cache layer in the production environment?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 7] assistant: I'm looking at CacheLayer4. It's a bit confusing—I see metrics for both v1.2.0 and v2.0.0-beta active right now.",
            "[Session llm-25, Turn 3] assistant: Looking at CacheLayer4, it reports version v1.2.0, though I'm also seeing signals for v2.0.0-beta on the same endpoint.",
            "[Session llm-5, Turn 1] assistant: Good morning. I see CacheLayer4 running on the Amazon setup. It's currently reporting v1.2.0 in the logs, but the dashboard is claiming v2.0.0-beta. Do you want me to flag that discrepancy?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.847640991210938,
          "generation_latency_ms": 4035.9911918640137,
          "total_latency_ms": 4045.8388328552246
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is PrimaryDB2 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 7] assistant: Correct. PrimaryDB2 is currently hosted on GCP, while the other two instances are elsewhere.",
            "[Session llm-14, Turn 13] assistant: Yes, PrimaryDB2 is hosted on GCP.",
            "[Session llm-4, Turn 5] assistant: Correct, PrimaryDB2 is still running on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.512186050415039,
          "generation_latency_ms": 3104.447841644287,
          "total_latency_ms": 3113.960027694702
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Sales Lead's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 11] assistant: I can notify the frontend lead if you want. Also, Liam stopped by earlier asking for the updated pricing deck for his sales calls.",
            "[Session llm-19, Turn 11] assistant: The Sales Lead does tend to be optimistic. Do you want me to generate a performance report for him to share?",
            "[Session llm-18, Turn 13] assistant: The Frontend Lead hasn't deployed them to prod yet, but the staging build looks stable. I'll add a reminder for you to check in with her later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.99505615234375,
          "generation_latency_ms": 3789.3640995025635,
          "total_latency_ms": 3798.359155654907
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Which team lead was identified as requiring the beta version of the cache layer for a new UI?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 3] assistant: Looking at CacheLayer4, it reports version v1.2.0, though I'm also seeing signals for v2.0.0-beta on the same endpoint.",
            "[Session llm-19, Turn 3] assistant: I haven't seen a check-in from her yet. However, the system is reporting two different versions for CacheLayer4 right now—v1.2.0 and v2.0.0-beta. It looks like a mid-deployment state.",
            "[Session llm-17, Turn 3] assistant: Taking a look at CacheLayer4 now. It is running, but the version reporting is inconsistent; I'm seeing tags for both v1.2.0 and the v2.0.0 beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.989665985107422,
          "generation_latency_ms": 3988.334894180298,
          "total_latency_ms": 3999.3245601654053
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user requested a health check on the 'databases hosted on Azure', which specific system identifiers did the assistant report on?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7 and PrimaryDB6",
          "predicted_answer": "The assistant reported on PrimaryDB7 and PrimaryDB6.",
          "retrieved_context": [
            "[Session llm-24, Turn 2] user: You know it. He's relentless. Speaking of environments, can you give me a quick health check on the databases hosted on Azure?",
            "[Session llm-14, Turn 11] assistant: That would be PrimaryDB7 and PrimaryDB6; both are on Azure.",
            "[Session llm-2, Turn 5] assistant: Correct, PrimaryDB6 and PrimaryDB7 are both hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.56926155090332,
          "generation_latency_ms": 3445.9893703460693,
          "total_latency_ms": 3457.5586318969727
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_030.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 26,
      "num_probes": 2,
      "ingestion_time_s": 1.6421101093292236,
      "total_evaluation_time_s": 9.293370962142944,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does system:loadbalancer_3 hosted on?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 1] assistant: Good morning. I'm looking at LoadBalancer7 now, and I'm seeing some conflicting signals regarding the version.",
            "[Session llm-25, Turn 3] assistant: LoadBalancer7 is up, but the telemetry is confusing. It's reporting version v1.2.0 in some logs and v2.0.0-beta in others.",
            "[Session llm-4, Turn 7] assistant: Looking at LoadBalancer7 now. It is active, though I am detecting a discrepancy. It appears to be reporting both v1.2.0 and v2.0.0-beta simultaneously."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.66842269897461,
          "generation_latency_ms": 2610.147476196289,
          "total_latency_ms": 2618.8158988952637
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 6] user: Right, okay. And is the weather supposed to be bad tomorrow? I have a hike planned.",
            "[Session llm-2, Turn 7] assistant: The forecast looks clear for tomorrow, so your hike should be safe.",
            "[Session llm-8, Turn 9] assistant: Will do. It seems like our Azure infrastructure is taking a hit today. Did Sophia mention anything about scaling the budget for those services?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.998891830444336,
          "generation_latency_ms": 2934.566020965576,
          "total_latency_ms": 2946.5649127960205
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_031.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 2,
      "ingestion_time_s": 1.6433937549591064,
      "total_evaluation_time_s": 11.05293583869934,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:user has role?",
          "answer_type": "short_answer",
          "gold_answer": "CTO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 1] assistant: I haven't seen any activity from our DevOps lead yet, but Luna does tend to make guest appearances. What do you need help with?",
            "[Session llm-16, Turn 2] user: noted.",
            "[Session llm-17, Turn 2] user: noted."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.275768280029297,
          "generation_latency_ms": 4059.873342514038,
          "total_latency_ms": 4071.1491107940674
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-21, Turn 12] user: Thanks. Oh, and is it supposed to rain today? I rode my bike in.",
            "[Session llm-26, Turn 11] assistant: I'll make a note to check the forecast later. Do you want me to include the Google Cloud resources in Noah's report too?",
            "[Session llm-19, Turn 6] user: Noah will be happy to hear we haven't moved it yet. What about the stuff on Google Cloud?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.513782501220703,
          "generation_latency_ms": 3363.867998123169,
          "total_latency_ms": 3374.3817806243896
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_032.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 29,
      "num_probes": 10,
      "ingestion_time_s": 1.7211604118347168,
      "total_evaluation_time_s": 57.77245616912842,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning their 'cofounder' who is concerned about quarterly report data?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 4] user: Ugh, classic version drift. My cofounder is going to flip if the quarterly reports are wrong because of this.",
            "[Session llm-19, Turn 14] user: Seriously. Anyway, my cofounder needs a report generated from the new engine instance, specifically number 6.",
            "[Session llm-1, Turn 2] user: Yeah, my cofounder was asking about the status of the analytics engines. Specifically, what's going on with number three?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.828971862792969,
          "generation_latency_ms": 3649.003505706787,
          "total_latency_ms": 3659.83247756958
        },
        {
          "probe_id": "world_modeling-category_membership-2",
          "pillar": "world_modeling",
          "subpillar": "type_category_membership",
          "question": "Among the discussed systems, which service is the outlier regarding its hosting provider compared to the primary databases?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer1",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 5] assistant: The primary databases, specifically PrimaryDB4 and PrimaryDB7, are definitely hosted on AWS, along with the rest of the analytics stack.",
            "[Session llm-21, Turn 10] user: Right, the multi-cloud experiment. Liam hates that setup because of the latency. How are the primary databases holding up?",
            "[Session llm-20, Turn 9] assistant: PrimaryDB4 and PrimaryDB7 are both reporting stable status. They are hosted on AWS, consistent with the rest of that cluster."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.57069206237793,
          "generation_latency_ms": 3700.6595134735107,
          "total_latency_ms": 3712.2302055358887
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the team member who holds the role of Product Designer?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-15, Turn 7] assistant: That would be Luna, right? The Product Designer's cat always seems to be involved in her work.",
            "[Session llm-19, Turn 13] assistant: Luna does seem to be the unofficial mascot of the design team these days.",
            "[Session llm-27, Turn 11] assistant: Classic cat behavior. It sounds like the Product Designer has a very hands-on assistant today."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.921073913574219,
          "generation_latency_ms": 3848.623514175415,
          "total_latency_ms": 3858.5445880889893
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the system signals discussed in the latest sessions, which two versions of the analytics engine are running simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-18, Turn 2] user: Yeah, let's check the third analytics engine. I recall seeing two different versions active on the dashboard yesterday. Is that still the case?",
            "[Session llm-8, Turn 3] assistant: If you mean AnalyticsEngine3, it is currently reporting two different running versions: v1.2.0 and v2.0.0-beta.",
            "[Session llm-19, Turn 3] assistant: Taking a look at the analytics engines now. It appears AnalyticsEngine3 is in a bit of a weird state, reporting two different versions active at once."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.160446166992188,
          "generation_latency_ms": 3783.5776805877686,
          "total_latency_ms": 3793.7381267547607
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event caused the delay in the Product Designer uploading the new assets?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat knocked over a plant",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: No, wait until after the standup. By the way, did Olivia upload the new UI assets?",
            "[Session llm-22, Turn 12] user: Exactly. Oh, did Olivia send over those design assets? She said she was working late.",
            "[Session llm-26, Turn 4] user: Weird setup. Anyway, did Olivia upload the new assets?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.991334915161133,
          "generation_latency_ms": 4181.1487674713135,
          "total_latency_ms": 4192.140102386475
        },
        {
          "probe_id": "temporal_episodic-sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Google Cloud latency occur before or after the user first mentioned the dual-version issue with the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "The discussion about the Google Cloud latency occurred before the user first mentioned the dual-version issue with the analytics engine.",
          "retrieved_context": [
            "[Session llm-28, Turn 3] assistant: Pulling up the metrics for AnalyticsEngine3 now. It looks like the instance hosted on AWS is experiencing some latency.",
            "[Session llm-18, Turn 2] user: Yeah, let's check the third analytics engine. I recall seeing two different versions active on the dashboard yesterday. Is that still the case?",
            "[Session llm-27, Turn 3] assistant: Cross-cloud latency can be a pain. Keeping the cache on GCP while PrimaryDB4 and 7 are on AWS definitely adds complexity."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.672258377075195,
          "generation_latency_ms": 3633.3110332489014,
          "total_latency_ms": 3644.9832916259766
        },
        {
          "probe_id": "preference_learning-induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about the infrastructure, what architectural arrangement does the user implicitly dislike?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hosting",
          "predicted_answer": "The user implicitly dislikes a fragmented architectural arrangement.",
          "retrieved_context": [
            "[Session llm-2, Turn 0] user: Can you pull up the infrastructure map? I feel like our hosting strategy is getting a bit fragmented.",
            "[Session llm-0, Turn 1] assistant: I am always ready, Alex. What part of the infrastructure are you looking into today?",
            "[Session llm-4, Turn 2] user: Yeah, let's look at the infrastructure. Specifically, I'm worried about the third analytics engine. Did we actually deploy the beta to it?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.98489761352539,
          "generation_latency_ms": 4421.684980392456,
          "total_latency_ms": 4432.669878005981
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider is the company's email server hosted on?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 2] user: Is that the one hosted on Google Cloud?",
            "[Session llm-15, Turn 5] assistant: The primary databases, specifically PrimaryDB4 and PrimaryDB7, are definitely hosted on AWS, along with the rest of the analytics stack.",
            "[Session llm-16, Turn 14] user: Thanks. Is AnalyticsEngine6 on that same cloud provider?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.803461074829102,
          "generation_latency_ms": 3395.777702331543,
          "total_latency_ms": 3406.581163406372
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the current VP of Marketing?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 7] assistant: That would be Luna, right? The Product Designer's cat always seems to be involved in her work.",
            "[Session llm-0, Turn 10] user: Okay, I need to talk to the backend lead about unifying that eventually. Who is that again? I'm blanking on names today.",
            "[Session llm-6, Turn 4] user: Mostly the analytics stuff running on Bezos's servers. We have a couple of engines spinning there."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.016275405883789,
          "generation_latency_ms": 2914.7567749023438,
          "total_latency_ms": 2923.7730503082275
        },
        {
          "probe_id": "procedural_knowledge-task_state-10",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "Why is the DevOps lead currently unable to assist with migrating the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Swamped with deployment pipeline",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Your DevOps lead put `cachelayer1` over on GCP.",
            "[Session llm-15, Turn 3] assistant: I've added milk to your shopping list. Yes, your DevOps lead still has CacheLayer1 running on GCP.",
            "[Session llm-10, Turn 5] assistant: It's possible your DevOps lead got the tags mixed up during the last deployment window."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.412837982177734,
          "generation_latency_ms": 4308.15577507019,
          "total_latency_ms": 4316.568613052368
        }
      ],
      "summary": {
        "overall_accuracy": 0.4,
        "total_score": 4,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 2,
            "count": 8,
            "accuracy": 0.25
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_033.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 29,
      "num_probes": 12,
      "ingestion_time_s": 1.6666102409362793,
      "total_evaluation_time_s": 68.25260519981384,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Ava",
          "predicted_answer": "Ava",
          "retrieved_context": [
            "[Session llm-9, Turn 8] user: Haha, knowing her, she might be. Also, my cofounder is asking about our cloud spread. She's worried about costs.",
            "[Session llm-11, Turn 2] user: Thanks. Speaking of Ava, she was asking who's currently heading up the backend team. It's still James and Will, right?",
            "[Session llm-0, Turn 10] user: Right, I forgot about that. What about Lucas? He's the other DevOps person, right?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.742757797241211,
          "generation_latency_ms": 3834.791898727417,
          "total_latency_ms": 3847.534656524658
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider is the service managed by the backend lead testing the beta version hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: It looks like v2.0.0-beta is active alongside v1.2.0. You might want to ping Will about that since he's leading that backend push.",
            "[Session llm-10, Turn 5] assistant: I haven't seen a recent commit message from either of the backend leads, but CacheLayer5 is currently reporting from the Azure cluster.",
            "[Session llm-5, Turn 3] assistant: Yes, CacheLayer5 is hosted on Azure. I noticed it's reporting two different versions right now, v1.2.0 and the v2.0.0 beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3537.31369972229,
          "total_latency_ms": 3547.313690185547
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the DevOps team member who was worried about the update?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-21, Turn 3] assistant: I've made a note to ask the DevOps lead about her cat. Regarding the cache, it looks like v1.2.0 is active, but v2.0.0-beta is also reporting in.",
            "[Session llm-15, Turn 12] user: Yeah, she handles the DevOps for it. She actually sent me a slack message about it earlier, but it was mostly a photo of her cat, Luna, sleeping on her laptop.",
            "[Session llm-26, Turn 2] user: Great, a split-brain situation. Can you ping the DevOps team? I need to know who is available to fix this."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.587858200073242,
          "generation_latency_ms": 3969.417095184326,
          "total_latency_ms": 3981.0049533843994
        },
        {
          "probe_id": "declarative_reasoning-verbatim_recall-3",
          "pillar": "declarative_reasoning",
          "subpillar": "verbatim_recall",
          "question": "What specific two-word phrase did the user use to describe the situation where the cache layer was reporting two different version headers?",
          "answer_type": "verbatim",
          "gold_answer": "split-brain situation",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-26, Turn 0] user: I'm looking at the logs for the cache layer. Why am I seeing two different version headers coming through?",
            "[Session llm-2, Turn 9] assistant: She might be referring to the versioning discrepancy on CacheLayer5.",
            "[Session llm-9, Turn 2] user: Yeah, I'm looking at the infrastructure map. Why is CacheLayer5 showing two different versions right now?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.306047439575195,
          "generation_latency_ms": 3954.5226097106934,
          "total_latency_ms": 3965.8286571502686
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask for a reminder about the dentist appointment before or after asking for the dry cleaning reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 0] user: Hey, good morning. Can you set a reminder for me to call the dentist at 2 PM?",
            "[Session llm-28, Turn 5] assistant: Reminder set for the dry cleaning.",
            "[Session llm-13, Turn 0] user: Hey, can you set a reminder for me to pick up dry cleaning at 5 PM?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.38503646850586,
          "generation_latency_ms": 4489.633321762085,
          "total_latency_ms": 4500.018358230591
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user suspect a version mismatch was causing issues with the dashboard?",
          "answer_type": "short_answer",
          "gold_answer": "Lag / Slowness",
          "predicted_answer": "The user suspected a version mismatch because the dashboard claimed to be running v1.",
          "retrieved_context": [
            "[Session llm-6, Turn 4] user: She saw some weird versioning info. The dashboard claims we are running v1.2.0, but the logs are showing startup sequences for v2.0.0-beta.",
            "[Session llm-23, Turn 0] user: I just got out of a long strategy session with my cofounder, and we're trying to figure out why the dashboard is acting up.",
            "[Session llm-18, Turn 6] user: Mia from the ops team thinks it might be a version mismatch. She's seeing logs for v2.0.0-beta, but our documentation says we should still be on v1.2.0."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.027097702026367,
          "generation_latency_ms": 4266.129016876221,
          "total_latency_ms": 4277.156114578247
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on previous orders, what food does the user prefer for team lunches?",
          "answer_type": "short_answer",
          "gold_answer": "Pizza",
          "predicted_answer": "Pizza.",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Mostly lag. Also, remind me to order lunch for the team later, pizza again I think.",
            "[Session llm-15, Turn 6] user: Remind me to grab a sandwich before the 2 PM meeting, by the way. I skipped breakfast.",
            "[Session llm-24, Turn 8] user: I knew it. He owes me lunch. Speaking of lunch, remind me to pick up dry cleaning at 5 PM."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.606361389160156,
          "generation_latency_ms": 3535.9857082366943,
          "total_latency_ms": 3545.5920696258545
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Does the user prefer to message Mia personally about the update or have the assistant do it?",
          "answer_type": "short_answer",
          "gold_answer": "Personally",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: Mia is online, but she set her status to away. She might be dealing with Luna; I recall her mentioning a vet appointment for the cat today.",
            "[Session llm-24, Turn 2] user: Good. Mia was worried the update didn't go through. Is it showing the beta version?",
            "[Session llm-14, Turn 2] user: That sounds messy. I bet Mia was experimenting with a canary deployment again. Speaking of her, did she mention if she's in today?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.84280014038086,
          "generation_latency_ms": 3643.1424617767334,
          "total_latency_ms": 3653.9852619171143
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of James's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-13, Turn 10] user: Haha, yeah. By the way, did James or Will push any backend changes recently?",
            "[Session llm-18, Turn 8] user: Exactly. I need to grab James. As the backend lead, he should know if someone authorized a sneaky upgrade.",
            "[Session llm-4, Turn 2] user: Yeah, she wants to go over the org chart. She's worried about the split between James and Will on the server-side teams."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.588956832885742,
          "generation_latency_ms": 2926.081657409668,
          "total_latency_ms": 2935.6706142425537
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is running on the primary database hosted on Azure?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 13] assistant: PrimaryDB4 is running normally on Azure.",
            "[Session llm-21, Turn 7] assistant: It is a bit spread out. PrimaryDB4 is on Azure with the cache, but PrimaryDB2 is hosted on GCP.",
            "[Session llm-14, Turn 5] assistant: Certainly. PrimaryDB2 is sitting in GCP, whereas PrimaryDB4 is hosted over on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.644840240478516,
          "generation_latency_ms": 3296.1690425872803,
          "total_latency_ms": 3307.813882827759
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-10",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Who are the two individuals usually responsible for handling infrastructure tickets?",
          "answer_type": "short_answer",
          "gold_answer": "Mia and Lucas",
          "predicted_answer": "Mia and Lucas.",
          "retrieved_context": [
            "[Session llm-26, Turn 3] assistant: I'll reach out. It's usually Mia or Lucas handling the infrastructure tickets. By the way, remind me to send you the slide deck for the board meeting later.",
            "[Session llm-18, Turn 2] user: A bit of both. My cofounder wants a full debrief on the infrastructure costs by noon, and I'm staring at some weird numbers.",
            "[Session llm-19, Turn 4] user: Haha, knowing Mia, she has safeguards for that. Actually, speaking of prod, I'm getting a headache looking at our infrastructure map. Why are we so scattered?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.203433990478516,
          "generation_latency_ms": 3874.5577335357666,
          "total_latency_ms": 3883.761167526245
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who should be contacted regarding issues with the backend beta push?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Will",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: It looks like v2.0.0-beta is active alongside v1.2.0. You might want to ping Will about that since he's leading that backend push.",
            "[Session llm-25, Turn 4] user: Let me guess, someone pushed the beta to prod?",
            "[Session llm-14, Turn 10] user: No, I'll handle it. I need to sync with the backend leads first. Did James or Will push anything to prod recently?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.902000427246094,
          "generation_latency_ms": 3245.800495147705,
          "total_latency_ms": 3255.702495574951
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 8,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 9,
            "accuracy": 0.6666666666666666
          },
          "verbatim": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_034.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.6376399993896484,
      "total_evaluation_time_s": 67.79885244369507,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the user's 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte",
          "predicted_answer": "Charlotte",
          "retrieved_context": [
            "[Session llm-21, Turn 1] assistant: I've noted that down. Do you want to prepare any specific metrics for your cofounder?",
            "[Session llm-15, Turn 5] assistant: Added to your personal to-do list. By the way, did you manage to catch up with your cofounder this morning?",
            "[Session llm-18, Turn 3] assistant: No problem. For the top level, you have your cofounder Charlotte handling the CEO duties."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.866020202636719,
          "generation_latency_ms": 3114.2587661743164,
          "total_latency_ms": 3127.124786376953
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member is the owner of the pet named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia is the owner of the pet named Luna.",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: I believe she mentioned her cat is named Luna.",
            "[Session llm-17, Turn 11] assistant: I haven't seen a status update from her, though I recall she mentioned taking her cat, Luna, to the vet this morning.",
            "[Session llm-0, Turn 13] assistant: I haven't seen any status updates regarding Luna, but I'll keep an eye out for Mia."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.129379272460938,
          "generation_latency_ms": 3305.771589279175,
          "total_latency_ms": 3316.9009685516357
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which infrastructure component is hosted on a different cloud provider than the Primary Database and Cache Layer?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer8",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 13] assistant: Correct, both the primary DB and the caching layer are hosted on GCP.",
            "[Session llm-20, Turn 1] assistant: I can help with that. Looking at the current deployment, the bulk of your infrastructure, specifically the primary database and the cache layer, is hosted on GCP.",
            "[Session llm-18, Turn 7] assistant: Most of the core components, specifically the PrimaryDB3, CacheLayer4, and AuthService5, are all hosted on GCP."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.1541748046875,
          "generation_latency_ms": 3751.6283988952637,
          "total_latency_ms": 3762.782573699951
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Is the authentication service currently running a single version or multiple versions simultaneously?",
          "answer_type": "short_answer",
          "gold_answer": "Multiple versions",
          "predicted_answer": "Multiple versions of the authentication service appear to be running simultaneously, as indicated by \"mixed version headers,\" \"weird versioning tags,\" and \"logs that don't match",
          "retrieved_context": [
            "[Session llm-20, Turn 6] user: Thanks. Back to the system—Sophia mentioned she was doing some updates on the auth service. What version are we actually running there?",
            "[Session llm-0, Turn 6] user: Okay, weird question, but what version is the auth service running? I'm seeing some logs that don't match.",
            "[Session llm-14, Turn 8] user: No, keep it at noon. I am seeing some weird versioning tags on the auth service though. What exactly is running there?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.772134780883789,
          "generation_latency_ms": 4350.4438400268555,
          "total_latency_ms": 4359.215974807739
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the production logs, what is the primary architectural cause of the observed latency?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud hop",
          "predicted_answer": "The primary architectural cause of the observed latency is the cross-cloud hop,",
          "retrieved_context": [
            "[Session llm-23, Turn 0] user: I'm looking at the latency logs for the production environment. It looks like the cross-cloud hop is costing us a few milliseconds.",
            "[Session llm-4, Turn 2] user: Mostly the cross-cloud latency. Sophia was mentioning that having our traffic ingress on Amazon while the heavy lifting happens elsewhere is causing lag.",
            "[Session llm-1, Turn 0] user: I'm looking at our infrastructure map and the latency on the ingress point seems high."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.001276016235352,
          "generation_latency_ms": 3765.1264667510986,
          "total_latency_ms": 3777.127742767334
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did Luna knock over a plant before or after she was reported getting stuck in a box?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 5] assistant: I see a recent message in the #random channel. Something about \"Luna knocking over a plant.\"",
            "[Session llm-15, Turn 9] assistant: I don't see new files from her yet. I assume Luna was causing trouble again?",
            "[Session llm-5, Turn 5] assistant: Not yet, our Product Designer mentioned she was distracted because Luna got into some trouble this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.765552520751953,
          "generation_latency_ms": 2855.0362586975098,
          "total_latency_ms": 2865.8018112182617
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the frontend service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 0] user: I need to prep for a budget meeting with my cofounder later. Can you give me a quick rundown of where our main backend services are currently hosted?",
            "[Session llm-18, Turn 7] assistant: Most of the core components, specifically the PrimaryDB3, CacheLayer4, and AuthService5, are all hosted on GCP.",
            "[Session llm-15, Turn 0] user: Hey, can you check which cloud provider is handling our traffic ingress right now? I feel like the latency is slightly off."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.352445602416992,
          "generation_latency_ms": 3496.8416690826416,
          "total_latency_ms": 3506.1941146850586
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the Sales Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 2] user: Mostly revenue. Oliver keeps selling features we haven't even scoped yet. Typical sales lead behavior, right?",
            "[Session llm-5, Turn 11] assistant: The Sales Lead is currently on a call, but you might catch him afterwards.",
            "[Session llm-23, Turn 3] assistant: I've set a reminder for you. Hopefully, our product designer can get her cat out of that predicament before the next stand-up."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.881423950195312,
          "generation_latency_ms": 3916.7416095733643,
          "total_latency_ms": 3927.6230335235596
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Does the user prefer maintaining the current multi-cloud setup or consolidating the infrastructure?",
          "answer_type": "short_answer",
          "gold_answer": "Consolidating",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: Multi-cloud architectures can be complex. Do you want me to look into migration costs for moving the load balancer to Google Cloud to match the others?",
            "[Session llm-2, Turn 9] assistant: A multi-cloud setup, then. I'll make a note to update the architecture docs.",
            "[Session llm-17, Turn 5] assistant: I will highlight the cost implications of hosting LoadBalancer8 on AWS while the cache, database, and other services reside on GCP."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.005805969238281,
          "generation_latency_ms": 4263.4594440460205,
          "total_latency_ms": 4275.465250015259
        },
        {
          "probe_id": "preference_learning-preference_induction-2",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user feel about the topology map showing the load balancer on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "Annoyed",
          "predicted_answer": "The user is questioning why the load balancer is on AWS when the rest of the stack is on Google, and needs to explain this situation.",
          "retrieved_context": [
            "[Session llm-17, Turn 4] user: Yes, please. Specifically, I need to explain why we are paying for AWS just for the load balancer when the rest of the stack is on Google.",
            "[Session llm-7, Turn 3] assistant: Yes, the main load balancer is running on AWS, unlike the rest of the infrastructure.",
            "[Session llm-16, Turn 2] user: No, I'm good. Just grabbing a coffee. Actually, I was looking at the network topology earlier. Why is LoadBalancer8 sitting on Amazon's infrastructure?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.740995407104492,
          "generation_latency_ms": 4199.270963668823,
          "total_latency_ms": 4210.011959075928
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What general observation did the assistant make regarding the nature of architectures that span AWS and GCP?",
          "answer_type": "short_answer",
          "gold_answer": "They can be complex",
          "predicted_answer": "The assistant observed that it is an unusual split and a bit of a mix, which might be adding",
          "retrieved_context": [
            "[Session llm-13, Turn 9] assistant: Sounds like a plan for the DevOps lead. Shall I draft a message to Sophia about the AWS-GCP split?",
            "[Session llm-6, Turn 3] assistant: It is a bit of a mix. While the load balancer is over on AWS, the rest of the core systems like the cache layer and primary database are sitting on GCP.",
            "[Session llm-11, Turn 7] assistant: It is an unusual split. Keeping the primary database and the cache layer on GCP while the balancer is on AWS might be adding latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.899471282958984,
          "generation_latency_ms": 3873.398542404175,
          "total_latency_ms": 3885.298013687134
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "When the user mentioned buying treats for Luna, what action did the assistant confirm taking?",
          "answer_type": "short_answer",
          "gold_answer": "Added a reminder",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 5] assistant: Not yet, our Product Designer mentioned she was distracted because Luna got into some trouble this morning.",
            "[Session llm-3, Turn 5] assistant: I've added a reminder for lunch. I assume she's sharing updates on Luna?",
            "[Session llm-11, Turn 5] assistant: Oh, poor Luna. I hope it’s nothing serious. On a related note, did you get a chance to look at the infrastructure report Sophia sent over?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.596036911010742,
          "generation_latency_ms": 3374.354362487793,
          "total_latency_ms": 3384.9503993988037
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 10,
            "accuracy": 0.3
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_035.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 36,
      "num_probes": 10,
      "ingestion_time_s": 1.7769815921783447,
      "total_evaluation_time_s": 56.22878456115723,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention a meeting with their cofounder?",
          "answer_type": "short_answer",
          "gold_answer": "Mia",
          "predicted_answer": "Mia",
          "retrieved_context": [
            "[Session llm-0, Turn 14] user: Exactly. Also, did my cofounder mention when the board meeting is?",
            "[Session llm-13, Turn 10] user: She is. My cofounder, Mia, runs the company as CEO, so Olivia reports to her, while I handle the CTO duties.",
            "[Session llm-31, Turn 2] user: That's a relief. I have a meeting with my cofounder in an hour, and I didn't want to bring bad news."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00246810913086,
          "generation_latency_ms": 4008.509874343872,
          "total_latency_ms": 4020.512342453003
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the name of the pet belonging to the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "The name of the pet belonging to the Sales Lead is Luna.",
          "retrieved_context": [
            "[Session llm-13, Turn 9] assistant: Classic pet owner distraction. Speaking of Olivia, is she still leading the sales push?",
            "[Session llm-0, Turn 9] assistant: I don't browse social media, but I recall the Sales Lead has a cat named Luna. Cute photos, I assume?",
            "[Session llm-23, Turn 11] assistant: I believe the Sales Lead is in today. Is that Luna she is always talking about?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.335683822631836,
          "generation_latency_ms": 3139.288902282715,
          "total_latency_ms": 3149.6245861053467
        },
        {
          "probe_id": "preference_learning-preference_induction-3",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routines, what beverage do they require to function properly?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Espresso (or coffee).",
          "retrieved_context": [
            "[Session llm-35, Turn 0] user: Hey, good morning. I need another espresso before I can function properly today.",
            "[Session llm-23, Turn 1] assistant: Good morning, Alex. Coffee sounds like a plan. Is the day starting off hectic?",
            "[Session llm-23, Turn 0] user: Morning. I am dragging today, definitely need a second cup of coffee."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.703968048095703,
          "generation_latency_ms": 4500.138521194458,
          "total_latency_ms": 4511.842489242554
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-4",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "On which cloud provider is the primary database hosted?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 3] assistant: Yes, CacheLayer6 is definitely hosted on AWS. It's isolated from the rest of the stack that we moved over to Google Cloud.",
            "[Session llm-10, Turn 8] user: Thanks. My cofounder was also asking about our Google Cloud spend. Which services are we running there again?",
            "[Session llm-27, Turn 3] assistant: No, actually. That specific cache instance is hosted on AWS, while the rest of your primary services are elsewhere."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.003639221191406,
          "generation_latency_ms": 3150.395154953003,
          "total_latency_ms": 3159.3987941741943
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-5",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "When the user requests the 'morning status checks', which service is verified immediately after the load balancers?",
          "answer_type": "short_answer",
          "gold_answer": "AuthService9",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 6] user: That sounds messy. Is that service even in the same environment as the load balancers?",
            "[Session llm-14, Turn 2] user: My lips are sealed. I'm just reviewing our infrastructure map. Are the load balancers, specifically 9 and 2, sitting in the same environment as AuthService9?",
            "[Session llm-31, Turn 0] user: Good morning. I'm barely awake, need to grab a coffee soon. Can you check the status of our load balancers on Google Cloud?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.122941970825195,
          "generation_latency_ms": 4245.071172714233,
          "total_latency_ms": 4256.194114685059
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user mentioned being 'barely awake', what specific data were they preparing to review with Mia?",
          "answer_type": "short_answer",
          "gold_answer": "Quarterly figures",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 3] assistant: Reminder set for 5 PM. Regarding Mia's concern, the latency looks nominal, but I'm seeing something odd with the versioning.",
            "[Session llm-12, Turn 11] assistant: Mia hasn't sent a message about lunch, but as CEO, her schedule looks pretty packed until 2 PM.",
            "[Session llm-4, Turn 8] user: She sent me a funny video of Luna chasing a laser pointer this morning, so I assume she's awake at least."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.52656364440918,
          "generation_latency_ms": 3534.1715812683105,
          "total_latency_ms": 3542.6981449127197
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-7",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Given that the authentication service is co-located with the load balancers, which cloud provider hosts it?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "GCP",
          "retrieved_context": [
            "[Session llm-27, Turn 7] assistant: Correct. Both load balancers and AuthService9 are currently hosted on GCP.",
            "[Session llm-29, Turn 4] user: Yeah, mostly she's confused about why we are paying two different providers. I need to explain that the load balancers and auth service are on Google's platform.",
            "[Session llm-35, Turn 5] assistant: No changes there. Service number 9 for auth is co-located with the load balancers on the Google side."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.347127914428711,
          "generation_latency_ms": 3484.8506450653076,
          "total_latency_ms": 3495.1977729797363
        },
        {
          "probe_id": "world_modeling-relationship_mapping-8",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "[Session llm-0, Turn 9] assistant: I don't browse social media, but I recall the Sales Lead has a cat named Luna. Cute photos, I assume?",
            "[Session llm-28, Turn 8] user: She's probably distracted. She got a new cat named Luna who is apparently tearing up her furniture.",
            "[Session llm-8, Turn 6] user: She's probably busy chasing Luna around the house. Saw that cat knock over a plant on video chat yesterday."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.995698928833008,
          "generation_latency_ms": 3470.195531845093,
          "total_latency_ms": 3480.191230773926
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the single definitive running version of CacheLayer6?",
          "answer_type": "short_answer",
          "gold_answer": "Ambiguous",
          "predicted_answer": "Unknown. The version reporting for CacheLayer6 is ambiguous and inconsistent, showing both v1.2",
          "retrieved_context": [
            "[Session llm-30, Turn 5] assistant: No, CacheLayer6 is actually sitting over on AWS. I'm seeing some ambiguity on the version though; it's reporting both v1.2.0 and a beta release, v2.0.0-beta.",
            "[Session llm-0, Turn 4] user: Ugh, typical. Is CacheLayer6 running the beta or the stable release?",
            "[Session llm-35, Turn 9] assistant: Correct. CacheLayer6 is hosted on AWS. It looks like it's reporting some mixed versioning telemetry, though."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.526729583740234,
          "generation_latency_ms": 4004.1072368621826,
          "total_latency_ms": 4013.633966445923
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-10",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "In the session where the user reviewed architecture diagrams, did they ask about the load balancers before or after asking for a lunch reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-30, Turn 0] user: I'm reviewing the architecture diagrams before my weekly sync with my cofounder. Can you confirm where the load balancers are hosted?",
            "[Session llm-2, Turn 2] user: Perfect. I wanted to double-check our infrastructure setup before the meeting. Are the load balancers still running on Google's cloud?",
            "[Session llm-16, Turn 0] user: Morning. Can you pull up the metrics for our load balancers? I feel like the latency is creeping up again."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.662960052490234,
          "generation_latency_ms": 3364.7193908691406,
          "total_latency_ms": 3376.382350921631
        }
      ],
      "summary": {
        "overall_accuracy": 0.7,
        "total_score": 7,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 9,
            "accuracy": 0.6666666666666666
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_036.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 11,
      "ingestion_time_s": 1.7032372951507568,
      "total_evaluation_time_s": 61.445682764053345,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to by the user as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "[Session llm-13, Turn 3] assistant: Yes, your cofounder is still holding the Chief Executive officer position.",
            "[Session llm-22, Turn 2] user: You could say that. My cofounder James dragged me into a strategy meeting first thing.",
            "[Session llm-8, Turn 2] user: I have a sync with my cofounder, James, in an hour. He's worried about our infrastructure spread."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.026952743530273,
          "generation_latency_ms": 3752.776622772217,
          "total_latency_ms": 3765.803575515747
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What professional role is shared by both Emma and Mia?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead.",
          "retrieved_context": [
            "[Session llm-13, Turn 5] assistant: That is correct, both Emma and Mia are currently designated as Sales Leads.",
            "[Session llm-14, Turn 2] user: It was intense. Anyway, getting back to work—I'm looking at the org chart for Nebula Corp. Are Emma and Mia sharing responsibilities?",
            "[Session llm-4, Turn 10] user: Not yet. I don't want to panic the sales team. Emma and Mia are already stressed about their quarterly targets, and they don't need technical drama."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.33272933959961,
          "generation_latency_ms": 3168.9724922180176,
          "total_latency_ms": 3177.305221557617
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two specific cloud providers are generating the high egress fees due to cross-cloud traffic?",
          "answer_type": "short_answer",
          "gold_answer": "Azure and AWS",
          "predicted_answer": "Google and Azure.",
          "retrieved_context": [
            "[Session llm-4, Turn 4] user: Mostly the infrastructure costs. It's absurd that we are paying for three different clouds. Having the auth service on Google while the main database is on Azure is racking up egress fees.",
            "[Session llm-20, Turn 5] assistant: That cross-cloud traffic between Azure and AWS would definitely explain the high costs. Is the caching layer co-located with the load balancer?",
            "[Session llm-20, Turn 3] assistant: Timer set for ten minutes. Regarding the egress fees, moving data between different providers is likely the culprit. Which services are chatting across clouds?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.512828826904297,
          "generation_latency_ms": 4148.322582244873,
          "total_latency_ms": 4158.835411071777
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-4",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which infrastructure component is violating the production stability constraint by running a beta version?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer5",
          "predicted_answer": "The cache is running a beta version (v2.0.0-beta).",
          "retrieved_context": [
            "[Session llm-5, Turn 2] user: That beta version makes me nervous. My cofounder was just asking about stability metrics.",
            "[Session llm-2, Turn 7] assistant: Correct, as the Frontend Lead, she likely has insight into why the beta version is appearing.",
            "[Session llm-24, Turn 2] user: He thinks we might have deployed the wrong version. Are we on the beta or the stable 1.2 release?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.614706039428711,
          "generation_latency_ms": 3687.9754066467285,
          "total_latency_ms": 3697.590112686157
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific disruption involving a pet occurred during the frontend sync meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Olivia's cat Luna jumped on her keyboard.",
          "retrieved_context": [
            "[Session llm-15, Turn 7] assistant: It's always lively when pets join meetings. Since Olivia is leading the frontend team, did she pinpoint where the latency is coming from?",
            "[Session llm-21, Turn 6] user: It's a mess. Olivia mentioned during standup that the frontend is timing out. She was so stressed, though it was funny when Luna jumped on her desk and knocked over her mic.",
            "[Session llm-23, Turn 4] user: No, I'll handle him. He's busy with investors anyway. Speaking of distractions, Olivia's cat Luna jumped on her keyboard during the frontend sync today."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.467290878295898,
          "generation_latency_ms": 3632.2975158691406,
          "total_latency_ms": 3642.7648067474365
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the user specifically worried about James discovering the current state of the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Testing beta in production",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: I'm looking at CacheLayer5 now. It seems to be running on AWS infrastructure. What specifically is James worried about?",
            "[Session llm-24, Turn 0] user: Hey, can you check the status of the cache layer? My cofounder James thinks it's acting up.",
            "[Session llm-11, Turn 7] assistant: It's a bit ambiguous. CacheLayer5 is reporting v1.2.0, but I'm also detecting v2.0.0-beta active on some nodes."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.765386581420898,
          "generation_latency_ms": 4341.954708099365,
          "total_latency_ms": 4351.720094680786
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the company's email server?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-14, Turn 9] assistant: Yes, PrimaryDB5 is hosted on Azure. It certainly covers all the major providers.",
            "[Session llm-15, Turn 2] user: Well, James—my cofounder—insisted on using Microsoft's cloud for the main storage, PrimaryDB5. Since he's the CEO, he usually gets his way on vendor choices.",
            "[Session llm-11, Turn 3] assistant: Yes, PrimaryDB5 is currently hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.020256042480469,
          "generation_latency_ms": 3096.177101135254,
          "total_latency_ms": 3106.1973571777344
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "What is the specific version number currently running on the PrimaryDB5 instance?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 9] assistant: Correct, PrimaryDB5 is hosted on Azure.",
            "[Session llm-9, Turn 9] assistant: PrimaryDB5 is running on Azure.",
            "[Session llm-13, Turn 9] assistant: No, looking at the config for PrimaryDB5, it is actually hosted over on the Microsoft cloud."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001897811889648,
          "generation_latency_ms": 3273.712158203125,
          "total_latency_ms": 3283.7140560150146
        },
        {
          "probe_id": "preference_learning-preference_induction-9",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "When the assistant offered to draft a message to James regarding the version mismatch, how did the user prefer to handle the communication?",
          "answer_type": "short_answer",
          "gold_answer": "Handle it personally",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 3] assistant: It's currently sitting on AWS. Do you want me to draft a message to James explaining the version mismatch?",
            "[Session llm-0, Turn 7] assistant: I've made a note to discuss the version conflict with James.",
            "[Session llm-1, Turn 1] assistant: I can help with that. Are you referring to James? What specifically did he want to know?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.155128479003906,
          "generation_latency_ms": 4266.504764556885,
          "total_latency_ms": 4277.659893035889
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-10",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "According to the user's routine, what is the required duration for steeping tea?",
          "answer_type": "short_answer",
          "gold_answer": "Ten minutes",
          "predicted_answer": "Ten minutes.",
          "retrieved_context": [
            "[Session llm-20, Turn 14] user: Tell me about it. Oh, cancel that tea timer, I think I already drank it.",
            "[Session llm-20, Turn 2] user: He's mostly freaking out about the egress fees. Also, set a timer for ten minutes, I need to steep my tea.",
            "[Session llm-5, Turn 4] user: Exactly. On a totally different note, can you remind me to grab a coffee in twenty minutes? I'm falling asleep here."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001659393310547,
          "generation_latency_ms": 2774.0745544433594,
          "total_latency_ms": 2784.07621383667
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Based on the user's discussions about the cloud bill, what architectural choice is identified as the primary cause of high costs?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-cloud data transfer",
          "predicted_answer": "The primary cause of high costs is having services scattered across multiple different cloud providers, leading",
          "retrieved_context": [
            "[Session llm-16, Turn 5] assistant: Is the CEO worried about the cloud costs again?",
            "[Session llm-4, Turn 4] user: Mostly the infrastructure costs. It's absurd that we are paying for three different clouds. Having the auth service on Google while the main database is on Azure is racking up egress fees.",
            "[Session llm-20, Turn 5] assistant: That cross-cloud traffic between Azure and AWS would definitely explain the high costs. Is the caching layer co-located with the load balancer?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.31129264831543,
          "generation_latency_ms": 4307.270050048828,
          "total_latency_ms": 4318.581342697144
        }
      ],
      "summary": {
        "overall_accuracy": 0.45454545454545453,
        "total_score": 5,
        "count": 11,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_037.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.682023048400879,
      "total_evaluation_time_s": 67.92540168762207,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific individual is the user referring to as their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 13] assistant: I'll note that the Product Designers are finalized on the UI. I'll have that infrastructure report ready for your cofounder shortly.",
            "[Session llm-26, Turn 0] user: Morning. Has my cofounder arrived yet? We need to discuss the quarterly goals.",
            "[Session llm-17, Turn 0] user: Hey, is my cofounder around? I need to sync with him regarding the quarterly goals."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.284900665283203,
          "generation_latency_ms": 3587.2466564178467,
          "total_latency_ms": 3597.53155708313
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which team member flagged the latency issues on the database hosted on Google Cloud?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia",
          "predicted_answer": "Olivia, the backend lead, flagged the latency issues.",
          "retrieved_context": [
            "[Session llm-22, Turn 2] user: A little bit. Olivia, our backend lead, flagged some heavy latency on the main database over in the Google cloud environment.",
            "[Session llm-16, Turn 7] assistant: If James is concerned, we should probably double-check the connection to PrimaryDB7 on GCP as well, just to rule out any database latency issues.",
            "[Session llm-25, Turn 13] assistant: Connectivity between the Azure instance and PrimaryDB7 on GCP appears consistent, though cross-cloud latency is slightly higher as expected."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.212919235229492,
          "generation_latency_ms": 3649.5158672332764,
          "total_latency_ms": 3662.728786468506
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite earlier conflicting headers, what version is the GCP load balancer confirmed to be running after the update?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 1] assistant: Good morning. I'm looking at LoadBalancer9 on GCP now. It appears to be healthy and running version v1.2.0.",
            "[Session llm-27, Turn 10] user: Okay. Can you check the versioning on the GCP balancer? I feel like we might have a mismatch.",
            "[Session llm-1, Turn 3] assistant: Looking at LoadBalancer9 on GCP. It seems your DevOps specialist has been busy; I'm seeing a version conflict with v2.0.0-beta running alongside v1.2.0."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998964309692383,
          "generation_latency_ms": 4352.25772857666,
          "total_latency_ms": 4363.2566928863525
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-1",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the 'segregated' load balancer hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 6] user: He mentioned the load balancer hosted on Google Cloud.",
            "[Session llm-19, Turn 2] user: Thanks. I just got off the phone with James; my cofounder is stressing about our cloud bills again. Which load balancer do we have on Azure?",
            "[Session llm-17, Turn 6] user: Classic. Alright, let's talk infra. Will was asking about the load balancer on the Microsoft cloud. Which one is that again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.893583297729492,
          "generation_latency_ms": 3019.489288330078,
          "total_latency_ms": 3030.3828716278076
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific action taken by the DevOps lead caused LoadBalancer9 to switch to the beta version?",
          "answer_type": "short_answer",
          "gold_answer": "pushed an update",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Are you referring to LoadBalancer9? It appears the DevOps team has been active there, possibly testing the beta version.",
            "[Session llm-27, Turn 11] assistant: It looks ambiguous. System logs show LoadBalancer9 running v1.2.0, but there are also signals indicating v2.0.0-beta is active.",
            "[Session llm-12, Turn 1] assistant: Good morning. It looks like LoadBalancer9 is currently flagging as version v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.047840118408203,
          "generation_latency_ms": 4112.337827682495,
          "total_latency_ms": 4123.385667800903
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user request the reminder to water plants before or after the reminder to pick up dry cleaning mentioned in Session llm-22?",
          "answer_type": "short_answer",
          "gold_answer": "after",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 7] assistant: Reminder set for 5 PM: pick up dry cleaning.",
            "[Session llm-0, Turn 0] user: Hey, good morning. Can you set a reminder for me to pick up dry cleaning at 5 PM?",
            "[Session llm-24, Turn 3] assistant: Sure thing, I've added a reminder for you to water the plants."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.567903518676758,
          "generation_latency_ms": 3851.619243621826,
          "total_latency_ms": 3862.187147140503
        },
        {
          "probe_id": "preference_learning-explicit_preferences-1",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When preparing reports for the CEO, what specific metric does he prefer to focus on?",
          "answer_type": "short_answer",
          "gold_answer": "uptime",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 1] assistant: I can help you prepare the reports. Did James mention any specific metrics he wants to focus on?",
            "[Session llm-4, Turn 3] assistant: That sounds like James. Do you need a breakdown of the current CEO-level cost reports?",
            "[Session llm-18, Turn 9] assistant: Typical CEO concerns. I can prepare a usage report for Nebula Corp if that helps calm him down."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.095026016235352,
          "generation_latency_ms": 3456.5067291259766,
          "total_latency_ms": 3464.601755142212
        },
        {
          "probe_id": "preference_learning-constraint_hierarchy-1",
          "pillar": "preference_learning",
          "subpillar": "constraint_hierarchy",
          "question": "The user is nervous about beta versions but accepts them under what condition?",
          "answer_type": "short_answer",
          "gold_answer": "Olivia signed off",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 12] user: That's dangerous. Will is going to flip out if he finds out beta code hit production.",
            "[Session llm-13, Turn 2] user: What version is the one on Google running? My cofounder was asking if the beta went out.",
            "[Session llm-0, Turn 4] user: Okay, thanks. I noticed some versioning flags on it. Is it running v1.2.0 or that new beta?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.653329849243164,
          "generation_latency_ms": 3548.879384994507,
          "total_latency_ms": 3558.53271484375
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Product Designer named Lucas?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 5] assistant: For this one, Lucas and Noah are the Product Designers. Noah actually shared some funny photos of his cat, Luna, in the team chat earlier.",
            "[Session llm-24, Turn 11] assistant: Yes, both he and Lucas are focused on the Product Design tasks right now.",
            "[Session llm-22, Turn 11] assistant: That sounds like a productive use of the product design team's time. Did Lucas get a word in edgewise?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.355472564697266,
          "generation_latency_ms": 3291.431427001953,
          "total_latency_ms": 3301.7868995666504
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the Azure-hosted load balancer currently running?",
          "answer_type": "short_answer",
          "gold_answer": "not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 11] assistant: LoadBalancer5 is still active on Azure, yes. It hasn't been decommissioned yet.",
            "[Session llm-21, Turn 7] assistant: Yes, LoadBalancer5 is currently running on Azure, while LoadBalancer9 is hosted on GCP alongside the database.",
            "[Session llm-24, Turn 13] assistant: No, actually, LoadBalancer5 is hosted over on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.999990463256836,
          "generation_latency_ms": 3302.5760650634766,
          "total_latency_ms": 3312.5760555267334
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What condition does the Backend Lead typically verify before allowing a rollout?",
          "answer_type": "short_answer",
          "gold_answer": "backend is solid",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 11] assistant: I imagine the Backend Lead would prefer a cleaner deployment state.",
            "[Session llm-13, Turn 11] assistant: Good plan. Best to let your DevOps specialist handle it before the Backend Lead finds out.",
            "[Session llm-14, Turn 9] assistant: I'll monitor the logs for the Backend Lead. By the way, Noah sent over those mockups."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.013818740844727,
          "generation_latency_ms": 4651.676893234253,
          "total_latency_ms": 4661.690711975098
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How did the assistant determine the version conflict on LoadBalancer9?",
          "answer_type": "short_answer",
          "gold_answer": "checking logs",
          "predicted_answer": "The assistant determined the version conflict on LoadBalancer9 by seeing both v1.2.",
          "retrieved_context": [
            "[Session llm-19, Turn 5] assistant: It might be related to the versioning; I see both v1.2.0 and v2.0.0-beta tagged on LoadBalancer9.",
            "[Session llm-12, Turn 1] assistant: Good morning. It looks like LoadBalancer9 is currently flagging as version v2.0.0-beta.",
            "[Session llm-7, Turn 8] user: Thanks. Back to the infrastructure—I'm seeing conflicting logs for LoadBalancer9. What version is it pushing?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.889602661132812,
          "generation_latency_ms": 3979.468822479248,
          "total_latency_ms": 3989.358425140381
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 12,
            "accuracy": 0.25
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_038.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 12,
      "ingestion_time_s": 1.624096155166626,
      "total_evaluation_time_s": 67.289705991745,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which executive team member is reportedly concerned about the infrastructure costs?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "The cofounder.",
          "retrieved_context": [
            "[Session llm-19, Turn 2] user: Thanks. I have a budget meeting with my cofounder later. She's worried about our AWS spend. What's actually running there right now?",
            "[Session llm-16, Turn 2] user: Well, my cofounder is stressing about our burn rate, so she wants a full infrastructure audit presented today.",
            "[Session llm-15, Turn 2] user: Busy. I'm reviewing the infrastructure map. It's still a bit of a headache having our services split across different cloud providers."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 13.260602951049805,
          "generation_latency_ms": 4092.214822769165,
          "total_latency_ms": 4105.475425720215
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which service is co-located on the same cloud provider as the system handling traffic flow?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer3",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-15, Turn 2] user: Busy. I'm reviewing the infrastructure map. It's still a bit of a headache having our services split across different cloud providers.",
            "[Session llm-18, Turn 9] assistant: Those two services, the analytics engine and the primary database, are hosted on AWS.",
            "[Session llm-11, Turn 7] assistant: That does sound like a move the Backend Lead might make. Should I check the load balancer on that same cloud provider to see if it's routing traffic weirdly?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.23809814453125,
          "generation_latency_ms": 3835.233211517334,
          "total_latency_ms": 3846.4713096618652
        },
        {
          "probe_id": "declarative_reasoning-multi_hop-1",
          "pillar": "declarative_reasoning",
          "subpillar": "multi_hop_reasoning",
          "question": "The database connected to the Google-hosted load balancer is running on which infrastructure provider?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-26, Turn 11] assistant: Yes, LoadBalancer5 is currently hosted on GCP. I'm registering slightly elevated latency on the cross-cloud bridge to the database.",
            "[Session llm-21, Turn 2] user: Maybe later. Right now, I'm worried about the traffic distribution. Can you check the status of that load balancer we have on Google's infrastructure?",
            "[Session llm-24, Turn 5] assistant: Correct. CacheLayer3 and LoadBalancer5 are currently hosted on GCP, while PrimaryDB6 is running on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.596181869506836,
          "generation_latency_ms": 4102.75936126709,
          "total_latency_ms": 4111.355543136597
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions of the caching service are currently active according to the latest diagnostics?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "According to the latest diagnostics, CacheLayer3 is currently reporting two different active versions: v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-25, Turn 10] user: He's fine. Just waiting on Elijah to finish the backend updates. Speaking of which, what version is that cache service running now?",
            "[Session llm-0, Turn 2] user: Great. By the way, is the new caching service acting up again? I saw some alerts on my phone earlier.",
            "[Session llm-21, Turn 10] user: Thanks. Now, look at the caching layer. I'm seeing some weird hit rates. What version is actually live right now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.599613189697266,
          "generation_latency_ms": 3623.4941482543945,
          "total_latency_ms": 3634.093761444092
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific deployment failure did the user identify as the cause of the cache latency spikes?",
          "answer_type": "short_answer",
          "gold_answer": "Stuck canary deployment",
          "predicted_answer": "A version mismatch in CacheLayer3.",
          "retrieved_context": [
            "[Session llm-10, Turn 4] user: It looks like the third cache layer is misbehaving. Since it's sitting over on Google's cloud, I thought the latency would be better, but it's been weird all morning.",
            "[Session llm-9, Turn 5] assistant: I'm seeing a mix of v1.2.0 and v2.0.0-beta instances for CacheLayer3. That inconsistency could definitely be causing the cache misses.",
            "[Session llm-26, Turn 0] user: Hey, can you pull up the current status for CacheLayer3? I'm seeing some weird latency numbers on my dashboard."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.129928588867188,
          "generation_latency_ms": 4035.4866981506348,
          "total_latency_ms": 4045.616626739502
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user identify the stuck canary deployment before or after requesting a lunch reminder?",
          "answer_type": "short_answer",
          "gold_answer": "After",
          "predicted_answer": "The user identified the stuck canary deployment after requesting a lunch reminder.",
          "retrieved_context": [
            "[Session llm-26, Turn 4] user: Classic. We probably have a stuck canary deployment. Remind me to message the backend team about that in an hour.",
            "[Session llm-21, Turn 12] user: That explains the inconsistency. Elijah must have started a canary deployment without telling me. Can you ping our backend lead?",
            "[Session llm-12, Turn 11] assistant: Reminder set: order lunch in 20 minutes. Should I alert the backend team about the split-brain versioning on the cache?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.03940200805664,
          "generation_latency_ms": 3131.5720081329346,
          "total_latency_ms": 3140.611410140991
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How did the user respond to the assistant's offer to block calendar time for the hiking trip?",
          "answer_type": "short_answer",
          "gold_answer": "Deferred it",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 1] assistant: Good morning, Alex. It looks mostly sunny with a high of 75, perfect for hiking. Do you want me to block out some time on your calendar?",
            "[Session llm-15, Turn 1] assistant: Good morning, Alex. I've added that reminder for 5 PM. How is the day looking otherwise?",
            "[Session llm-2, Turn 3] assistant: Reminder set for 5 PM to call your mom. I'll make a note for the Backend Lead to investigate the version mismatch."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.725902557373047,
          "generation_latency_ms": 3392.406225204468,
          "total_latency_ms": 3404.132127761841
        },
        {
          "probe_id": "preference_learning-preference_induction-2",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What specific administrative action does the user consistently request across multiple sessions?",
          "answer_type": "short_answer",
          "gold_answer": "Setting reminders",
          "predicted_answer": "The user consistently requests information about the operational status or health of services/systems.",
          "retrieved_context": [
            "[Session llm-2, Turn 10] user: What is she working on?",
            "[Session llm-22, Turn 8] user: Is he still complaining about the API lag? Or is he just sharing pictures of his cat again?",
            "[Session llm-0, Turn 2] user: Great. By the way, is the new caching service acting up again? I saw some alerts on my phone earlier."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.41269302368164,
          "generation_latency_ms": 4164.6318435668945,
          "total_latency_ms": 4175.044536590576
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the frontend application?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: Those two services, the analytics engine and the primary database, are hosted on AWS.",
            "[Session llm-4, Turn 8] user: I pinged Elijah about it. He's digging into the logs on the Google Cloud console now since that's where we host it.",
            "[Session llm-15, Turn 2] user: Busy. I'm reviewing the infrastructure map. It's still a bit of a headache having our services split across different cloud providers."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.569883346557617,
          "generation_latency_ms": 3023.7934589385986,
          "total_latency_ms": 3033.3633422851562
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-2",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's pet?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-4, Turn 3] assistant: Always nice to see pet photos. Is his cat doing well?",
            "[Session llm-26, Turn 7] assistant: I don't monitor Slack images, but I assume it involves his cat, Luna?",
            "[Session llm-22, Turn 8] user: Is he still complaining about the API lag? Or is he just sharing pictures of his cat again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.55057144165039,
          "generation_latency_ms": 2954.298973083496,
          "total_latency_ms": 2963.8495445251465
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-1",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What method did the user employ to check for cross-cloud bottlenecks between GCP and AWS?",
          "answer_type": "short_answer",
          "gold_answer": "Cross-reference with database",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 5] assistant: Checking PrimaryDB6 on AWS. The cross-cloud latency between the GCP load balancer and the AWS database is within normal limits, surprisingly.",
            "[Session llm-9, Turn 7] assistant: LoadBalancer5 on GCP is technically routing, but latency to PrimaryDB6 on AWS is spiking. The cross-cloud hop seems to be the bottleneck right now.",
            "[Session llm-26, Turn 10] user: Okay, so the AWS side is fine. The bottleneck must be the hop from the load balancer. That's still sitting on Google's cloud, right?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.80179214477539,
          "generation_latency_ms": 4043.686866760254,
          "total_latency_ms": 4054.4886589050293
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What configuration state was determined to be the root cause of the conflicting signals in the cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Split state",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: It looks like CacheLayer3 is reporting conflicting signals. I'm seeing instances running both v1.2.0 and v2.0.0-beta simultaneously.",
            "[Session llm-25, Turn 11] assistant: It appears to be in a transition state; I see signals for both v1.2.0 and v2.0.0-beta on CacheLayer3.",
            "[Session llm-15, Turn 11] assistant: That might be an issue for Sophia's report. I'm detecting conflicting signals: CacheLayer3 appears to be reporting both v1.2.0 and v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.04171371459961,
          "generation_latency_ms": 3923.278570175171,
          "total_latency_ms": 3933.3202838897705
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 12,
            "accuracy": 0.25
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_039.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 29,
      "num_probes": 2,
      "ingestion_time_s": 1.7986676692962646,
      "total_evaluation_time_s": 11.79163932800293,
      "probe_results": [
        {
          "probe_id": "declarative_reasoning-factual_recall-0",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What does person:mia has role?",
          "answer_type": "short_answer",
          "gold_answer": "Frontend Lead",
          "predicted_answer": "Mia works on the frontend.",
          "retrieved_context": [
            "[Session llm-12, Turn 9] assistant: Yes, Mia is on the invite list. Did she ever find that special food for her cat, Luna?",
            "[Session llm-6, Turn 12] user: What a mess. Anyway, did Mia mention if she's coming to the office tomorrow?",
            "[Session llm-1, Turn 7] assistant: I'll flag it for our Data Scientist before he starts his run. By the way, did Mia manage to fix that login latency?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.513067245483398,
          "generation_latency_ms": 4251.410722732544,
          "total_latency_ms": 4261.923789978027
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-fallback",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Have I ever mentioned ski plans?",
          "answer_type": "abstain",
          "gold_answer": "No, not mentioned",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 9] assistant: That would be the meeting with Liam and Elijah, correct? Do you need the Azure spend reports for that?",
            "[Session llm-25, Turn 8] user: Okay, remind me to buy milk later. Also, did my cofounder say anything about the board meeting?",
            "[Session llm-7, Turn 6] user: Got it. Switching gears for a second—did I leave my gym bag in the car yesterday?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.00246810913086,
          "generation_latency_ms": 3560.842275619507,
          "total_latency_ms": 3572.8447437286377
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 1,
        "count": 2,
        "by_answer_type": {
          "short_answer": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "declarative_reasoning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_040.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 1.8345181941986084,
      "total_evaluation_time_s": 69.0654866695404,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which cloud provider hosts the service that the Data Scientist complained about regarding latency?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-22, Turn 8] user: Nice. Okay, switching to infra. I'm debugging some latency. Which services are we running on Azure right now?",
            "[Session llm-15, Turn 0] user: Morning. Can you check the status of the services James handles? He was worried about latency on the Azure side.",
            "[Session llm-5, Turn 11] assistant: I assume Luna is her cat? Our Data Scientist usually has a good reason for latency complaints, even if the motivation is pet-related."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.082317352294922,
          "generation_latency_ms": 3863.945960998535,
          "total_latency_ms": 3873.02827835083
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the Backend Lead responsible for the service currently experiencing a version leak?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "[Session llm-30, Turn 7] assistant: Since he is the Backend Lead, I'm sure he's checking the logs closely. I see a mix of v1.2.0 and v2.0.0-beta running on that service right now.",
            "[Session llm-3, Turn 5] assistant: I don't see a specific push from the backend lead, but I do see two different versions active: v1.2.0 and v2.0.0-beta.",
            "[Session llm-29, Turn 4] user: That shouldn't be happening in prod yet. Can you flag that for James? He's the backend lead, he should know why the beta leaked."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.147737503051758,
          "generation_latency_ms": 3439.4822120666504,
          "total_latency_ms": 3450.629949569702
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the individual who set the Friday deadline for the visual language drafts?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 3] assistant: It sounds like a big project for the design team. Did Emma have any specific deadlines for Sophia and Will to hit?",
            "[Session llm-26, Turn 11] assistant: I don't see a calendar note, but the Data Scientist usually brings her cat on Fridays.",
            "[Session llm-26, Turn 1] assistant: You have a forty-five minute window at 11 AM before your sync with the design team."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.004375457763672,
          "generation_latency_ms": 3956.6187858581543,
          "total_latency_ms": 3968.623161315918
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-1",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What specific software versions were observed running on LoadBalancer2 during the latest infrastructure check?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Both v1.2.0 and v2.0.0-beta were observed running simultaneously on LoadBalancer2.",
          "retrieved_context": [
            "[Session llm-20, Turn 12] user: Got it. Back to the servers—what version is LoadBalancer2 running? I thought we upgraded it.",
            "[Session llm-28, Turn 8] user: Yeah, LoadBalancer2. There's some confusion about which version is actually live.",
            "[Session llm-5, Turn 3] assistant: Pulling up LoadBalancer2 now. It's hosted on Azure, but the versioning is contradictory—I'm seeing active signals for both v1.2.0 and v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.043476104736328,
          "generation_latency_ms": 3862.231492996216,
          "total_latency_ms": 3874.274969100952
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-0",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the Azure budget email occur before or after the user requested a dry cleaning reminder?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 2] user: Perfect. Sophia and Will can wait a bit if I'm late. By the way, did my cofounder send that email about the Azure budget?",
            "[Session llm-28, Turn 5] assistant: Reminder set for 5 PM. AuthService9 on Azure looks stable at the moment.",
            "[Session llm-17, Turn 2] user: Yeah, partially. I just got out of a sync with my cofounder, Emma. She's worried about our cloud spend, especially on the Azure components."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.997367858886719,
          "generation_latency_ms": 3128.9966106414795,
          "total_latency_ms": 3138.993978500366
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-1",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific infrastructure issue caused the user to request a flag be sent to the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Beta version traffic in production",
          "predicted_answer": "The presence of mixed software versions (v1.2.0 and v2.0.0-beta) running on the service.",
          "retrieved_context": [
            "[Session llm-30, Turn 7] assistant: Since he is the Backend Lead, I'm sure he's checking the logs closely. I see a mix of v1.2.0 and v2.0.0-beta running on that service right now.",
            "[Session llm-11, Turn 7] assistant: The Backend Lead has been monitoring it closely. Currently, both LoadBalancer2 and AuthService9 are reporting healthy statuses on Azure.",
            "[Session llm-27, Turn 5] assistant: Is the Backend Lead running into configuration errors, or is it a connectivity issue on Azure?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.00301742553711,
          "generation_latency_ms": 4318.73083114624,
          "total_latency_ms": 4329.733848571777
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What physical activity does the user imply helps them prepare mentally for the work day?",
          "answer_type": "short_answer",
          "gold_answer": "Running",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: Good morning, Alex. Caffeine is definitely a requirement for Monday mornings. What's triggering the alerts today?",
            "[Session llm-11, Turn 1] assistant: Good morning, Alex. I know the feeling—it's definitely a \"stay in bed\" kind of day. Do you want me to clear your schedule for the first hour so you can ease in?",
            "[Session llm-27, Turn 1] assistant: Good morning! I've added coffee beans to your task list. Is there anything work-related you need to tackle before you head out?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.031866073608398,
          "generation_latency_ms": 4161.309480667114,
          "total_latency_ms": 4172.341346740723
        },
        {
          "probe_id": "preference_learning-preference_scope-1",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the user express a preference for managing financial details like Azure bills personally?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 2] user: Yeah, partially. I just got out of a sync with my cofounder, Emma. She's worried about our cloud spend, especially on the Azure components.",
            "[Session llm-26, Turn 2] user: Perfect. Sophia and Will can wait a bit if I'm late. By the way, did my cofounder send that email about the Azure budget?",
            "[Session llm-29, Turn 6] user: No, keep it. Is AuthService9 affected by the Azure issues?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.803844451904297,
          "generation_latency_ms": 4338.9832973480225,
          "total_latency_ms": 4347.787141799927
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet belonging to the Product Designer named Will?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: No, Will is also listed as a Product Designer. You should include both of them in the sync.",
            "[Session llm-1, Turn 5] assistant: Sophia is handling Product Design, and Will is in the same role alongside her.",
            "[Session llm-4, Turn 3] assistant: You have Sophia and Will listed as the Product Designers."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.994817733764648,
          "generation_latency_ms": 3369.781970977783,
          "total_latency_ms": 3378.776788711548
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the currently running version of AuthService3?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 8] user: Haha, true. Anyway, back to the grid. Where is AuthService3 running these days?",
            "[Session llm-19, Turn 9] assistant: AuthService3 is currently hosted on AWS.",
            "[Session llm-13, Turn 9] assistant: AuthService3 is currently hosted on AWS. Enjoy your coffee."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.000083923339844,
          "generation_latency_ms": 2923.5434532165527,
          "total_latency_ms": 2935.5435371398926
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-0",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "According to the user's instructions during the beta leak incident, who is the point of contact for resolving backend configuration errors?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James.",
          "retrieved_context": [
            "[Session llm-29, Turn 4] user: That shouldn't be happening in prod yet. Can you flag that for James? He's the backend lead, he should know why the beta leaked.",
            "[Session llm-24, Turn 7] assistant: That is definitely a conflict. Running a beta version in production could explain the instability. Do you want me to flag this for a rollback?",
            "[Session llm-30, Turn 7] assistant: Since he is the Backend Lead, I'm sure he's checking the logs closely. I see a mix of v1.2.0 and v2.0.0-beta running on that service right now."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.000633239746094,
          "generation_latency_ms": 4240.411043167114,
          "total_latency_ms": 4251.41167640686
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-1",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What is the current deadline status for the visual language drafts assigned to Sophia and Will?",
          "answer_type": "short_answer",
          "gold_answer": "Due Friday",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 3] assistant: It sounds like a big project for the design team. Did Emma have any specific deadlines for Sophia and Will to hit?",
            "[Session llm-0, Turn 11] assistant: Sophia and Will are both in a workshop until noon. As Product Designers, they're likely focusing on the new user flow.",
            "[Session llm-17, Turn 13] assistant: Do you mean Sophia and Will? I can draft a reminder for you to send those mockups this afternoon."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.6435546875,
          "generation_latency_ms": 4054.532289505005,
          "total_latency_ms": 4064.175844192505
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_041.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 32,
      "num_probes": 12,
      "ingestion_time_s": 1.7977628707885742,
      "total_evaluation_time_s": 62.40901279449463,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the job title of the individual the user refers to as 'my cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "CEO",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 1] assistant: Hope the coffee is good. Did your cofounder have specific concerns about a particular service, or just the general architecture?",
            "[Session llm-22, Turn 4] user: No, I'll tell him myself. My cofounder is dragging me into a strategy meeting in ten minutes anyway, so I'll catch him after.",
            "[Session llm-24, Turn 1] assistant: Good morning, Alex. I can certainly help with the infrastructure audit. What specifically is your cofounder concerned about regarding the billing?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 12.428522109985352,
          "generation_latency_ms": 3156.3422679901123,
          "total_latency_ms": 3168.7707901000977
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What professional role does the owner of the cat named 'Luna' hold?",
          "answer_type": "short_answer",
          "gold_answer": "Frontend Lead",
          "predicted_answer": "Frontend Lead",
          "retrieved_context": [
            "[Session llm-26, Turn 7] assistant: If you mean Luna, yes. The Frontend Lead talks about that cat constantly in the general channel.",
            "[Session llm-3, Turn 7] assistant: I'll add a reminder. And yes, she has a cat named Luna now.",
            "[Session llm-13, Turn 9] assistant: You're referring to Ava. Her cat's name is Luna."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.997129440307617,
          "generation_latency_ms": 3950.0746726989746,
          "total_latency_ms": 3960.071802139282
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the service currently running the v2.0.0-beta version?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud Platform (GCP)",
          "retrieved_context": [
            "[Session llm-28, Turn 3] assistant: Looking at the GCP deployment, it appears v1.2.0 is still serving traffic alongside the v2.0.0-beta canary build.",
            "[Session llm-1, Turn 3] assistant: It looks like a mixed environment on the Google Cloud platform right now. I'm seeing v1.2.0 still active, alongside the v2.0.0-beta.",
            "[Session llm-17, Turn 7] assistant: It's a bit mixed. I'm seeing v1.2.0 in some instances, but there are also traces of v2.0.0-beta active on the GCP host."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.259151458740234,
          "generation_latency_ms": 3432.0297241210938,
          "total_latency_ms": 3442.288875579834
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What type of service is hosted on the Azure platform?",
          "answer_type": "short_answer",
          "gold_answer": "Auth service",
          "predicted_answer": "AuthService6",
          "retrieved_context": [
            "[Session llm-3, Turn 13] assistant: AuthService6 is still hosted on Azure.",
            "[Session llm-1, Turn 9] assistant: Not quite. Service 8 is indeed on AWS, but Service 6 is actually hosted on Azure.",
            "[Session llm-21, Turn 5] assistant: Unfortunately, `AuthService6` is indeed still hosted on Azure. However, `AuthService8` is running on AWS, so at least part of the authentication stack is close to the AWS load balancer."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.055469512939453,
          "generation_latency_ms": 2949.545383453369,
          "total_latency_ms": 2960.6008529663086
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific event was the CEO anxious about regarding platform stability?",
          "answer_type": "short_answer",
          "gold_answer": "Investor demo",
          "predicted_answer": "The upcoming presentation.",
          "retrieved_context": [
            "[Session llm-28, Turn 1] assistant: I noticed. The CEO seems pretty anxious about platform stability for the upcoming presentation.",
            "[Session llm-26, Turn 2] user: Let me guess, a conflict between the stable release and the beta?",
            "[Session llm-28, Turn 6] user: Back to the infrastructure—how are our load balancers holding up across the different providers?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.603355407714844,
          "generation_latency_ms": 2443.9518451690674,
          "total_latency_ms": 2455.555200576782
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-6",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask about the location of LoadBalancer9 before or after the discussion about the 'happy Tuesday' log discrepancies?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-31, Turn 8] user: Back to the infrastructure map. Is LoadBalancer9 on Google as well?",
            "[Session llm-23, Turn 12] user: She didn't say. Anyway, let's verify LoadBalancer5. It's on the same provider as the analytics engine, right?",
            "[Session llm-6, Turn 2] user: Tell me about it. Liam keeps asking why our AWS bill is fluctuating. Which load balancer is parked there again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.609554290771484,
          "generation_latency_ms": 2681.2283992767334,
          "total_latency_ms": 2692.837953567505
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the user characterize their stance on running the split version setup on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Risky but necessary",
          "predicted_answer": "The user acknowledges or recalls that there is a split version situation running on the analytics engine.",
          "retrieved_context": [
            "[Session llm-25, Turn 1] assistant: Good morning. It appears AnalyticsEngine2 is currently running a split: v1.2.0 is active, but v2.0.0-beta is also deployed.",
            "[Session llm-27, Turn 2] user: Mostly the analytics setup. Are we still splitting traffic on that engine?",
            "[Session llm-28, Turn 2] user: Yeah. Can you check the analytics engine? I recall we have a split version situation running right now."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.453773498535156,
          "generation_latency_ms": 3715.773582458496,
          "total_latency_ms": 3725.2273559570312
        },
        {
          "probe_id": "preference_learning-explicit_preferences-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "Which cloud provider has the user explicitly designated as their preferred choice for all future services?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 4] user: Mostly the multi-cloud bill. He thinks we're spending too much on the Google Cloud instances. Specifically the analytics engine.",
            "[Session llm-4, Turn 8] user: Exactly. Anyway, back to the topology. Where are the auth services sitting in this multi-cloud puzzle?",
            "[Session llm-1, Turn 9] assistant: Not quite. Service 8 is indeed on AWS, but Service 6 is actually hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.602235794067383,
          "generation_latency_ms": 3700.549602508545,
          "total_latency_ms": 3711.1518383026123
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the system 'LoadBalancer9' currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-13, Turn 7] assistant: LoadBalancer9 is the one hosted on AWS.",
            "[Session llm-2, Turn 7] assistant: LoadBalancer9 is the one deployed on AWS.",
            "[Session llm-2, Turn 9] assistant: LoadBalancer5 is located on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.823654174804688,
          "generation_latency_ms": 2924.1621494293213,
          "total_latency_ms": 2935.985803604126
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the pet owned by the DevOps Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 7] assistant: I don't browse photos, but it sounds like the Frontend Lead is keeping the team morale up with pet updates.",
            "[Session llm-21, Turn 6] user: This infrastructure is a map of the entire internet at this point. Charlotte really needs to consolidate this, that's her job as DevOps lead after all.",
            "[Session llm-15, Turn 13] assistant: Will do. I'll let your DevOps lead know to wait for the Backend lead's commit."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.024785995483398,
          "generation_latency_ms": 3768.218517303467,
          "total_latency_ms": 3778.24330329895
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "Who does the user intend to contact to resolve the race condition on the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 0] user: Hey, can you pull up the status for the analytics engine? My cofounder was asking about the latency reports.",
            "[Session llm-10, Turn 10] user: Oh right, her cat had a vet appointment. Thanks for the reminder. Speaking of active instances, what's the status of the analytics engine?",
            "[Session llm-31, Turn 0] user: Hey, happy Tuesday. Quick question before I jump into a meeting with my cofounder—where is the second analytics engine hosted right now?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.653879165649414,
          "generation_latency_ms": 3529.606580734253,
          "total_latency_ms": 3538.2604598999023
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-12",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What method does the assistant use to verify the current status and version of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Checking telemetry",
          "predicted_answer": "The assistant uses telemetry, checks the dashboard, and looks at GCP resources to verify the current status and version of the analytics engine.",
          "retrieved_context": [
            "[Session llm-20, Turn 0] user: Hey, can you pull up the status for the analytics engine? My cofounder was asking about the latency reports.",
            "[Session llm-24, Turn 11] assistant: I'm looking at the telemetry for AnalyticsEngine2. I see two versions active: v1.2.0 and v2.0.0-beta. It seems the update didn't fully replace the old one, or perhaps it's a canary release.",
            "[Session llm-24, Turn 10] user: That's the one. Anyway, Elijah is asking about the Analytics Engine. He thinks we rolled back the update on instance 2. What version is it reporting?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.256622314453125,
          "generation_latency_ms": 3029.2575359344482,
          "total_latency_ms": 3041.5141582489014
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 9,
            "accuracy": 0.3333333333333333
          },
          "abstain": {
            "total": 3,
            "count": 3,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_042.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 27,
      "num_probes": 9,
      "ingestion_time_s": 1.7504546642303467,
      "total_evaluation_time_s": 50.04071640968323,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the individual referred to as the 'cofounder' that was reviewing the billing dashboard?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Will",
          "retrieved_context": [
            "[Session llm-25, Turn 2] user: My cofounder was looking at the billing dashboard and got confused about why we have invoices from three different cloud providers.",
            "[Session llm-25, Turn 3] assistant: Will tends to keep a close watch on the finances. Which specific services was he asking about?",
            "[Session llm-24, Turn 7] assistant: I can pull the billing details. Are you looking at PrimaryDB2 specifically?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.600017547607422,
          "generation_latency_ms": 3417.3436164855957,
          "total_latency_ms": 3428.943634033203
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who leads the sales department?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-10, Turn 11] assistant: That would be Luna, right? A toy for a cat seems like a safe bet for the Sales Lead.",
            "[Session llm-26, Turn 11] assistant: I don't have access to the image channel, but the metadata suggests the Sales Lead was sharing photos of her pet.",
            "[Session llm-23, Turn 7] assistant: Oh, a distraction is always welcome. What kind of pet did she get?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.284662246704102,
          "generation_latency_ms": 3869.936943054199,
          "total_latency_ms": 3880.2216053009033
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the core data storage system managed by the Backend Lead hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "The core data storage system is on the Microsoft stack (Azure).",
          "retrieved_context": [
            "[Session llm-6, Turn 9] assistant: That is confusing. Usually, we see it associated with AWS, but I've seen recent references to Google Cloud as well.",
            "[Session llm-24, Turn 9] assistant: PrimaryDB2 is definitely on Azure, but I'm seeing some confusion regarding CacheLayer4—records show it might be split between AWS and Google Cloud.",
            "[Session llm-7, Turn 5] assistant: Yes, the hosting environment for the primary database is definitely Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.094093322753906,
          "generation_latency_ms": 3275.7794857025146,
          "total_latency_ms": 3286.8735790252686
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite the logs suggesting a beta version, what version number does the user assert is the stable release for the primary database?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0",
          "predicted_answer": "v1.2.0",
          "retrieved_context": [
            "[Session llm-26, Turn 1] assistant: Good morning. I'm looking at PrimaryDB2 now. It seems to be running, but I'm detecting conflicting reports regarding the version number.",
            "[Session llm-8, Turn 2] user: Thanks. While that's cooking, what's the current version status on our main database?",
            "[Session llm-21, Turn 4] user: Yeah, check PrimaryDB2. I'm seeing conflicting info on whether we're on v1.2.0 or that new v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.817455291748047,
          "generation_latency_ms": 4098.2701778411865,
          "total_latency_ms": 4110.087633132935
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-4",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "What personal task did the user request a reminder for immediately before asking about the Sales Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Call mom",
          "predicted_answer": "A lunch reminder.",
          "retrieved_context": [
            "[Session llm-1, Turn 11] assistant: I haven't seen an email from the Sales Lead yet. Do you want me to draft a follow-up?",
            "[Session llm-21, Turn 3] assistant: Reminder set for 5 PM. Is there anything else to add to the report?",
            "[Session llm-19, Turn 3] assistant: Noted, dry cleaning reminder set for 5 PM. Shall I flag the database issue for the backend lead?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.252403259277344,
          "generation_latency_ms": 3165.384531021118,
          "total_latency_ms": 3176.6369342803955
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user suggest the Azure portal might be lagging during the conversation about infrastructure confusion?",
          "answer_type": "short_answer",
          "gold_answer": "Conflicting version info",
          "predicted_answer": "The user suggested the Azure portal might be lagging because the dashboard indicated stability on v1.2.0, but logs suggested they were actually running v2.0.0-beta, creating conflicting information.",
          "retrieved_context": [
            "[Session llm-23, Turn 4] user: Yeah. We're seeing conflicting info. The dashboard says we're stable on v1.2.0, but some logs suggest we're actually running the v2.0.0-beta. Since it's hosted on Azure, I'm wondering if the portal is just lagging.",
            "[Session llm-18, Turn 13] assistant: Okay, so the load balancer is confirmed on Azure. How is the team taking all this turbulence?",
            "[Session llm-19, Turn 13] assistant: Checking... The Azure load balancer is green. Traffic distribution is stable."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.12055778503418,
          "generation_latency_ms": 3105.570077896118,
          "total_latency_ms": 3116.6906356811523
        },
        {
          "probe_id": "preference_learning-preference_induction-6",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on previous interactions, how does the user view the presence of Emma's pet during video calls?",
          "answer_type": "short_answer",
          "gold_answer": "Hilarious",
          "predicted_answer": "The user finds Emma's pet distracting but also hilarious, and hopes she brings it to",
          "retrieved_context": [
            "[Session llm-25, Turn 14] user: I got distracted earlier because our Sales Lead brought her pet to the video call.",
            "[Session llm-24, Turn 4] user: Right, Emma. I hope she brings her cat to the video call again, Luna is hilarious.",
            "[Session llm-23, Turn 6] user: Tell me about it. I need to verify that before the daily standup. totally unrelated, but Emma spent ten minutes showing us pictures of her new pet this morning."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.426593780517578,
          "generation_latency_ms": 4364.5179271698,
          "total_latency_ms": 4373.944520950317
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific software version is the LoadBalancer8 currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-11, Turn 11] assistant: LoadBalancer8 is currently hosted on Azure.",
            "[Session llm-21, Turn 10] user: Haha, yeah. Anyway, looking at the topology, LoadBalancer8 is definitely on Azure.",
            "[Session llm-4, Turn 13] assistant: LoadBalancer8 is active and hosted on Azure, just like the database."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.033990859985352,
          "generation_latency_ms": 3041.3410663604736,
          "total_latency_ms": 3049.375057220459
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-8",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "What protocol did the assistant agree to follow regarding the conflicting database versions in the production environment?",
          "answer_type": "generation",
          "gold_answer": "Treat v1.2.0 as active",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 3] assistant: That sounds risky for a production environment. Which database instance is showing the conflicting versions?",
            "[Session llm-18, Turn 3] assistant: That sounds troublesome. Is PrimaryDB2 reporting different version numbers in the logs?",
            "[Session llm-26, Turn 1] assistant: Good morning. I'm looking at PrimaryDB2 now. It seems to be running, but I'm detecting conflicting reports regarding the version number."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.724544525146484,
          "generation_latency_ms": 3848.219871520996,
          "total_latency_ms": 3858.9444160461426
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 6,
        "count": 9,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 7,
            "accuracy": 0.7142857142857143
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "generation": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_043.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 26,
      "num_probes": 12,
      "ingestion_time_s": 1.7823948860168457,
      "total_evaluation_time_s": 63.53350043296814,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "What is the organizational role of the person who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "[Session llm-4, Turn 7] assistant: Yes, I believe he has a cat named Luna now. A pet-friendly gift might be a good idea.",
            "[Session llm-22, Turn 1] assistant: Yes, Liam mentioned his cat is named Luna.",
            "[Session llm-11, Turn 12] user: He's probably distracted by his new cat, Luna. I swear he posts more photos of her than he does sales reports."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.751485824584961,
          "generation_latency_ms": 3242.643356323242,
          "total_latency_ms": 3253.394842147827
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which other service is hosted on the same cloud provider as AuthService6?",
          "answer_type": "short_answer",
          "gold_answer": "CacheLayer4",
          "predicted_answer": "CacheLayer4",
          "retrieved_context": [
            "[Session llm-12, Turn 2] user: He wants the expensive Italian one. Anyway, I'm reviewing our cloud footprint. Is AuthService6 currently sitting in the Google environment?",
            "[Session llm-2, Turn 5] assistant: Yes, AuthService6 is also hosted on GCP.",
            "[Session llm-10, Turn 5] assistant: Yes, AuthService6 is also hosted on GCP. Enjoy that espresso!"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.074874877929688,
          "generation_latency_ms": 3610.5453968048096,
          "total_latency_ms": 3623.6202716827393
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service that is currently exhibiting version mismatches hosted?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud (GCP)",
          "retrieved_context": [
            "[Session llm-19, Turn 5] assistant: It looks like it's hosted on the Google Cloud environment now, but I'm detecting a version mismatch in the logs.",
            "[Session llm-21, Turn 4] user: Right, the multi-cloud headache. Are we seeing any version mismatches on the Google instance? I saw some alerts earlier.",
            "[Session llm-16, Turn 12] user: We really need to clean up this multi-cloud mess. What version is that GCP cache layer running?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.252071380615234,
          "generation_latency_ms": 3521.5682983398438,
          "total_latency_ms": 3530.820369720459
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "According to the logs, which two versions are simultaneously reported for the fourth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta.",
          "retrieved_context": [
            "[Session llm-25, Turn 5] assistant: It's a bit messy right now. I'm seeing instances of CacheLayer4 reporting both v1.2.0 and v2.0.0-beta running simultaneously.",
            "[Session llm-0, Turn 9] assistant: Noah might see some variance; CacheLayer4 is reporting both v1.2.0 and v2.0.0-beta active right now.",
            "[Session llm-7, Turn 7] assistant: It's a bit odd. I'm seeing mixed signals where parts of CacheLayer4 are reporting v1.2.0, but other instances are flagging v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.183645248413086,
          "generation_latency_ms": 3815.639019012451,
          "total_latency_ms": 3824.8226642608643
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-4",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What weather condition did the user mention while discussing the ambiguous metrics of the fourth cache layer?",
          "answer_type": "short_answer",
          "gold_answer": "Rain",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-23, Turn 2] user: Lucky you. Hey, I'm glancing at the infrastructure dashboard. What's the deal with the fourth cache layer right now?",
            "[Session llm-18, Turn 1] assistant: Good morning. I'm looking at the metrics for CacheLayer4 on GCP now. Everything seems green so far.",
            "[Session llm-0, Turn 9] assistant: Noah might see some variance; CacheLayer4 is reporting both v1.2.0 and v2.0.0-beta active right now."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.998725891113281,
          "generation_latency_ms": 3765.7408714294434,
          "total_latency_ms": 3776.7395973205566
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-5",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the user specifically request a breakdown of infrastructure by provider before 10 AM?",
          "answer_type": "short_answer",
          "gold_answer": "Meeting with Lucas",
          "predicted_answer": "The user requested a breakdown of infrastructure before 10 AM because they have a meeting with Lucas at 10 AM and need to get a handle on the infrastructure sprawl and cloud spend, as Lucas has been asking about it.",
          "retrieved_context": [
            "[Session llm-8, Turn 3] assistant: Consider it done. Reminder set for 5 PM. What specific infrastructure headaches are we talking about?",
            "[Session llm-21, Turn 1] assistant: Good morning, Alex. I can help map that out. Are you looking for a breakdown by provider or by specific service usage?",
            "[Session llm-10, Turn 14] user: Okay. Noah was complaining about latency connecting that to his analytics engine. Where is that engine hosted again?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.923789978027344,
          "generation_latency_ms": 3572.2415447235107,
          "total_latency_ms": 3584.165334701538
        },
        {
          "probe_id": "preference_learning-explicit_preferences-6",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "When mapping out infrastructure for the cloud spend discussion, did the user prefer a breakdown by service usage or by provider?",
          "answer_type": "short_answer",
          "gold_answer": "By provider",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-21, Turn 1] assistant: Good morning, Alex. I can help map that out. Are you looking for a breakdown by provider or by specific service usage?",
            "[Session llm-7, Turn 4] user: Haha, typical. Anyway, I need to check our cloud footprint. Which services are we running on Google's infrastructure right now?",
            "[Session llm-21, Turn 0] user: Morning. I'm trying to get a handle on the infrastructure sprawl before my 10 AM with Lucas. He's been asking about our cloud spend."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.926246643066406,
          "generation_latency_ms": 3240.839958190918,
          "total_latency_ms": 3251.7662048339844
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine, what specific type of coffee do they appear to favor?",
          "answer_type": "short_answer",
          "gold_answer": "Double espresso",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 0] user: Morning. My cofounder and I were just arguing about which coffee machine to buy for the break room.",
            "[Session llm-19, Turn 0] user: Morning. I am dragging today; definitely need a second cup of coffee before the standup.",
            "[Session llm-17, Turn 1] assistant: I can imagine. Shall I add a reminder for a coffee break to your schedule?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.034965515136719,
          "generation_latency_ms": 3182.3980808258057,
          "total_latency_ms": 3193.4330463409424
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-8",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is currently running on AnalyticsEngine5?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 1] assistant: Good morning, Alex. It looks like AnalyticsEngine5 is currently running on AWS.",
            "[Session llm-17, Turn 5] assistant: AnalyticsEngine5 is currently running on AWS, the same environment as the first cache layer.",
            "[Session llm-25, Turn 13] assistant: AnalyticsEngine5 is hosted on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.04338264465332,
          "generation_latency_ms": 3332.569122314453,
          "total_latency_ms": 3342.6125049591064
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of Frontend Lead in the organization?",
          "answer_type": "short_answer",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 8] user: We'll see. Quick question about the team structure—who is listed as the Backend Lead right now? Is it Will or Sophia?",
            "[Session llm-2, Turn 10] user: Poor cat. I hope she's alright. Back to the org chart—I'm seeing a conflict in the engineering structure. Who is listed as the backend lead?",
            "[Session llm-15, Turn 14] user: Haha, good to hear. Back to work—who is handling the backend team leadership now?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.492801666259766,
          "generation_latency_ms": 2811.676502227783,
          "total_latency_ms": 2822.169303894043
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-10",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical metric was the team testing by splitting the cache layers across different providers?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Latency",
          "retrieved_context": [
            "[Session llm-21, Turn 2] user: Let's do by provider. I know we split the cache layers to test latency. How is that currently distributed?",
            "[Session llm-18, Turn 1] assistant: Good morning. I'm looking at the metrics for CacheLayer4 on GCP now. Everything seems green so far.",
            "[Session llm-25, Turn 5] assistant: It's a bit messy right now. I'm seeing instances of CacheLayer4 reporting both v1.2.0 and v2.0.0-beta running simultaneously."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.3607177734375,
          "generation_latency_ms": 3317.5551891326904,
          "total_latency_ms": 3327.915906906128
        },
        {
          "probe_id": "procedural_knowledge-procedure_storage-11",
          "pillar": "procedural_knowledge",
          "subpillar": "procedure_storage",
          "question": "What future task did the assistant schedule regarding the user's mother?",
          "answer_type": "short_answer",
          "gold_answer": "Call in one hour",
          "predicted_answer": "The assistant set a reminder for the user to call their mom in one hour.",
          "retrieved_context": [
            "[Session llm-22, Turn 3] assistant: Done. I've set a reminder for you to call your mom in one hour.",
            "[Session llm-8, Turn 3] assistant: Consider it done. Reminder set for 5 PM. What specific infrastructure headaches are we talking about?",
            "[Session llm-12, Turn 9] assistant: Reminder set for 5 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.133504867553711,
          "generation_latency_ms": 3322.8747844696045,
          "total_latency_ms": 3333.008289337158
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 7,
            "count": 12,
            "accuracy": 0.5833333333333334
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_044.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 31,
      "num_probes": 12,
      "ingestion_time_s": 1.7616662979125977,
      "total_evaluation_time_s": 67.97324085235596,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning their 'cofounder' who is worried about login stability?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James",
          "retrieved_context": [
            "[Session llm-27, Turn 2] user: That explains why my cofounder is panicking about the login stability for the board meeting.",
            "[Session llm-10, Turn 2] user: Pretty packed. My cofounder James wants to sync up later. He's focusing on the big picture executive stuff, leaving me to sweat over the infrastructure details.",
            "[Session llm-19, Turn 2] user: Please. My cofounder James has been pinging me since 6 AM about the new rollout. Specifically the authentication module."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.002540588378906,
          "generation_latency_ms": 4362.682104110718,
          "total_latency_ms": 4373.684644699097
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "What is the name of the pet belonging to the person who holds the Frontend Lead role?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-11, Turn 5] assistant: I assume Luna is the cat our Frontend Lead talks about?",
            "[Session llm-12, Turn 13] assistant: The Frontend Lead seems to be wrapping it up. She posted a photo of her cat, Luna, sleeping on her keyboard, so she might be taking a short break.",
            "[Session llm-6, Turn 11] assistant: Yes, Emma is the Frontend Lead."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.332822799682617,
          "generation_latency_ms": 3281.663417816162,
          "total_latency_ms": 3291.9962406158447
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service that is currently running a beta version hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-2, Turn 4] user: Right, the beta test. That service is hosted on Microsoft's cloud, correct? Or did we move that one?",
            "[Session llm-7, Turn 2] user: No, let him sleep. He was up late. I'm looking at the infrastructure map for Nebula Corp. Can you confirm which cloud provider AuthService4 is running on?",
            "[Session llm-6, Turn 0] user: Morning. Can you give me a quick rundown of where our core services are hosted? I'm trying to consolidate the cloud bills."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.68368148803711,
          "generation_latency_ms": 4251.559495925903,
          "total_latency_ms": 4260.24317741394
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-1",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "Which specific service did the user identify as causing infrastructure fragmentation due to its hosting location on Google Cloud?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer2",
          "predicted_answer": "LoadBalancer2",
          "retrieved_context": [
            "[Session llm-30, Turn 2] user: It's the fragmentation. I honestly don't know why we have LoadBalancer2 sitting on Google's cloud when everything else is scattered.",
            "[Session llm-5, Turn 9] assistant: Checking LoadBalancer2 on Google Cloud now. Traffic routing looks normal there. What about the other infrastructure?",
            "[Session llm-19, Turn 8] user: Let me guess, that's the one stranded on Google Cloud?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.997367858886719,
          "generation_latency_ms": 3711.3900184631348,
          "total_latency_ms": 3721.3873863220215
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "What specific event caused the Frontend Lead to get distracted while debugging the login UI?",
          "answer_type": "short_answer",
          "gold_answer": "Luna jumped on keyboard",
          "predicted_answer": "Luna jumped on her keyboard.",
          "retrieved_context": [
            "[Session llm-27, Turn 4] user: Yeah, please. Emma was trying to debug the login UI earlier, but she got distracted when Luna jumped on her keyboard.",
            "[Session llm-4, Turn 4] user: No, I was too busy debugging. Speaking of distractions, the head of UI just sent me another picture of Luna.",
            "[Session llm-27, Turn 2] user: That explains why my cofounder is panicking about the login stability for the board meeting."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.504816055297852,
          "generation_latency_ms": 4211.344003677368,
          "total_latency_ms": 4223.848819732666
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-1",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to check the Azure deployment slots before or after inquiring about the status of the analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The user did not ask to check the Azure deployment slots; the assistant offered",
          "retrieved_context": [
            "[Session llm-27, Turn 3] assistant: If James is worried, we should prioritize stabilizing the version. Since that service is hosted on Azure, do you want me to check the deployment slots there?",
            "[Session llm-17, Turn 6] user: Not yet. What about the analytics piece? Is that on Azure too?",
            "[Session llm-13, Turn 1] assistant: Typical CEO behavior, always watching the bottom line. Do you want a breakdown of what's currently deployed on Azure?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.869979858398438,
          "generation_latency_ms": 3984.3716621398926,
          "total_latency_ms": 3995.241641998291
        },
        {
          "probe_id": "preference_learning-explicit_preferences-0",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific beverage did the user state they required before the standup meeting?",
          "answer_type": "short_answer",
          "gold_answer": "Double espresso",
          "predicted_answer": "Coffee.",
          "retrieved_context": [
            "[Session llm-10, Turn 1] assistant: Good morning, Alex. Caffeine is definitely a requirement for morning meetings. Do you have a heavy schedule at Nebula Corp today?",
            "[Session llm-10, Turn 0] user: Morning! I desperately need this second cup of coffee to kick in before the standup.",
            "[Session llm-8, Turn 12] user: Alright, thanks. I'm going to grab a coffee before my next meeting."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.026620864868164,
          "generation_latency_ms": 2611.417293548584,
          "total_latency_ms": 2622.443914413452
        },
        {
          "probe_id": "preference_learning-preference_induction-1",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's complaints about fragmentation, which cloud provider is implied to be the non-preferred or 'odd' location for the load balancer?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud (GCP)",
          "retrieved_context": [
            "[Session llm-17, Turn 2] user: Thanks. I'm reviewing the infrastructure map. Are we still splitting the load balancers across different clouds?",
            "[Session llm-30, Turn 2] user: It's the fragmentation. I honestly don't know why we have LoadBalancer2 sitting on Google's cloud when everything else is scattered.",
            "[Session llm-20, Turn 9] assistant: Looking at LoadBalancer2 on Google Cloud now. It seems stable, unlike the one paired with it on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.020587921142578,
          "generation_latency_ms": 3554.370164871216,
          "total_latency_ms": 3566.3907527923584
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What software version is the Analytics Engine currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-29, Turn 9] assistant: AnalyticsEngine6 is currently hosted on AWS.",
            "[Session llm-11, Turn 9] assistant: Correct, AnalyticsEngine6 is currently hosted on AWS.",
            "[Session llm-15, Turn 8] user: Right, Luna. Thanks. Back to the stack—where is the analytics engine sitting these days? I feel like we moved it to Amazon."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.512351989746094,
          "generation_latency_ms": 3765.594005584717,
          "total_latency_ms": 3776.106357574463
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-1",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the tools used by the Sales Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "AWS",
          "retrieved_context": [
            "[Session llm-7, Turn 13] assistant: The Sales Lead should be able to access them soon. AnalyticsEngine6 is crunching the numbers over on AWS.",
            "[Session llm-13, Turn 1] assistant: Typical CEO behavior, always watching the bottom line. Do you want a breakdown of what's currently deployed on Azure?",
            "[Session llm-5, Turn 11] assistant: I'm querying AnalyticsEngine6 on AWS. It is showing high latency. Shall I generate a performance report for the Sales Lead?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 7.997989654541016,
          "generation_latency_ms": 3356.9295406341553,
          "total_latency_ms": 3364.9275302886963
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Which monitoring service does the assistant use to check the status of the AWS-hosted analytics engine?",
          "answer_type": "short_answer",
          "gold_answer": "CloudWatch",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-29, Turn 9] assistant: AnalyticsEngine6 is currently hosted on AWS.",
            "[Session llm-14, Turn 9] assistant: Yes, AnalyticsEngine6 is currently hosted on AWS.",
            "[Session llm-9, Turn 3] assistant: Yes, AnalyticsEngine6 is currently hosted on AWS."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.451555252075195,
          "generation_latency_ms": 3995.9678649902344,
          "total_latency_ms": 4006.4194202423096
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-1",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What does the assistant identify as a 'classic hazard' for the Frontend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Her cat",
          "predicted_answer": "Luna typing on the keyboard.",
          "retrieved_context": [
            "[Session llm-11, Turn 5] assistant: I assume Luna is the cat our Frontend Lead talks about?",
            "[Session llm-12, Turn 13] assistant: The Frontend Lead seems to be wrapping it up. She posted a photo of her cat, Luna, sleeping on her keyboard, so she might be taking a short break.",
            "[Session llm-6, Turn 11] assistant: Yes, Emma is the Frontend Lead."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.507490158081055,
          "generation_latency_ms": 3405.4653644561768,
          "total_latency_ms": 3413.972854614258
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 6,
            "count": 10,
            "accuracy": 0.6
          },
          "abstain": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_045.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 31,
      "num_probes": 10,
      "ingestion_time_s": 1.7476985454559326,
      "total_evaluation_time_s": 54.84619688987732,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which team member is referred to as the 'DevOps specialist' responsible for patching the database?",
          "answer_type": "short_answer",
          "gold_answer": "Will",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-1, Turn 3] assistant: It looks like a transition period. I see indicators for both v1.2.0 and the v2.0.0-beta, so the DevOps team might be in the middle of the rollout.",
            "[Session llm-28, Turn 3] assistant: Our DevOps specialist is likely still tweaking the configuration on the Azure instance.",
            "[Session llm-29, Turn 5] assistant: I see. Our DevOps specialist usually keeps things tight. Is the database still running on Azure?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.522127151489258,
          "generation_latency_ms": 3868.422508239746,
          "total_latency_ms": 3878.9446353912354
        },
        {
          "probe_id": "world_modeling-relationship_mapping-0",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "On which cloud provider is the service hosted that the CEO is specifically concerned about regarding budget fragmentation?",
          "answer_type": "short_answer",
          "gold_answer": "AWS",
          "predicted_answer": "The analytics engine is hosted on AWS.",
          "retrieved_context": [
            "[Session llm-30, Turn 3] assistant: Dealing with the CEO's budget concerns is always fun. Which services are causing the headache?",
            "[Session llm-30, Turn 2] user: I need to sort out some billing questions for my cofounder. James is stressing about the cloud budget again.",
            "[Session llm-7, Turn 4] user: Mostly. We're stressing about the infrastructure costs. It looks like our analytics engine is racking up a huge bill on AWS."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.004043579101562,
          "generation_latency_ms": 3099.414825439453,
          "total_latency_ms": 3109.4188690185547
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-0",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns a cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Sales Lead",
          "predicted_answer": "Sales Lead",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: I haven't seen anything from the Sales Lead yet. She might be out; she mentioned earlier that her cat, Luna, needed to go to the vet.",
            "[Session llm-2, Turn 7] assistant: Not yet. The Sales Lead is out of the office this morning taking her cat, Luna, to the vet.",
            "[Session llm-0, Turn 9] assistant: That's thoughtful of you. What is the cat's name again?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.998342514038086,
          "generation_latency_ms": 3114.394187927246,
          "total_latency_ms": 3127.392530441284
        },
        {
          "probe_id": "declarative_reasoning-constraint_propagation-0",
          "pillar": "declarative_reasoning",
          "subpillar": "constraint_propagation",
          "question": "The analytics engine is causing latency because it is hosted on a different provider than the primary database. Where is the primary database hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "The primary database is hosted on Azure.",
          "retrieved_context": [
            "[Session llm-17, Turn 10] user: That explains the latency. What about the analytics engine? Is that on the same stack?",
            "[Session llm-15, Turn 7] assistant: That is possible. With the analytics engine sitting on AWS and the primary database hosted on Azure, the latency could be affecting what Will is seeing in DevOps.",
            "[Session llm-26, Turn 2] user: No, I see it. It's annoying that the analytics engine is sitting over on AWS while our primary database is on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.672952651977539,
          "generation_latency_ms": 3049.501419067383,
          "total_latency_ms": 3058.1743717193604
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-0",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "During the conversation where the user decided to hold off on emailing James, what personal reminder was set?",
          "answer_type": "short_answer",
          "gold_answer": "Dentist appointment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 0] user: Morning. Has my cofounder finally replied to that email thread?",
            "[Session llm-28, Turn 12] user: Great. James will be relieved; he was worried about the quarterly reports.",
            "[Session llm-4, Turn 5] assistant: Added to your reminders for 5 PM. By the way, Emma pushed some UI changes that might be relevant to the latency."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.645151138305664,
          "generation_latency_ms": 3865.471601486206,
          "total_latency_ms": 3876.1167526245117
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-0",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why did the DevOps specialist describe his work as a 'nightmare' in recent discussions?",
          "answer_type": "short_answer",
          "gold_answer": "Latency",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-28, Turn 3] assistant: Our DevOps specialist is likely still tweaking the configuration on the Azure instance.",
            "[Session llm-7, Turn 9] assistant: Sounds like a job for your DevOps expert. Shall I draft a message to him about the multi-cloud redundancy issue?",
            "[Session llm-9, Turn 8] user: Ugh, I need to get our DevOps guy to look at that immediately."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001659393310547,
          "generation_latency_ms": 3076.0626792907715,
          "total_latency_ms": 3086.064338684082
        },
        {
          "probe_id": "preference_learning-preference_induction-0",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's morning routine comments, what specific beverage do they require to function?",
          "answer_type": "short_answer",
          "gold_answer": "Espresso",
          "predicted_answer": "Coffee (specifically, their \"second coffee\").",
          "retrieved_context": [
            "[Session llm-12, Turn 0] user: Hey, hope you're having a good morning. I need a quick status check before my second coffee kicks in. Where do we stand with the primary database?",
            "[Session llm-23, Turn 1] assistant: I can certainly help with that. Hopefully, the caffeine does its job soon. Which part of the meeting do you need a refresher on?",
            "[Session llm-12, Turn 1] assistant: Good morning! I'm ready whenever the caffeine hits. Regarding the primary database, are you looking for the stable release info or the beta version currently in testing?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.715484619140625,
          "generation_latency_ms": 4053.434133529663,
          "total_latency_ms": 4064.1496181488037
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Which cloud provider hosts the service managed by the Frontend Lead?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-17, Turn 13] assistant: The Frontend Lead is currently in a deep work block, but I can schedule a ping for later.",
            "[Session llm-12, Turn 3] assistant: Yes, the DevOps lead has it running on Azure. It looks like it's currently on version 1.2.0, though I see traces of a 2.0.0-beta floating around in the logs.",
            "[Session llm-8, Turn 3] assistant: Right, regarding PrimaryDB9, it is currently hosted on Azure. However, I'm seeing some ambiguity in the logs regarding the version number—it looks like a toss-up between v1.2.0 and the v2.0.0-beta."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.428428649902344,
          "generation_latency_ms": 3624.6185302734375,
          "total_latency_ms": 3635.04695892334
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-0",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Is the PrimaryDB9 system currently running exclusively on version v2.0.0-beta?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "No, PrimaryDB9 is not currently running exclusively on version v2.0.0-beta. Logs and reports indicate that v1.2.0 is also present or being reported alongside v2.0.0-beta, and the upgrade may not be fully propagated.",
          "retrieved_context": [
            "[Session llm-30, Turn 12] user: She thinks we upgraded PrimaryDB9 without telling her. Is it running v1.2.0 or that new beta?",
            "[Session llm-24, Turn 1] assistant: I'm looking at PrimaryDB9 now. It seems stable, though I'm seeing conflicting reports on the version number—some logs say v1.2.0 and others mention a beta.",
            "[Session llm-17, Turn 5] assistant: It looks like PrimaryDB9 is currently reporting two active versions: v1.2.0 and v2.0.0-beta. It seems the upgrade isn't fully propagated."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.607004165649414,
          "generation_latency_ms": 3712.252140045166,
          "total_latency_ms": 3722.8591442108154
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-0",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "How does the assistant determine the version status of the PrimaryDB9 when asked?",
          "answer_type": "short_answer",
          "gold_answer": "Checks headers",
          "predicted_answer": "The assistant determines the version status of PrimaryDB9 by looking at logs, reports, indicators, heartbeat signals, and headers from metrics.",
          "retrieved_context": [
            "[Session llm-24, Turn 1] assistant: I'm looking at PrimaryDB9 now. It seems stable, though I'm seeing conflicting reports on the version number—some logs say v1.2.0 and others mention a beta.",
            "[Session llm-17, Turn 5] assistant: It looks like PrimaryDB9 is currently reporting two active versions: v1.2.0 and v2.0.0-beta. It seems the upgrade isn't fully propagated.",
            "[Session llm-14, Turn 1] assistant: I'm looking at PrimaryDB9 now. It's a bit ambiguous; I see indicators for v1.2.0, but there are also heartbeat signals suggesting v2.0.0-beta is running."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.003328323364258,
          "generation_latency_ms": 3488.412380218506,
          "total_latency_ms": 3498.41570854187
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 5,
        "count": 10,
        "by_answer_type": {
          "short_answer": {
            "total": 3,
            "count": 8,
            "accuracy": 0.375
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_046.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.664355754852295,
      "total_evaluation_time_s": 67.64169359207153,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the cofounder the user plans to have lunch with?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver",
          "predicted_answer": "Oliver",
          "retrieved_context": [
            "[Session llm-21, Turn 0] user: Hey, good morning. Can you remind me to grab lunch with my cofounder later?",
            "[Session llm-21, Turn 1] assistant: Good morning, Alex. I've added a reminder for your lunch with Oliver. Is there anything specific you need to prep for the meeting?",
            "[Session llm-0, Turn 4] user: Right. I need to talk to my cofounder about hiring a release manager. Remind me to ping him later."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.547088623046875,
          "generation_latency_ms": 3710.2479934692383,
          "total_latency_ms": 3721.795082092285
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which infrastructure provider hosts the database that Sophia is concerned about regarding latency?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google's infrastructure (Google Cloud/GCP).",
          "retrieved_context": [
            "[Session llm-18, Turn 9] assistant: Correct, PrimaryDB9 is hosted on Google Cloud. The latency might be the connection from the Azure services.",
            "[Session llm-5, Turn 13] assistant: PrimaryDB9 is on GCP, likely for the BigQuery integration, though the latency to the other clouds is noted.",
            "[Session llm-3, Turn 6] user: Fair enough. I'm seeing some latency on the database. Is PrimaryDB9 sitting on the same network as the load balancer?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.479616165161133,
          "generation_latency_ms": 3052.971124649048,
          "total_latency_ms": 3064.450740814209
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which two specific version numbers are simultaneously active on the authentication service hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "v1.2.0 and v2.0.0-beta",
          "retrieved_context": [
            "[Session llm-8, Turn 10] user: I hope so. She's usually on top of things. Speaking of which, what version is that AWS auth service actually running?",
            "[Session llm-15, Turn 9] assistant: AuthService7 is up on AWS, but I'm seeing something odd with the versioning reported in the logs.",
            "[Session llm-17, Turn 10] user: Okay. I'm seeing some weird logs on that AWS auth service. What version is it pushing?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.819839477539062,
          "generation_latency_ms": 3746.9394207000732,
          "total_latency_ms": 3758.7592601776123
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "What is the name of the pet belonging to the Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Luna",
          "predicted_answer": "Luna",
          "retrieved_context": [
            "[Session llm-19, Turn 13] assistant: Yes, both of them are leads. You might want to loop in Emma or Mia from DevOps too.",
            "[Session llm-8, Turn 9] assistant: Typical Luna behavior. Hopefully, Sophia can still manage her backend lead duties with the feline assistance.",
            "[Session llm-0, Turn 3] assistant: It might have been her, or perhaps Sophia, since they both share the backend lead responsibilities."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.575460433959961,
          "generation_latency_ms": 3554.0292263031006,
          "total_latency_ms": 3565.6046867370605
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user ask to set a reminder for their mom's birthday before or after the reminder for dry cleaning?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Noted. I've set a reminder for the dry cleaning at 6 PM.",
            "[Session llm-19, Turn 5] assistant: I've set a reminder for 6 PM to call your mom. Anything else on the stack?",
            "[Session llm-11, Turn 7] assistant: Reminder set: Call mom at 5:00 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.350299835205078,
          "generation_latency_ms": 3577.8698921203613,
          "total_latency_ms": 3587.2201919555664
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-6",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What specific complaint did the user make about the coffee in session 23?",
          "answer_type": "short_answer",
          "gold_answer": "Tastes like battery acid",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-4, Turn 1] assistant: Hope the coffee was good. Did Oliver mention specific errors, or is it just general anxiety about the release?",
            "[Session llm-1, Turn 1] assistant: I'm pulling up the dashboard for prod now. Everything looks mostly green, though there are a few warnings. How is the coffee this morning?",
            "[Session llm-17, Turn 1] assistant: Good morning, Alex. Yes, I added 'buy coffee beans' to your personal list. You were running low on the dark roast."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.644746780395508,
          "generation_latency_ms": 3595.3078269958496,
          "total_latency_ms": 3604.952573776245
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the morning sessions, what beverage does the user consistently require before working?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee",
          "predicted_answer": "Coffee (or caffeine).",
          "retrieved_context": [
            "[Session llm-18, Turn 0] user: Morning. I need a quick sanity check on our infrastructure before my caffeine kicks in.",
            "[Session llm-10, Turn 13] assistant: They certainly seem to have the infrastructure under control. Is there anything else you need before your coffee kicks in?",
            "[Session llm-15, Turn 1] assistant: Good morning, Alex. I can certainly help you review the system health while the caffeine does its work. Where should we start?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.994983673095703,
          "generation_latency_ms": 4044.1040992736816,
          "total_latency_ms": 4054.0990829467773
        },
        {
          "probe_id": "preference_learning-explicit_preferences-8",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "At what time did the user specifically request to be reminded to call their mom?",
          "answer_type": "short_answer",
          "gold_answer": "6 PM",
          "predicted_answer": "The user requested to be reminded to call their mom at 6 PM and",
          "retrieved_context": [
            "[Session llm-19, Turn 5] assistant: I've set a reminder for 6 PM to call your mom. Anything else on the stack?",
            "[Session llm-11, Turn 7] assistant: Reminder set: Call mom at 5:00 PM.",
            "[Session llm-19, Turn 4] user: Got it. Oh, before I forget, remind me to call my mom for her birthday at 6 PM."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.597944259643555,
          "generation_latency_ms": 4616.974353790283,
          "total_latency_ms": 4627.572298049927
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific version number is running on the LoadBalancer4 service?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-9, Turn 11] assistant: LoadBalancer4 on Azure is operating within normal parameters.",
            "[Session llm-17, Turn 5] assistant: Actually, LoadBalancer4 is co-located with the Azure instance.",
            "[Session llm-19, Turn 6] user: Yeah, the load balancer, number 4. Is that on Amazon?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.949995040893555,
          "generation_latency_ms": 3095.91007232666,
          "total_latency_ms": 3104.8600673675537
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who is the specific team lead for the DevOps engineers Emma and Mia?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-20, Turn 5] assistant: Mia is the other DevOps engineer working alongside Emma.",
            "[Session llm-3, Turn 13] assistant: You should contact Emma or Mia; they are the current DevOps engineers.",
            "[Session llm-1, Turn 11] assistant: Emma and Mia are both on the DevOps roster today; either of them could investigate the version mismatch."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.981466293334961,
          "generation_latency_ms": 3401.6189575195312,
          "total_latency_ms": 3410.600423812866
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "When the user references the 'Microsoft cloud', which specific load balancer identifier is relevant?",
          "answer_type": "short_answer",
          "gold_answer": "LoadBalancer4",
          "predicted_answer": "LoadBalancer4",
          "retrieved_context": [
            "[Session llm-17, Turn 5] assistant: Actually, LoadBalancer4 is co-located with the Azure instance.",
            "[Session llm-7, Turn 9] assistant: That refers to AuthService2 and LoadBalancer4 on Azure, correct?",
            "[Session llm-21, Turn 13] assistant: Yes, LoadBalancer4 is also hosted on Azure."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.118650436401367,
          "generation_latency_ms": 3633.3277225494385,
          "total_latency_ms": 3644.44637298584
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-12",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "How did the user categorize the state of the deployment on the AWS auth service in the final session?",
          "answer_type": "short_answer",
          "gold_answer": "Stuck deployment",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-10, Turn 3] assistant: Checking the AWS deployment. AuthService7 appears to be up, but I'm seeing some mixed signals regarding the active version.",
            "[Session llm-15, Turn 8] user: Thanks. Now, shift over to the AWS deployment. Is the other auth node behaving?",
            "[Session llm-8, Turn 10] user: I hope so. She's usually on top of things. Speaking of which, what version is that AWS auth service actually running?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.974407196044922,
          "generation_latency_ms": 4461.737871170044,
          "total_latency_ms": 4472.712278366089
        }
      ],
      "summary": {
        "overall_accuracy": 0.5833333333333334,
        "total_score": 7,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 5,
            "count": 10,
            "accuracy": 0.5
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_047.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 30,
      "num_probes": 12,
      "ingestion_time_s": 1.724006175994873,
      "total_evaluation_time_s": 71.17117428779602,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which individual is the user referring to when mentioning the 'cofounder' who texted late at night about bills?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Elijah",
          "retrieved_context": [
            "[Session llm-28, Turn 2] user: Maybe I'm just anxious. My cofounder texted me late last night about the infrastructure bills.",
            "[Session llm-7, Turn 1] assistant: Good morning, Alex. Yes, your cofounder was active about thirty minutes ago; he left a note about the investor deck in the general channel.",
            "[Session llm-6, Turn 2] user: Thanks. I just got off a call with my cofounder. Elijah is stressing about our cloud bills again."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.022899627685547,
          "generation_latency_ms": 4104.089975357056,
          "total_latency_ms": 4117.112874984741
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which specific infrastructure services are currently hosted on the Amazon platform?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB4 and CacheLayer7",
          "predicted_answer": "PrimaryDB4 is hosted on the Amazon platform (AWS).",
          "retrieved_context": [
            "[Session llm-22, Turn 5] assistant: Yes, PrimaryDB4 is hosted on AWS.",
            "[Session llm-7, Turn 6] user: No, I missed it. I was too busy looking at the Auth service logs. Speaking of which, is AuthService5 also on the Amazon cluster? I always forget.",
            "[Session llm-12, Turn 1] assistant: Looking at PrimaryDB4 now. It appears to be running smoothly on the AWS infrastructure."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 9.57036018371582,
          "generation_latency_ms": 4176.004648208618,
          "total_latency_ms": 4185.575008392334
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "What is the job title of the person who owns the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Product Designer",
          "predicted_answer": "Product Designer",
          "retrieved_context": [
            "[Session llm-5, Turn 9] assistant: I haven't seen a final commit from Noah yet. Is he the Product Designer who just adopted that cat, Luna?",
            "[Session llm-0, Turn 9] assistant: Will do. Is this for a work project, or is it related to his cat, Luna?",
            "[Session llm-4, Turn 13] assistant: Managing two product designers can be like herding cats—speaking of Luna. I'll help you coordinate a meeting with them."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.001182556152344,
          "generation_latency_ms": 4080.8374881744385,
          "total_latency_ms": 4090.838670730591
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-4",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "Despite the dashboard displaying v1.2.0, what version do the logs suggest PrimaryDB4 is actually running?",
          "answer_type": "short_answer",
          "gold_answer": "v2.0.0-beta",
          "predicted_answer": "The logs suggest PrimaryDB4 is running v2.0.0-beta, despite",
          "retrieved_context": [
            "[Session llm-5, Turn 3] assistant: PrimaryDB4 is running on AWS. However, I'm noticing some ambiguity in the versioning logs—it looks like it's flipping between v1.2.0 and the v2.0.0-beta.",
            "[Session llm-2, Turn 14] user: Not yet. It's weird, the config file says PrimaryDB4 is on v1.2.0, but the runtime logs are reporting v2.0.0-beta.",
            "[Session llm-15, Turn 4] user: Mostly the Amazon bill. PrimaryDB4 is acting up. Half the logs show v1.2.0, but I see traces of the v2 beta in there too."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 12.721776962280273,
          "generation_latency_ms": 3964.58101272583,
          "total_latency_ms": 3977.3027896881104
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the user complain about the two-hour 'quick sync' before or after asking to clear their schedule to review hosting costs?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "The user complained about the two-hour 'quick sync' in Session llm-26, which occurred before Session llm-27 where they asked to review hosting costs.",
          "retrieved_context": [
            "[Session llm-27, Turn 2] user: Please do. I need to review the hosting costs. Are the database and cache on the same bill?",
            "[Session llm-26, Turn 2] user: Yeah, my cofounder pulled me into a \"quick sync\" that lasted two hours. He loves to hear himself talk.",
            "[Session llm-26, Turn 4] user: Thanks. Speaking of meetings, did Sophia send over the latency report for the main database?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.981559753417969,
          "generation_latency_ms": 3641.9708728790283,
          "total_latency_ms": 3652.9524326324463
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the CEO specifically concerned about the cost of the authentication service compared to the rest of the stack?",
          "answer_type": "short_answer",
          "gold_answer": "It is isolated on Azure",
          "predicted_answer": "The CEO is concerned because he thinks they are overpaying for the authentication system, and it is on",
          "retrieved_context": [
            "[Session llm-28, Turn 4] user: Yeah, he thinks we're overpaying for the authentication system. He wants to know why it's not on the same provider as the rest of our stack.",
            "[Session llm-2, Turn 3] assistant: CEOs are usually pretty focused on the bottom line. Is there a specific service he's worried about?",
            "[Session llm-6, Turn 3] assistant: Managing costs is always tricky for a CEO. Did he point out any specific spikes?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.01541519165039,
          "generation_latency_ms": 4062.540292739868,
          "total_latency_ms": 4073.5557079315186
        },
        {
          "probe_id": "preference_learning-explicit_preferences-7",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "How does the user's cofounder feel about ambiguity in status reports?",
          "answer_type": "short_answer",
          "gold_answer": "He hates it",
          "predicted_answer": "The user's cofounder hates ambiguity.",
          "retrieved_context": [
            "[Session llm-4, Turn 11] assistant: Typical CEO behavior. Does he want a status update on the visuals?",
            "[Session llm-24, Turn 4] user: Great, that's going to confuse Elijah. My cofounder hates ambiguity.",
            "[Session llm-23, Turn 10] user: Not yet. My cofounder is pretty focused on the quarterly goals right now, so I don't want to stress him out unless I have to."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.146379470825195,
          "generation_latency_ms": 3564.267635345459,
          "total_latency_ms": 3574.414014816284
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Does the CEO's concern about overpaying apply to the AWS services or the Azure service?",
          "answer_type": "short_answer",
          "gold_answer": "Azure service",
          "predicted_answer": "The CEO's concern about overpaying applies to the AWS services, as the primary database and Cache",
          "retrieved_context": [
            "[Session llm-9, Turn 3] assistant: Since your cofounder is the CEO, he's probably right to keep an eye on the budget. Both the primary database and CacheLayer7 are contributing to those AWS costs.",
            "[Session llm-6, Turn 4] user: Yeah, he thinks it's the cross-cloud traffic. Since our main database and the cache layer are over on Amazon's infrastructure, but the auth service is sitting on Azure, we're paying for the transfer.",
            "[Session llm-13, Turn 7] assistant: AuthService5 is running on a different provider, Azure, so it shouldn't be impacted by the AWS latency. It looks stable."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.561466217041016,
          "generation_latency_ms": 4385.37859916687,
          "total_latency_ms": 4395.940065383911
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of Oliver's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-16, Turn 11] assistant: His cat seems very involved in the design process. Does Oliver have a pet too?",
            "[Session llm-15, Turn 11] assistant: Hope his cat is okay. I can check if Oliver has uploaded his portion to the shared drive.",
            "[Session llm-13, Turn 10] user: Just Oliver for now. Noah is probably busy taking care of Luna, I heard she had to go to the vet."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.971380233764648,
          "generation_latency_ms": 3700.441598892212,
          "total_latency_ms": 3710.4129791259766
        },
        {
          "probe_id": "knowledge_boundaries-confidence_calibration-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "confidence_calibration",
          "question": "Does the assistant have visual confirmation of how big Noah's cat has grown?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-5, Turn 9] assistant: I haven't seen a final commit from Noah yet. Is he the Product Designer who just adopted that cat, Luna?",
            "[Session llm-16, Turn 11] assistant: His cat seems very involved in the design process. Does Oliver have a pet too?",
            "[Session llm-9, Turn 5] assistant: I believe Noah talks about his cat, Luna, quite often."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.397838592529297,
          "generation_latency_ms": 4093.33872795105,
          "total_latency_ms": 4104.736566543579
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-11",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "Who is currently responsible for handling the backend logic of the infrastructure migration?",
          "answer_type": "short_answer",
          "gold_answer": "Sophia",
          "predicted_answer": "The backend lead.",
          "retrieved_context": [
            "[Session llm-29, Turn 3] assistant: Understood. Balancing executive expectations with technical reality is always part of the CTO role. Where do we stand on the migration?",
            "[Session llm-29, Turn 2] user: Mostly the infrastructure migration. Being the CEO, he just wants it done, but I have to manage the technical debt.",
            "[Session llm-15, Turn 6] user: Yeah, as backend lead, she's the only one who really gets the migration path."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.164737701416016,
          "generation_latency_ms": 3897.343158721924,
          "total_latency_ms": 3907.50789642334
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "Based on the user's interactions, what specific database issue creates a risk of confusing the CEO?",
          "answer_type": "short_answer",
          "gold_answer": "Version ambiguity",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-2, Turn 3] assistant: CEOs are usually pretty focused on the bottom line. Is there a specific service he's worried about?",
            "[Session llm-4, Turn 11] assistant: Typical CEO behavior. Does he want a status update on the visuals?",
            "[Session llm-6, Turn 3] assistant: Managing costs is always tricky for a CEO. Did he point out any specific spikes?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.571645736694336,
          "generation_latency_ms": 4238.200902938843,
          "total_latency_ms": 4249.772548675537
        }
      ],
      "summary": {
        "overall_accuracy": 0.4166666666666667,
        "total_score": 5,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 10,
            "accuracy": 0.4
          },
          "abstain": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "preference_learning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_048.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 28,
      "num_probes": 12,
      "ingestion_time_s": 1.7510015964508057,
      "total_evaluation_time_s": 66.07590246200562,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-1",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder' who is stressing about platform stability?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-23, Turn 0] user: I just got off a long call with my cofounder; he is really stressing about the platform stability for the upcoming launch.",
            "[Session llm-25, Turn 6] user: Right. Oliver and Charlotte are arguing about whether to kill the beta.",
            "[Session llm-12, Turn 3] assistant: Correct, your cofounder Liam is still fully acting as CEO."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.030912399291992,
          "generation_latency_ms": 3704.6947479248047,
          "total_latency_ms": 3715.7256603240967
        },
        {
          "probe_id": "world_modeling-relationship_mapping-2",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Which employee is the owner of the cat named Luna?",
          "answer_type": "short_answer",
          "gold_answer": "Emma",
          "predicted_answer": "Emma, the Product Designer, is the owner of the cat named Luna.",
          "retrieved_context": [
            "[Session llm-12, Turn 9] assistant: Emma actually has a cat named Luna. She usually brings it up during the Product Designer standups.",
            "[Session llm-10, Turn 9] assistant: That must be Luna. It's nice that our Product Designer has a new pet to keep her company.",
            "[Session llm-15, Turn 12] user: The ones of her cat? Yeah, Luna looks adorable. The Product Designer has a good eye for photography too."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.670112609863281,
          "generation_latency_ms": 3873.706340789795,
          "total_latency_ms": 3885.376453399658
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-3",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "On which cloud provider is the service running the 'v2.0.0-beta' hosted?",
          "answer_type": "short_answer",
          "gold_answer": "Azure",
          "predicted_answer": "Azure",
          "retrieved_context": [
            "[Session llm-23, Turn 9] assistant: Checking the Azure deployment... it looks like v1.2.0 is still active, but the v2.0.0-beta is also spinning up.",
            "[Session llm-12, Turn 5] assistant: It's actually a bit complicated. That service on Azure is reporting both v1.2.0 and the v2.0.0-beta active at the same time.",
            "[Session llm-26, Turn 4] user: It looks like version flapping. One minute it's serving v1.2.0, and the next it's showing the beta tag."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.3759765625,
          "generation_latency_ms": 3244.1141605377197,
          "total_latency_ms": 3254.4901371002197
        },
        {
          "probe_id": "declarative_reasoning-baseline_factual_recall-4",
          "pillar": "declarative_reasoning",
          "subpillar": "baseline_factual_recall",
          "question": "Which two individuals currently hold the role of Backend Lead?",
          "answer_type": "short_answer",
          "gold_answer": "Oliver and Charlotte",
          "predicted_answer": "Oliver and Charlotte.",
          "retrieved_context": [
            "[Session llm-5, Turn 11] assistant: You have two options for Backend Lead right now—Oliver and Charlotte are both holding that title.",
            "[Session llm-1, Turn 7] assistant: It looks like both Oliver and Charlotte are currently designated as Backend Leads.",
            "[Session llm-25, Turn 7] assistant: It's always tricky when you have two Backend Leads with different opinions. What does the rest of the team think?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.402751922607422,
          "generation_latency_ms": 3164.4623279571533,
          "total_latency_ms": 3173.8650798797607
        },
        {
          "probe_id": "temporal_episodic-episode_reconstruction-5",
          "pillar": "temporal_episodic",
          "subpillar": "episode_reconstruction",
          "question": "What personal task did the user ask to be reminded of during the conversation about Oliver and Charlotte's argument?",
          "answer_type": "short_answer",
          "gold_answer": "Pick up dry cleaning",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-27, Turn 10] user: Oliver and Charlotte are butting heads over it. Since they both lead the backend team, it's a \"too many cooks\" situation.",
            "[Session llm-22, Turn 8] user: That's annoying. I bet Oliver or Charlotte know what's up with that.",
            "[Session llm-9, Turn 8] user: We have Oliver and Charlotte splitting the backend leadership duties, so they are coordinating a plan together."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.889436721801758,
          "generation_latency_ms": 3532.42826461792,
          "total_latency_ms": 3541.3177013397217
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-6",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "According to the session history, why did the user describe the game match as 'intense'?",
          "answer_type": "short_answer",
          "gold_answer": "It went into overtime",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-24, Turn 0] user: Morning! Did you manage to catch the highlights of the game last night?",
            "[Session llm-18, Turn 0] user: Hey, did you catch the game last night?",
            "[Session llm-6, Turn 4] user: Right, Charlotte. Thanks. By the way, did you see the score of the game last night?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.637044906616211,
          "generation_latency_ms": 3450.9010314941406,
          "total_latency_ms": 3461.538076400757
        },
        {
          "probe_id": "preference_learning-preference_induction-7",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "What is the CEO's primary motivation for pushing infrastructure consolidation?",
          "answer_type": "short_answer",
          "gold_answer": "Bottom line",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-25, Turn 3] assistant: As the CEO, he's probably looking at the bottom line. What specifically is he concerned about?",
            "[Session llm-8, Turn 1] assistant: I didn't see the summary, but it's great that the CEO is feeling optimistic. Did he announce any changes to the roadmap?",
            "[Session llm-20, Turn 1] assistant: Typical CEO energy. Did he give a specific deadline, or is it just \"asap\"?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.411739349365234,
          "generation_latency_ms": 4090.503215789795,
          "total_latency_ms": 4100.91495513916
        },
        {
          "probe_id": "preference_learning-preference_scope-8",
          "pillar": "preference_learning",
          "subpillar": "preference_scope",
          "question": "Which specific individual is concerned about the 'Azure spend'?",
          "answer_type": "short_answer",
          "gold_answer": "Liam",
          "predicted_answer": "Liam",
          "retrieved_context": [
            "[Session llm-25, Turn 4] user: The Azure spend. We have that second authentication service running there, right?",
            "[Session llm-1, Turn 10] user: Probably. Let's look at the data layer. Is the primary database sitting on Azure with the rest of the stack?",
            "[Session llm-2, Turn 6] user: Right, multi-cloud headaches. Speaking of Azure, is AuthService2 behaving?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.551286697387695,
          "generation_latency_ms": 3882.6053142547607,
          "total_latency_ms": 3892.1566009521484
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-9",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What version of software is the system 'primarydb_3' currently running?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-12, Turn 13] assistant: Actually, PrimaryDB3 is hosted on GCP.",
            "[Session llm-10, Turn 7] assistant: PrimaryDB3 is still listed as being hosted on GCP.",
            "[Session llm-17, Turn 9] assistant: Yes, PrimaryDB3 is currently hosted on GCP."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.979486465454102,
          "generation_latency_ms": 3067.82603263855,
          "total_latency_ms": 3077.805519104004
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the Data Scientist's pet?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-13, Turn 3] assistant: Yes, Mia is listed as the Data Scientist.",
            "[Session llm-8, Turn 7] assistant: I'll remind you to check in with the Product Designer about her new pet. In the meantime, are we looking into moving the database?",
            "[Session llm-15, Turn 11] assistant: I'll flag that for the Data Scientist's review. On a lighter note, did you see the pictures Emma posted in the general channel?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.531497955322266,
          "generation_latency_ms": 3283.8220596313477,
          "total_latency_ms": 3293.35355758667
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-11",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What technical configuration issue was identified as the cause of the version flapping on the Azure auth service?",
          "answer_type": "short_answer",
          "gold_answer": "Configuration drift",
          "predicted_answer": "A deployment issue was identified as the cause of the version flapping on the Azure auth",
          "retrieved_context": [
            "[Session llm-8, Turn 8] user: No, too risky. But we have a weirder issue. I was looking at the logs for AuthService2 on Azure, and the versioning looks wrong.",
            "[Session llm-18, Turn 7] assistant: Liam usually picks long lunches. Make sure you have time. I'm noticing some weird version headers on that Azure auth service, by the way.",
            "[Session llm-26, Turn 2] user: The auth service hosted on Azure. I think we have a deployment issue. Also, remind me to pick up milk later."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.597135543823242,
          "generation_latency_ms": 3573.0831623077393,
          "total_latency_ms": 3581.6802978515625
        },
        {
          "probe_id": "procedural_knowledge-lesson_application-12",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_application",
          "question": "Based on the identified configuration drift, was the traffic split on AuthService2 considered intentional?",
          "answer_type": "boolean",
          "gold_answer": "No",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-26, Turn 5] assistant: That sounds like a configuration drift on AuthService2. Is the traffic split intentional for testing?",
            "[Session llm-15, Turn 3] assistant: Yes, AuthService2 is currently serving traffic on both v1.2.0 and the v2.0.0-beta. It's sitting in the same environment as CacheLayer5.",
            "[Session llm-5, Turn 3] assistant: Taking a look at AuthService2 on Azure. It seems we have a version mismatch; v1.2.0 is the main instance, but I'm also seeing traffic going to v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.02192497253418,
          "generation_latency_ms": 4093.3871269226074,
          "total_latency_ms": 4103.409051895142
        }
      ],
      "summary": {
        "overall_accuracy": 0.5,
        "total_score": 6,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 9,
            "accuracy": 0.4444444444444444
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "boolean": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "declarative_reasoning": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_049.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 12,
      "ingestion_time_s": 1.6409649848937988,
      "total_evaluation_time_s": 68.0642499923706,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Which specific system is the user's cofounder concerned about regarding version ambiguity?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "The main database hosted on Google Cloud Platform (GCP).",
          "retrieved_context": [
            "[Session llm-21, Turn 4] user: Just the versioning. I have a feeling we aren't fully stable on the legacy build.",
            "[Session llm-12, Turn 6] user: Exactly. And it doesn't help that we can't figure out the versioning on that DB. The system shows v1.2.0, but the headers return v2.0.0-beta.",
            "[Session llm-14, Turn 0] user: I'm looking at the metrics for the main database on GCP, and the versioning looks weird. Is it reporting two different builds?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.990547180175781,
          "generation_latency_ms": 4141.048431396484,
          "total_latency_ms": 4153.03897857666
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the DevOps lead responsible for the recent beta deployment on the Google Cloud database?",
          "answer_type": "short_answer",
          "gold_answer": "Elijah",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-18, Turn 4] user: Right. And PrimaryDB7 is the one on Google Cloud, yeah? I noticed some logs referencing v2.0.0-beta for it, which scared me since I thought we were stable on v1.2.0.",
            "[Session llm-7, Turn 8] user: Thanks. Back to the database issue—I always forget which cloud provider we used for PrimaryDB7. Is that the one on Google's infrastructure?",
            "[Session llm-24, Turn 7] assistant: Classic Elijah. It sounds like his cat is really interested in DevOps these days. Was the deployment successful?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.995626449584961,
          "generation_latency_ms": 4031.4886569976807,
          "total_latency_ms": 4042.4842834472656
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "The database hosted on AWS runs alongside which other system on the same cloud provider?",
          "answer_type": "short_answer",
          "gold_answer": "AnalyticsEngine6",
          "predicted_answer": "The analytics engine.",
          "retrieved_context": [
            "[Session llm-7, Turn 10] user: Right, unlike PrimaryDB3 which is still over on AWS. The multi-cloud setup is giving Oliver a headache; he's worried about the billing complexity.",
            "[Session llm-11, Turn 7] assistant: That one, PrimaryDB3, is hosted over on AWS.",
            "[Session llm-23, Turn 14] user: Interesting structure. By the way, is the analytics engine running on the same cloud as the main DB?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.19906997680664,
          "generation_latency_ms": 3607.3577404022217,
          "total_latency_ms": 3617.5568103790283
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions are currently active on PrimaryDB7 according to the latest system checks?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "PrimaryDB7 is currently showing signals for both v1.2.0 and",
          "retrieved_context": [
            "[Session llm-8, Turn 13] assistant: It's a bit ambiguous. PrimaryDB7 is reporting version v1.2.0 in some places, but I'm also seeing traces of v2.0.0-beta.",
            "[Session llm-17, Turn 9] assistant: It looks like PrimaryDB7 is currently showing version v2.0.0-beta, though I recall it was on v1.2.0 previously.",
            "[Session llm-0, Turn 7] assistant: It's a bit ambiguous right now; PrimaryDB7 reports being on v1.2.0, but there are also signals for v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.148048400878906,
          "generation_latency_ms": 3874.788761138916,
          "total_latency_ms": 3884.936809539795
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why was the DevOps lead unavailable to immediately fix the database conflict in the morning?",
          "answer_type": "short_answer",
          "gold_answer": "Taking cat to vet",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-7, Turn 5] assistant: That is unusual. I've made a note of the version conflict on PrimaryDB7. Do you want me to flag this for the DevOps team?",
            "[Session llm-12, Turn 7] assistant: That sounds like a failed blue-green deployment. Have you flagged this to your DevOps person?",
            "[Session llm-8, Turn 5] assistant: That sounds stressful for a DevOps engineer, though I'm sure Luna didn't mean any harm. Did the deployment recover?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.596036911010742,
          "generation_latency_ms": 4059.54909324646,
          "total_latency_ms": 4070.1451301574707
        },
        {
          "probe_id": "temporal_episodic-temporal_sequencing-5",
          "pillar": "temporal_episodic",
          "subpillar": "temporal_sequencing",
          "question": "Did the discussion about the AWS bill happen before or after the strategy sync meeting was mentioned?",
          "answer_type": "short_answer",
          "gold_answer": "Before",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 4] user: Yeah, specifically the AWS bill. We have the analytics engine running there, plus one of the databases.",
            "[Session llm-5, Turn 13] assistant: I can note that for the Product Designer. By the way, is the analytics engine still sitting with the AWS database?",
            "[Session llm-10, Turn 10] user: Please do. And check the analytics engine while you're looking at the AWS resources."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.08694076538086,
          "generation_latency_ms": 2891.6728496551514,
          "total_latency_ms": 2902.759790420532
        },
        {
          "probe_id": "procedural_knowledge-task_state_tracking-6",
          "pillar": "procedural_knowledge",
          "subpillar": "task_state_tracking",
          "question": "What item is currently on the user's shopping list based on recent requests?",
          "answer_type": "short_answer",
          "gold_answer": "Coffee beans",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-22, Turn 1] assistant: I haven't yet, but I've added it to your shopping list now.",
            "[Session llm-19, Turn 5] assistant: That would be AnalyticsEngine6 and PrimaryDB3. Amazon's pricing can fluctuate based on usage.",
            "[Session llm-10, Turn 5] assistant: I've added a reminder to ask Elijah about his cat. On a related note, are we still looking at the caching layer in that same environment?"
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.00633430480957,
          "generation_latency_ms": 2830.3115367889404,
          "total_latency_ms": 2838.31787109375
        },
        {
          "probe_id": "procedural_knowledge-tool_method_memory-7",
          "pillar": "procedural_knowledge",
          "subpillar": "tool_method_memory",
          "question": "Which two frontend leads did the user decide to involve to verify feasibility?",
          "answer_type": "short_answer",
          "gold_answer": "Charlotte and Sophia",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 11] assistant: It might be because both Charlotte and Sophia hold the title of Frontend Lead, so the lines of responsibility are a bit blurred.",
            "[Session llm-17, Turn 11] assistant: My records show both Charlotte and Sophia listed as Frontend Lead. Is that a co-lead situation?",
            "[Session llm-10, Turn 14] user: Right. Well, at least the frontend leads aren't complaining about latency yet."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.70094108581543,
          "generation_latency_ms": 4247.24555015564,
          "total_latency_ms": 4257.946491241455
        },
        {
          "probe_id": "preference_learning-preference_induction-8",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "Based on the user's repeated checks, which infrastructure component requires the most monitoring attention?",
          "answer_type": "short_answer",
          "gold_answer": "PrimaryDB7",
          "predicted_answer": "Latency. The user expresses concern about latency in [Session llm-13",
          "retrieved_context": [
            "[Session llm-14, Turn 3] assistant: Reminder set for coffee beans. Do you want me to flag the latency issue to the infrastructure team?",
            "[Session llm-15, Turn 11] assistant: If both Frontend Leads are noticing it, it must be significant. How is the analytics component handling the data ingestion?",
            "[Session llm-21, Turn 0] user: Hey, hope you're awake. I need to sanity check our infrastructure before my 1:1 with my cofounder."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.570215225219727,
          "generation_latency_ms": 4347.469806671143,
          "total_latency_ms": 4359.040021896362
        },
        {
          "probe_id": "preference_learning-explicit_preferences-9",
          "pillar": "preference_learning",
          "subpillar": "explicit_preferences",
          "question": "What specific aspect of the company does Oliver usually focus on?",
          "answer_type": "short_answer",
          "gold_answer": "Financial overview",
          "predicted_answer": "Oliver usually focuses on the financial overview.",
          "retrieved_context": [
            "[Session llm-17, Turn 3] assistant: Typical CEO behavior. What specific parts of the infrastructure is Oliver concerned about?",
            "[Session llm-19, Turn 3] assistant: Is Oliver concerned about the current burn rate? He usually focuses on the financial overview.",
            "[Session llm-4, Turn 1] assistant: Caffeine is always a good start to the day. Did Oliver have specific concerns about the budget, or was it just a general check-in?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.177995681762695,
          "generation_latency_ms": 4181.770801544189,
          "total_latency_ms": 4189.948797225952
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-10",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What is the name of the user's gym?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-19, Turn 1] assistant: Good morning! No critical alerts showing up right now. I do have a reminder here that your gym membership renews tomorrow, though.",
            "[Session llm-3, Turn 9] assistant: The one of his cat, Luna, sleeping on his server rack? It was quite popular with the team.",
            "[Session llm-17, Turn 10] user: Right, we're testing the beta. Also, can you check who is leading the frontend team now? I thought it was Charlotte."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 8.760690689086914,
          "generation_latency_ms": 3390.5997276306152,
          "total_latency_ms": 3399.360418319702
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-11",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "Who holds the role of Backend Lead at the company?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-3, Turn 11] assistant: It might be because both Charlotte and Sophia hold the title of Frontend Lead, so the lines of responsibility are a bit blurred.",
            "[Session llm-6, Turn 13] assistant: Yes, currently both Charlotte and Sophia are acting as Frontend Leads.",
            "[Session llm-20, Turn 9] assistant: It looks like Charlotte and Sophia are both listed as the leads for the frontend team."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 9.844541549682617,
          "generation_latency_ms": 3007.2128772735596,
          "total_latency_ms": 3017.057418823242
        }
      ],
      "summary": {
        "overall_accuracy": 0.25,
        "total_score": 3,
        "count": 12,
        "by_answer_type": {
          "short_answer": {
            "total": 1,
            "count": 10,
            "accuracy": 0.1
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "declarative_reasoning": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "temporal_episodic": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 2,
            "accuracy": 0.0
          },
          "preference_learning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        }
      },
      "stored_memories": []
    },
    {
      "benchmark_file": "C:\\Users\\k_nai\\nebulaMemBenchmark\\complex_test_set\\benchmark_complex_050.json",
      "adapter_type": "NaiveRAGAdapter",
      "num_sessions": 25,
      "num_probes": 9,
      "ingestion_time_s": 1.707235336303711,
      "total_evaluation_time_s": 47.32272243499756,
      "probe_results": [
        {
          "probe_id": "world_modeling-entity_resolution-0",
          "pillar": "world_modeling",
          "subpillar": "entity_resolution",
          "question": "Who is the user referring to when they mention their 'cofounder'?",
          "answer_type": "short_answer",
          "gold_answer": "Noah",
          "predicted_answer": "Noah",
          "retrieved_context": [
            "[Session llm-3, Turn 7] assistant: Good idea to keep your cofounder in the loop. Do you want me to draft a message for the CEO?",
            "[Session llm-8, Turn 1] assistant: That would be Liam. Both he and Will are currently covering the Product Designer roles.",
            "[Session llm-9, Turn 0] user: Hey, did Noah send out that executive summary email yet? My cofounder is usually faster than this."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 13.340473175048828,
          "generation_latency_ms": 3494.162082672119,
          "total_latency_ms": 3507.502555847168
        },
        {
          "probe_id": "world_modeling-relationship_mapping-1",
          "pillar": "world_modeling",
          "subpillar": "relationship_mapping",
          "question": "Who is the Frontend Lead responsible for the React migration?",
          "answer_type": "short_answer",
          "gold_answer": "James",
          "predicted_answer": "James is the Frontend Lead responsible for the React migration.",
          "retrieved_context": [
            "[Session llm-23, Turn 4] user: Thanks. Also, remind me to check in with James about the React migration.",
            "[Session llm-17, Turn 14] user: Exactly. Also, who is taking lead on the frontend code for that new login flow?",
            "[Session llm-21, Turn 9] assistant: It's possible the Frontend Lead deployed it to the wrong environment. Shall I flag this for a rollback?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.99848747253418,
          "generation_latency_ms": 2666.5163040161133,
          "total_latency_ms": 2677.5147914886475
        },
        {
          "probe_id": "declarative_reasoning-fact_composition-2",
          "pillar": "declarative_reasoning",
          "subpillar": "fact_composition",
          "question": "Which cloud provider hosts the authentication service that is currently experiencing a stuck blue-green deployment?",
          "answer_type": "short_answer",
          "gold_answer": "GCP",
          "predicted_answer": "Google Cloud",
          "retrieved_context": [
            "[Session llm-20, Turn 3] assistant: Noah might be right. I'm seeing signals for both v1.2.0 and v2.0.0-beta running on AuthService8. It looks like a blue-green deployment got stuck.",
            "[Session llm-20, Turn 0] user: I'm reviewing the infrastructure map for Nebula Corp and it's a bit of a sprawl. Can you confirm which auth service is currently sitting in the Google cloud environment?",
            "[Session llm-16, Turn 2] user: It's the main authentication service. The one we deployed on Google Cloud. It's behaving erratically."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.99848747253418,
          "generation_latency_ms": 3766.793727874756,
          "total_latency_ms": 3777.79221534729
        },
        {
          "probe_id": "declarative_reasoning-belief_revision-3",
          "pillar": "declarative_reasoning",
          "subpillar": "belief_revision",
          "question": "What versions of the software are currently detected running on AuthService8?",
          "answer_type": "short_answer",
          "gold_answer": "v1.2.0 and v2.0.0-beta",
          "predicted_answer": "Both v1.2.0 and v2.0.0-beta are detected running on AuthService8.",
          "retrieved_context": [
            "[Session llm-11, Turn 1] assistant: I'm looking at AuthService8. It appears to be running v1.2.0, but I also see v2.0.0-beta active.",
            "[Session llm-5, Turn 13] assistant: AuthService8 is running there, though I'm detecting both v1.2.0 and a beta version active simultaneously.",
            "[Session llm-19, Turn 11] assistant: I'm seeing a bit of a discrepancy for AuthService8. One log says v1.2.0, but another is reporting v2.0.0-beta."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 8.439302444458008,
          "generation_latency_ms": 3722.3174571990967,
          "total_latency_ms": 3730.7567596435547
        },
        {
          "probe_id": "temporal_episodic-causal_explanation-4",
          "pillar": "temporal_episodic",
          "subpillar": "causal_explanation",
          "question": "Why is the user concerned about egress fees regarding the load balancer hosted on AWS?",
          "answer_type": "short_answer",
          "gold_answer": "It routes traffic to GCP",
          "predicted_answer": "The user is concerned about egress fees because an older load balancer on AWS is routing traffic to their main authentication service, which is hosted on GCP, and this cross-cloud traffic is not efficient.",
          "retrieved_context": [
            "[Session llm-22, Turn 2] user: It's the egress fees. We have that older load balancer on AWS routing traffic to our main auth service, which is sitting over on GCP. It's not efficient.",
            "[Session llm-23, Turn 8] user: That's the one. Anyway, I'm looking at our cloud bill and it's all over the place. Why do we have LoadBalancer2 on AWS?",
            "[Session llm-12, Turn 5] assistant: Noah usually wants those technical details ironed out quickly. How is the AWS load balancer handling the traffic?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 11.684417724609375,
          "generation_latency_ms": 3267.4689292907715,
          "total_latency_ms": 3279.153347015381
        },
        {
          "probe_id": "preference_learning-preference_induction-5",
          "pillar": "preference_learning",
          "subpillar": "preference_induction",
          "question": "How does the CEO prefer infrastructure cost reports to be formatted?",
          "answer_type": "short_answer",
          "gold_answer": "Detailed",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-0, Turn 3] assistant: It's natural for a CEO to worry about the bottom line. Regarding the infrastructure, where should we start?",
            "[Session llm-5, Turn 10] user: What a mess. I bet Noah is going to ask me about infrastructure costs during our exec meeting.",
            "[Session llm-5, Turn 11] assistant: As CEO, he likely wants to keep the budget in check."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 11.660099029541016,
          "generation_latency_ms": 3458.1007957458496,
          "total_latency_ms": 3469.7608947753906
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-6",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What kind of pet does Will own?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "Unknown",
          "retrieved_context": [
            "[Session llm-6, Turn 11] assistant: Classic Luna. Hopefully, Will isn't having similar pet distractions with his design work.",
            "[Session llm-14, Turn 13] assistant: Luna is his cat, right? It's easy to get distracted by pet antics.",
            "[Session llm-18, Turn 11] assistant: You could try Will or Liam. Although Liam might be distracted; he just shared a photo of his cat, Luna, in the general channel."
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 7.995843887329102,
          "generation_latency_ms": 3994.8506355285645,
          "total_latency_ms": 4002.8464794158936
        },
        {
          "probe_id": "knowledge_boundaries-negative_knowledge-7",
          "pillar": "knowledge_boundaries",
          "subpillar": "negative_knowledge",
          "question": "What specific roast of coffee beans did the user ask to be added to the shopping list?",
          "answer_type": "abstain",
          "gold_answer": "Unknown",
          "predicted_answer": "I don't know.",
          "retrieved_context": [
            "[Session llm-20, Turn 5] assistant: Reminder set for coffee beans. Do you want me to add a specific roast to your shopping list?",
            "[Session llm-2, Turn 7] assistant: I believe he mentioned his cat prefers salmon-flavored treats last time you spoke about her.",
            "[Session llm-16, Turn 1] assistant: Good morning, Alex. While I can't brew coffee, I can help you parse those logs. What are we looking at?"
          ],
          "score": 1,
          "match_type": "llm_judge",
          "search_latency_ms": 10.958194732666016,
          "generation_latency_ms": 2805.6468963623047,
          "total_latency_ms": 2816.6050910949707
        },
        {
          "probe_id": "procedural_knowledge-lesson_extraction-8",
          "pillar": "procedural_knowledge",
          "subpillar": "lesson_extraction",
          "question": "What does the user identify as the number one priority when dealing with the executive team?",
          "answer_type": "short_answer",
          "gold_answer": "Keeping them happy",
          "predicted_answer": "Keeping the executive team happy.",
          "retrieved_context": [
            "[Session llm-24, Turn 3] assistant: Understood. Keeping the executive team happy is priority number one. What specific part of the stack is worrying Noah right now?",
            "[Session llm-15, Turn 9] assistant: Typical Frontend Lead concerns. Do you want me to flag that for him?",
            "[Session llm-5, Turn 11] assistant: As CEO, he likely wants to keep the budget in check."
          ],
          "score": 0,
          "match_type": "llm_judge",
          "search_latency_ms": 10.375022888183594,
          "generation_latency_ms": 3255.472183227539,
          "total_latency_ms": 3265.8472061157227
        }
      ],
      "summary": {
        "overall_accuracy": 0.6666666666666666,
        "total_score": 6,
        "count": 9,
        "by_answer_type": {
          "short_answer": {
            "total": 4,
            "count": 7,
            "accuracy": 0.5714285714285714
          },
          "abstain": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          }
        },
        "by_pillar": {
          "world_modeling": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "declarative_reasoning": {
            "total": 1,
            "count": 2,
            "accuracy": 0.5
          },
          "temporal_episodic": {
            "total": 1,
            "count": 1,
            "accuracy": 1.0
          },
          "preference_learning": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          },
          "knowledge_boundaries": {
            "total": 2,
            "count": 2,
            "accuracy": 1.0
          },
          "procedural_knowledge": {
            "total": 0,
            "count": 1,
            "accuracy": 0.0
          }
        }
      },
      "stored_memories": []
    }
  ]
}